{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a32018-8eea-411f-8401-97e4c6d02ece",
   "metadata": {},
   "source": [
    "### **Approfondimento sulle Strategie di Valutazione nei Modelli di Machine Learning**\n",
    "\n",
    "#### **1. Introduzione alla Valutazione dei Modelli di Classificazione**\n",
    "Quando costruiamo un modello di classificazione, √® fondamentale disporre di metodi sistematici per misurare la sua efficacia. La valutazione dei modelli ci aiuta a comprendere quanto il modello sia in grado di distinguere correttamente le classi nei dati di input.\n",
    "\n",
    "La classificazione pu√≤ assumere diverse forme, a seconda del tipo di problema da risolvere:\n",
    "\n",
    "1. **Classificazione binaria**  \n",
    "   - Il modello deve distinguere tra due sole classi.  \n",
    "   - Esempio: Email spam vs. non spam, esito di un test medico positivo vs. negativo.\n",
    "  \n",
    "2. **Classificazione multi-classe**  \n",
    "   - Il modello deve assegnare ogni istanza a una sola classe tra pi√π possibili.  \n",
    "   - Esempio: Riconoscimento di cifre scritte a mano (0-9), categorizzazione di articoli di notizie in \"sport\", \"politica\" o \"economia\".\n",
    "\n",
    "3. **Classificazione multi-etichetta**  \n",
    "   - Ogni istanza pu√≤ appartenere a pi√π classi contemporaneamente.  \n",
    "   - Esempio: Assegnazione di pi√π tag a un articolo (un post pu√≤ essere classificato sia come \"tecnologia\" che \"business\").\n",
    "\n",
    "#### **2. Fattori che Influenzano la Scelta delle Metriche di Valutazione**\n",
    "Non esiste una metrica universale che sia sempre adatta a ogni problema di classificazione. La scelta delle metriche dipende da diversi aspetti:\n",
    "\n",
    "1. **Distribuzione delle classi nei dati**  \n",
    "   - Se le classi sono **bilanciate** (cio√® il numero di esempi per ogni classe √® simile), metriche standard come l'**accuratezza** possono essere utili.  \n",
    "   - Se le classi sono **sbilanciate** (es. il 95% delle email sono non spam e solo il 5% sono spam), metriche come **precision, recall e F1-score** sono pi√π adatte.\n",
    "\n",
    "2. **Costo relativo degli errori**  \n",
    "   - In alcuni problemi, gli errori non hanno lo stesso peso.  \n",
    "   - Esempio: In un test medico, classificare erroneamente un paziente malato come sano √® molto pi√π grave del contrario. In questi casi, il **recall** (capacit√† del modello di identificare i positivi) √® pi√π importante della **precisione**.\n",
    "\n",
    "3. **Necessit√† di ottenere probabilit√† o solo etichette**  \n",
    "   - Alcune applicazioni richiedono non solo la classe predetta, ma anche una probabilit√† associata.  \n",
    "   - Esempio: Nei sistemi di raccomandazione o nella medicina, conoscere la probabilit√† che un paziente abbia una malattia pu√≤ essere pi√π utile della semplice classificazione binaria.\n",
    "\n",
    "4. **Obiettivi specifici dell'applicazione**  \n",
    "   - Se il focus √® la massima precisione (evitare falsi positivi), useremo metriche diverse rispetto a quando il focus √® il massimo recall (ridurre i falsi negativi).  \n",
    "   - Esempio: Nei filtri anti-spam, un falso negativo (email spam classificata come non spam) potrebbe essere meno grave di un falso positivo (email importante finisce nella cartella spam).\n",
    "\n",
    "#### **3. Creazione di un Ambiente per la Valutazione**\n",
    "Prima di valutare un modello, √® necessario impostare un ambiente sperimentale:\n",
    "\n",
    "1. **Divisione del dataset**  \n",
    "   - Si suddivide il dataset in training set (per addestrare il modello) e test set (per valutarlo).\n",
    "   - Spesso si utilizza la **validazione incrociata (cross-validation)** per ottenere una stima pi√π affidabile delle prestazioni.\n",
    "\n",
    "2. **Generazione di dati di esempio**  \n",
    "   - Si possono creare dataset sintetici per testare i modelli.\n",
    "   - Librerie come `scikit-learn` forniscono funzioni come `make_classification()` per generare dati di esempio.\n",
    "\n",
    "3. **Scelta delle metriche**  \n",
    "   - Accuratezza, precisione, recall, F1-score, AUC-ROC, log-loss sono alcune delle metriche principali.\n",
    "   - Ogni metrica √® pi√π o meno adatta a seconda del contesto.\n",
    "\n",
    "Da qui, possiamo iniziare a esplorare le singole metriche e vedere come applicarle per valutare i modelli di classificazione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181de02-dc79-427a-a1e0-73e0ac28fa64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### **1Ô∏è‚É£ Importazione delle librerie**\n",
    "```python\n",
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "```\n",
    "Queste librerie sono fondamentali per l'analisi dei dati:\n",
    "- **NumPy (`np`)**: Fornisce supporto per array multidimensionali e funzioni matematiche.\n",
    "- **Pandas (`pd`)**: Permette di manipolare e analizzare dati tabulari con DataFrame.\n",
    "- **Matplotlib (`plt`)**: Serve per la visualizzazione di grafici.\n",
    "- **Seaborn (`sns`)**: Libreria di visualizzazione basata su Matplotlib con uno stile pi√π accattivante.\n",
    "\n",
    "```python\n",
    "# Machine learning libraries\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "```\n",
    "Librerie per il machine learning:\n",
    "- **`make_classification`**: Genera dataset sintetici per classificazione.\n",
    "- **`train_test_split`**: Divide i dati in set di training e testing.\n",
    "- **`LogisticRegression`**: Algoritmo di regressione logistica.\n",
    "- **`RandomForestClassifier`**: Algoritmo di classificazione basato su alberi decisionali.\n",
    "\n",
    "```python\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    roc_curve, \n",
    "    roc_auc_score,\n",
    "    precision_recall_curve, \n",
    "    average_precision_score,\n",
    ")\n",
    "```\n",
    "Metriche di valutazione:\n",
    "- **Matrice di confusione (`confusion_matrix`)**: Analizza gli errori di classificazione.\n",
    "- **Accuratezza (`accuracy_score`)**: Percentuale di predizioni corrette.\n",
    "- **Precisione (`precision_score`)**: Rapporto tra veri positivi e il totale delle predizioni positive.\n",
    "- **Recall (`recall_score`)**: Percentuale di veri positivi catturati dal modello.\n",
    "- **F1-score (`f1_score`)**: Media armonica di precisione e recall.\n",
    "- **ROC Curve (`roc_curve`) e AUC (`roc_auc_score`)**: Valutano la capacit√† del modello nel distinguere le classi.\n",
    "- **Curva Precision-Recall (`precision_recall_curve`) e punteggio medio di precisione (`average_precision_score`)**: Analizzano la performance nei dataset squilibrati.\n",
    "\n",
    "```python\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "```\n",
    "Impostazioni per la visualizzazione:\n",
    "- **Stile `seaborn-whitegrid`**: Rende i grafici pi√π leggibili.\n",
    "- **Tavolozza colori `viridis`**: Schema di colori ad alto contrasto.\n",
    "- **Parametri `plt.rcParams`**: Definiscono dimensioni e font dei grafici.\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Creazione dei dataset**\n",
    "```python\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "```\n",
    "- Fissa il seme casuale per rendere riproducibili i risultati.\n",
    "\n",
    "```python\n",
    "# Create a balanced dataset\n",
    "X_balanced, y_balanced = make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    weights=[0.5, 0.5],  # Equal class probabilities\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "- **Genera un dataset bilanciato**:\n",
    "  - 10.000 campioni con 20 feature totali.\n",
    "  - 10 feature contengono informazioni utili per la classificazione.\n",
    "  - 5 feature sono ridondanti (correlate con le informative).\n",
    "  - 2 classi (0 e 1) con probabilit√† uguali (`weights=[0.5, 0.5]`).\n",
    "\n",
    "```python\n",
    "# Create an imbalanced dataset (10% minority class)\n",
    "X_imbalanced, y_imbalanced = make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    weights=[0.9, 0.1],  # Imbalanced class probabilities\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "- **Genera un dataset sbilanciato**:\n",
    "  - Stesse caratteristiche del precedente.\n",
    "  - **90% della classe 0 e solo 10% della classe 1** (`weights=[0.9, 0.1]`).\n",
    "  - Serve per testare le metriche in situazioni di disequilibrio tra le classi.\n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Divisione in training e testing**\n",
    "```python\n",
    "# Split datasets into training and testing sets\n",
    "X_bal_train, X_bal_test, y_bal_train, y_bal_test = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.25, random_state=42\n",
    ")\n",
    "```\n",
    "- **Divide il dataset bilanciato**:\n",
    "  - 75% per il training.\n",
    "  - 25% per il testing.\n",
    "\n",
    "```python\n",
    "X_imb_train, X_imb_test, y_imb_train, y_imb_test = train_test_split(\n",
    "    X_imbalanced, y_imbalanced, test_size=0.25, random_state=42\n",
    ")\n",
    "```\n",
    "- **Divide il dataset sbilanciato** con le stesse proporzioni.\n",
    "\n",
    "---\n",
    "\n",
    "### **4Ô∏è‚É£ Controllo della distribuzione delle classi**\n",
    "```python\n",
    "# Check class distributions\n",
    "print(\"Balanced dataset class distribution:\")\n",
    "print(pd.Series(y_balanced).value_counts(normalize=True))\n",
    "```\n",
    "- **Conta le occorrenze delle classi** nel dataset bilanciato.\n",
    "- **Normalizza i valori** (`normalize=True`) per ottenere percentuali.\n",
    "\n",
    "```python\n",
    "print(\"\\nImbalanced dataset class distribution:\")\n",
    "print(pd.Series(y_imbalanced).value_counts(normalize=True))\n",
    "```\n",
    "- **Stesso controllo per il dataset sbilanciato**.\n",
    "\n",
    "#### **Output atteso:**\n",
    "```python\n",
    "Balanced dataset class distribution:\n",
    "0    0.5005\n",
    "1    0.4995\n",
    "```\n",
    "- Conferma che il dataset bilanciato ha quasi il **50% di classe 0 e 50% di classe 1**.\n",
    "\n",
    "```python\n",
    "Imbalanced dataset class distribution:\n",
    "0    0.8966\n",
    "1    0.1034\n",
    "```\n",
    "- Il dataset sbilanciato ha circa **90% classe 0 e 10% classe 1**.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîç Riepilogo**\n",
    "1. **Abbiamo importato librerie essenziali** per machine learning e analisi dei dati.\n",
    "2. **Abbiamo creato due dataset di classificazione**:\n",
    "   - Uno bilanciato (50%-50%).\n",
    "   - Uno sbilanciato (90%-10%).\n",
    "3. **Abbiamo diviso i dati in training e testing** (75%-25%).\n",
    "4. **Abbiamo verificato la distribuzione delle classi**.\n",
    "\n",
    "üìå **Prossimi passi?** Potremmo:\n",
    "- Allenare modelli di classificazione sui due dataset.\n",
    "- Valutare le prestazioni con le metriche importate.\n",
    "- Visualizzare i risultati.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd41c08-529a-4ea6-9f94-1bd08ba55aa6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Addestramento dei modelli**\n",
    "L'obiettivo di queste righe √® addestrare due modelli di regressione logistica e due modelli di foresta casuale su dataset bilanciati e sbilanciati.\n",
    "\n",
    "```python\n",
    "# Train a logistic regression model on the balanced dataset\n",
    "lr_balanced = LogisticRegression(random_state=42)\n",
    "lr_balanced.fit(X_bal_train, y_bal_train)\n",
    "```\n",
    "- Qui viene creato un modello di **regressione logistica** (`LogisticRegression`) con un parametro `random_state=42` per garantire la riproducibilit√† dei risultati.\n",
    "- Il modello viene addestrato sui dati di **training del dataset bilanciato** (`X_bal_train`, `y_bal_train`).\n",
    "\n",
    "```python\n",
    "# Train a logistic regression model on the imbalanced dataset\n",
    "lr_imbalanced = LogisticRegression(random_state=42)\n",
    "lr_imbalanced.fit(X_imb_train, y_imb_train)\n",
    "```\n",
    "- Qui si segue lo stesso approccio, ma per il dataset **sbilanciato** (`X_imb_train`, `y_imb_train`).\n",
    "\n",
    "---\n",
    "\n",
    "### **Generazione delle predizioni**\n",
    "Una volta addestrati i modelli, vengono generate le predizioni e le probabilit√† associate.\n",
    "\n",
    "```python\n",
    "# Generate predictions and probability scores\n",
    "y_bal_pred = lr_balanced.predict(X_bal_test)\n",
    "y_bal_prob = lr_balanced.predict_proba(X_bal_test)[:, 1]\n",
    "```\n",
    "- `predict(X_bal_test)`: genera le predizioni **binari** (0 o 1) per il dataset bilanciato.\n",
    "- `predict_proba(X_bal_test)[:, 1]`: restituisce la probabilit√† predetta per la classe positiva (`1`).\n",
    "\n",
    "```python\n",
    "y_imb_pred = lr_imbalanced.predict(X_imb_test)\n",
    "y_imb_prob = lr_imbalanced.predict_proba(X_imb_test)[:, 1]\n",
    "```\n",
    "- Stessa operazione per il dataset **sbilanciato**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Addestramento di un secondo modello: Random Forest**\n",
    "Oltre alla regressione logistica, viene addestrato un altro modello: **Random Forest**, per fare un confronto.\n",
    "\n",
    "```python\n",
    "# Train a second model (Random Forest) for comparison\n",
    "rf_balanced = RandomForestClassifier(random_state=42)\n",
    "rf_balanced.fit(X_bal_train, y_bal_train)\n",
    "```\n",
    "- Qui viene addestrato un modello di **Random Forest** (`RandomForestClassifier`) con gli stessi dati bilanciati.\n",
    "\n",
    "```python\n",
    "y_bal_pred_rf = rf_balanced.predict(X_bal_test)\n",
    "y_bal_prob_rf = rf_balanced.predict_proba(X_bal_test)[:, 1]\n",
    "```\n",
    "- `predict(X_bal_test)`: predizioni binarie (0 o 1) del modello Random Forest sul dataset bilanciato.\n",
    "- `predict_proba(X_bal_test)[:, 1]`: probabilit√† della classe positiva.\n",
    "\n",
    "```python\n",
    "rf_imbalanced = RandomForestClassifier(random_state=42)\n",
    "rf_imbalanced.fit(X_imb_train, y_imb_train)\n",
    "y_imb_pred_rf = rf_imbalanced.predict(X_imb_test)\n",
    "y_imb_prob_rf = rf_imbalanced.predict_proba(X_imb_test)[:, 1]\n",
    "```\n",
    "- Stesso procedimento, ma applicato al **dataset sbilanciato**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Metriche di classificazione di base\n",
    "\n",
    "\n",
    "### **Definizione della Confusion Matrix**\n",
    "La **Confusion Matrix** aiuta a visualizzare le prestazioni del modello mostrando:\n",
    "- **Vero Positivo (TP)**: quando il modello predice 1 e la realt√† √® 1.\n",
    "- **Falso Positivo (FP)**: quando il modello predice 1 ma la realt√† √® 0.\n",
    "- **Vero Negativo (TN)**: quando il modello predice 0 e la realt√† √® 0.\n",
    "- **Falso Negativo (FN)**: quando il modello predice 0 ma la realt√† √® 1.\n",
    "\n",
    "```python\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "```\n",
    "- Questa funzione prende i valori **veri** (`y_true`) e **predetti** (`y_pred`).\n",
    "- Utilizza la funzione `confusion_matrix()` di `sklearn.metrics` per calcolare la matrice di confusione.\n",
    "\n",
    "```python\n",
    "    # Extract values for annotation\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "```\n",
    "- `cm.ravel()` appiattisce la matrice `cm` per ottenere i valori in ordine `[TN, FP, FN, TP]`.\n",
    "\n",
    "```python\n",
    "    # Create a heatmap visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "```\n",
    "- `sns.heatmap()` crea una **mappa di calore** della matrice di confusione.\n",
    "- `annot=True`: visualizza i numeri nelle celle.\n",
    "- `fmt='d'`: indica che i valori sono numeri interi.\n",
    "- `cmap='Blues'`: imposta il colore della heatmap in blu.\n",
    "- `xticklabels` e `yticklabels` assegnano etichette agli assi.\n",
    "\n",
    "```python\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```\n",
    "- `plt.title(title)`: imposta il titolo della matrice.\n",
    "- `plt.tight_layout()`: ottimizza la disposizione degli elementi nel grafico.\n",
    "- `plt.show()`: mostra il grafico.\n",
    "\n",
    "```python\n",
    "    # Print TP, FP, TN, FN values\n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Negatives (FN): {fn}\")\n",
    "```\n",
    "- Stampa a schermo i valori della matrice di confusione.\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualizzazione delle confusion matrices**\n",
    "Infine, vengono chiamate le funzioni per visualizzare la matrice di confusione per entrambi i dataset:\n",
    "\n",
    "```python\n",
    "plot_confusion_matrix(y_bal_test, y_bal_pred, \"Confusion Matrix - Balanced Dataset\")\n",
    "plot_confusion_matrix(y_imb_test, y_imb_pred, \"Confusion Matrix - Imbalanced Dataset\")\n",
    "```\n",
    "- Viene mostrata la **Confusion Matrix** per il **dataset bilanciato**.\n",
    "- Viene mostrata la **Confusion Matrix** per il **dataset sbilanciato**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "- La regressione logistica e la Random Forest vengono addestrate su due dataset con distribuzioni diverse.\n",
    "- Si generano predizioni binarie e probabilit√†.\n",
    "- Si visualizzano le confusion matrices per comprendere le prestazioni dei modelli.\n",
    "- Questo ci aiuter√† a capire come i modelli si comportano quando una classe √® molto pi√π rara dell‚Äôaltra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9ea45-6635-4577-ad3b-6b5ac56e2b57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 1 CONFUSION MATRIX BALANCED DATA\n",
    "\n",
    "La matrice di confusione caricata rappresenta uno strumento per valutare le prestazioni di un modello di classificazione. √à suddivisa in quattro sezioni principali che riflettono il confronto tra i valori previsti e quelli effettivi:\n",
    "\n",
    "1. **True Negatives (TN)**: 1054 casi in cui il modello ha predetto correttamente una classe negativa.\n",
    "2. **False Positives (FP)**: 226 casi in cui il modello ha predetto positivamente, ma il risultato effettivo era negativo.\n",
    "3. **False Negatives (FN)**: 213 casi in cui il modello non ha rilevato correttamente una classe positiva (errore di omissione).\n",
    "4. **True Positives (TP)**: 1007 casi in cui il modello ha predetto correttamente una classe positiva.\n",
    "\n",
    "Questa distribuzione ci consente di calcolare metriche fondamentali per analizzare la performance del modello, come:\n",
    "\n",
    "- **Precisione**: Quanti dei risultati positivi previsti sono davvero corretti.\n",
    "- **Recall (o Sensibilit√†)**: Quanti dei positivi effettivi il modello √® riuscito a identificare.\n",
    "- **F1-Score**: Una media armonica tra Precision e Recall per bilanciare i due aspetti.\n",
    "\n",
    "\n",
    "### 2 CONFUSION MATRIX BALANCED DATA\n",
    "La tabella si compone di quattro quadranti che rappresentano i seguenti valori:\n",
    "\n",
    "1. **True Negatives (TN)**: 2221  \n",
    "   Questi sono i casi in cui il modello ha predetto correttamente una classe negativa.\n",
    "\n",
    "2. **False Positives (FP)**: 20  \n",
    "   Questi sono i casi in cui il modello ha predetto erroneamente una classe positiva per dati che in realt√† appartengono alla classe negativa.\n",
    "\n",
    "3. **False Negatives (FN)**: 141  \n",
    "   Qui il modello ha fallito nel riconoscere una classe positiva, classificandola invece come negativa.\n",
    "\n",
    "4. **True Positives (TP)**: 118  \n",
    "   Questi rappresentano i casi in cui il modello ha identificato correttamente la classe positiva.\n",
    "\n",
    "### Perch√© √® importante?\n",
    "\n",
    "- La matrice di confusione consente di calcolare metriche chiave come:\n",
    "   - **Accuracy**: Misura la percentuale di previsioni corrette, calcolata come $$(TP + TN) / (TP + TN + FP + FN)$$.\n",
    "   - **Precision**: Indica la proporzione di previsioni positive corrette, calcolata come $$TP / (TP + FP)$$.\n",
    "   - **Recall (Sensibilit√†)**: Mostra quanti dei positivi effettivi sono stati identificati dal modello, calcolata come $$TP / (TP + FN)$$.\n",
    "   - **F1-Score**: Media armonica di Precision e Recall, ideale per dataset sbilanciati.\n",
    "\n",
    "Questo tipo di analisi √® particolarmente utile per dataset squilibrati, dove una classe √® molto pi√π rappresentata dell'altra, aiutando a comprendere meglio i punti di forza e debolezza del modello.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23935a5a-f1df-43a4-8f72-752050c1933f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "## **Calcolo dell'Accuracy**\n",
    "L'**Accuracy** (accuratezza) √® una delle metriche pi√π intuitive per valutare un modello di classificazione ed √® definita come:\n",
    "\n",
    "\\[$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$\\]\n",
    "\n",
    "### **Codice: Calcolo dell'Accuracy**\n",
    "```python\n",
    "# Calculate accuracy for both datasets\n",
    "balanced_accuracy = accuracy_score(y_bal_test, y_bal_pred)\n",
    "imbalanced_accuracy = accuracy_score(y_imb_test, y_imb_pred)\n",
    "```\n",
    "- `accuracy_score(y_bal_test, y_bal_pred)`: calcola l'accuracy per il **dataset bilanciato** confrontando i valori **veri** (`y_bal_test`) con quelli **predetti** (`y_bal_pred`).\n",
    "- `accuracy_score(y_imb_test, y_imb_pred)`: calcola l'accuracy per il **dataset sbilanciato**.\n",
    "\n",
    "```python\n",
    "print(f\"Balanced dataset accuracy: {balanced_accuracy:.4f}\")\n",
    "print(f\"Imbalanced dataset accuracy: {imbalanced_accuracy:.4f}\")\n",
    "```\n",
    "- Stampa i valori dell'accuracy con **4 cifre decimali**.\n",
    "\n",
    "Output:\n",
    "```\n",
    "Balanced dataset accuracy: 0.8244\n",
    "Imbalanced dataset accuracy: 0.9356\n",
    "```\n",
    "- Il modello ha un'accuracy **del 82.44%** sul dataset **bilanciato**.\n",
    "- Il modello ha un'accuracy **del 93.56%** sul dataset **sbilanciato**, che sembra molto alta, ma potrebbe essere fuorviante.\n",
    "\n",
    "---\n",
    "\n",
    "### **Baseline: Predire sempre la classe maggioritaria**\n",
    "Per verificare quanto sia effettivamente utile il modello sul dataset **sbilanciato**, possiamo confrontarlo con una strategia molto semplice: **predire sempre la classe pi√π frequente**.\n",
    "\n",
    "```python\n",
    "# Let's see what happens if we always predict the majority class in the imbalanced dataset\n",
    "majority_predictions = np.zeros_like(y_imb_test)  # Assuming 0 is the majority class\n",
    "```\n",
    "- `np.zeros_like(y_imb_test)`: crea un array di zeri con la stessa forma di `y_imb_test`, ipotizzando che la **classe dominante sia 0**.\n",
    "- Questo significa che il modello **non sta realmente facendo alcuna previsione**, ma sta semplicemente predicendo sempre la classe pi√π comune.\n",
    "\n",
    "```python\n",
    "majority_accuracy = accuracy_score(y_imb_test, majority_predictions)\n",
    "print(f\"Imbalanced dataset - majority class baseline accuracy: {majority_accuracy:.4f}\")\n",
    "```\n",
    "- Calcola l'accuracy di questa strategia di baseline.\n",
    "- Stampa il risultato.\n",
    "\n",
    "Output:\n",
    "```\n",
    "Imbalanced dataset - majority class baseline accuracy: 0.8964\n",
    "```\n",
    "- Anche predicendo **sempre** la classe pi√π frequente otteniamo un'accuracy del **89.64%**, quindi il modello (che aveva il 93.56%) **non √® poi cos√¨ tanto migliore**.\n",
    "- Questo dimostra che **l'accuracy non √® sempre una buona metrica** per dataset sbilanciati!\n",
    "\n",
    "---\n",
    "\n",
    "## **Calcolo della Precision**\n",
    "La **Precision** √® definita come:\n",
    "\n",
    "\\[$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$\\]\n",
    "\n",
    "- Indica **quante delle predizioni positive sono effettivamente positive**.\n",
    "- √à particolarmente utile quando **i falsi positivi sono costosi**, come nelle diagnosi mediche.\n",
    "\n",
    "### **Codice: Calcolo della Precision**\n",
    "```python\n",
    "# Calculate precision for both datasets\n",
    "balanced_precision = precision_score(y_bal_test, y_bal_pred)\n",
    "imbalanced_precision = precision_score(y_imb_test, y_imb_pred)\n",
    "```\n",
    "- `precision_score(y_bal_test, y_bal_pred)`: calcola la precisione per il **dataset bilanciato**.\n",
    "- `precision_score(y_imb_test, y_imb_pred)`: calcola la precisione per il **dataset sbilanciato**.\n",
    "\n",
    "```python\n",
    "print(f\"Balanced dataset precision: {balanced_precision:.4f}\")\n",
    "print(f\"Imbalanced dataset precision: {imbalanced_precision:.4f}\")\n",
    "```\n",
    "- Stampa i valori della precisione.\n",
    "\n",
    "Output:\n",
    "```\n",
    "Balanced dataset precision: 0.8167\n",
    "Imbalanced dataset precision: 0.8551\n",
    "```\n",
    "- **Dataset bilanciato**: il modello ha una precisione **del 81.67%**.\n",
    "- **Dataset sbilanciato**: il modello ha una precisione **dell'85.51%**.\n",
    "- Questo significa che quando il modello predice un caso **positivo**, ha una probabilit√† dell'85.51% di essere corretto.\n",
    "\n",
    "---\n",
    "\n",
    "## **Calcolo del Recall (Sensibilit√†)**\n",
    "Il **Recall** √® definito come:\n",
    "\n",
    "\\[$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}$\n",
    "\\]\n",
    "\n",
    "- Indica **quanti dei casi realmente positivi sono stati identificati dal modello**.\n",
    "- √à importante quando **i falsi negativi sono costosi**, ad esempio in malattie gravi (meglio un falso allarme che non rilevarlo!).\n",
    "\n",
    "### **Codice: Calcolo del Recall**\n",
    "```python\n",
    "# Calculate recall for both datasets\n",
    "balanced_recall = recall_score(y_bal_test, y_bal_pred)\n",
    "imbalanced_recall = recall_score(y_imb_test, y_imb_pred)\n",
    "```\n",
    "- `recall_score(y_bal_test, y_bal_pred)`: calcola il recall per il **dataset bilanciato**.\n",
    "- `recall_score(y_imb_test, y_imb_pred)`: calcola il recall per il **dataset sbilanciato**.\n",
    "\n",
    "```python\n",
    "print(f\"Balanced dataset recall: {balanced_recall:.4f}\")\n",
    "print(f\"Imbalanced dataset recall: {imbalanced_recall:.4f}\")\n",
    "```\n",
    "- Stampa i valori del recall.\n",
    "\n",
    "Output:\n",
    "```\n",
    "Balanced dataset recall: 0.8254\n",
    "Imbalanced dataset recall: 0.4556\n",
    "```\n",
    "- **Dataset bilanciato**: il modello ha un recall **dell'82.54%**.\n",
    "- **Dataset sbilanciato**: il modello ha un recall **del 45.56%**.\n",
    "- Questo significa che il modello **manca pi√π della met√† dei casi positivi reali nel dataset sbilanciato**!\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusione**\n",
    "1. **L'accuracy √® fuorviante nei dataset sbilanciati**: Un'accuracy del **93.56%** sembra ottima, ma predire **sempre la classe pi√π frequente** gi√† porta a **89.64%**, quindi il modello **non √® molto utile**.\n",
    "2. **La precisione √® buona nel dataset sbilanciato (85.51%)**, il che significa che il modello √® affidabile nel **predire i positivi**, ma...\n",
    "3. **Il recall √® molto basso nel dataset sbilanciato (45.56%)**, il che significa che il modello **perde molti casi positivi reali**.\n",
    "4. **Meglio usare metriche pi√π avanzate**, come **F1-score, AUC-ROC e confusion matrix**, per valutare modelli su dataset sbilanciati\n",
    "\n",
    "\n",
    "***Un **dataset bilanciato** e un **dataset sbilanciato** si riferiscono alla distribuzione delle **classi** nel caso di problemi di classificazione, ossia quando l'obiettivo del modello √® quello di predire una variabile categorica (classi).\n",
    "\n",
    "### **Dataset Bilanciato**\n",
    "Un **dataset bilanciato** √® un dataset in cui tutte le **classi** (categorie) della variabile target sono rappresentate in modo **uguale o quasi uguale** in termini di numero di campioni. In altre parole, ogni classe ha una quantit√† simile di esempi.\n",
    "\n",
    "#### Esempio:\n",
    "Supponiamo di avere un dataset con una variabile target che rappresenta se un paziente √® **malato** o **sano**. In un dataset bilanciato, ci sarebbero, ad esempio, **50%** di pazienti malati e **50%** di pazienti sani.\n",
    "\n",
    "#### Vantaggi:\n",
    "- I modelli di classificazione hanno una buona probabilit√† di generalizzare correttamente, poich√© ogni classe ha lo stesso peso e quindi nessuna classe domina.\n",
    "- I metodi di validazione, come la **cross-validation**, sono pi√π facili da interpretare, poich√© le performance non sono influenzate dalla prevalenza di una classe rispetto all'altra.\n",
    "\n",
    "---\n",
    "\n",
    "### **Dataset Sbilanciato**\n",
    "Un **dataset sbilanciato** √® un dataset in cui una o pi√π **classi** sono rappresentate in modo **significativamente maggiore** rispetto alle altre. Questo accade quando una classe √® molto pi√π numerosa delle altre, creando una situazione di **disparit√† tra le classi**.\n",
    "\n",
    "#### Esempio:\n",
    "Nel caso di un dataset che cerca di predire se un paziente √® **malato** o **sano**, un dataset sbilanciato potrebbe contenere **95%** di pazienti sani e solo **5%** di pazienti malati.\n",
    "\n",
    "#### Problemi:\n",
    "- I modelli di classificazione possono diventare **predittori di maggioranza**, cio√® tendono a predire la classe pi√π frequente, ignorando quella meno frequente. In questo esempio, un modello potrebbe classificare ogni paziente come sano, ottenendo un'accuratezza alta (95%), ma non identificando correttamente i pazienti malati (classe di minoranza).\n",
    "- Le **metriche di performance** come l'accuratezza potrebbero non essere significative in questi casi, poich√© non riflettono correttamente la capacit√† del modello di identificare la classe minoritaria. Metriche alternative, come la **precisione**, il **richiamo** (recall), o la **F1-score**, sono pi√π appropriate per valutare il modello in questi scenari.\n",
    "- Il modello potrebbe anche soffrire di **overfitting** sulla classe maggioritaria, mentre non √® in grado di generalizzare bene sulla classe minoritaria.\n",
    "\n",
    "---\n",
    "\n",
    "### **Come affrontare un dataset sbilanciato?**\n",
    "Esistono diverse tecniche per bilanciare un dataset sbilanciato:\n",
    "\n",
    "1. **Resampling**:\n",
    "   - **Oversampling**: Aumentare il numero di esempi della classe minoritaria (ad esempio duplicando o generando nuovi campioni).\n",
    "   - **Undersampling**: Ridurre il numero di esempi della classe maggioritaria (ad esempio eliminando campioni).\n",
    "   \n",
    "2. **Tecniche di classificazione specializzate**: Alcuni algoritmi, come **Random Forest** o **Gradient Boosting**, sono pi√π robusti nel gestire dataset sbilanciati.\n",
    "\n",
    "3. **Pesatura delle classi**: Alcuni algoritmi consentono di applicare pesi maggiori alla classe minoritaria, in modo che l'algoritmo presti pi√π attenzione a quella classe durante l'allenamento.\n",
    "\n",
    "4. **Uso di metriche alternative**: Utilizzare metriche come **precisione**, **recall** e **F1-score** che sono pi√π informativi in presenza di un dataset sbilanciato.\n",
    "\n",
    "In generale, affrontare un dataset sbilanciato richiede un'attenzione particolare nella progettazione del modello e nella scelta delle metriche di valutazione.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efe8513-dbc8-4054-82d4-0b21c9eed4e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Certo! Vediamo nel dettaglio riga per riga il codice che calcola **F1 Score**, **Specificity** e introduce la **Threshold-Based Evaluation**.\n",
    "\n",
    "---\n",
    "\n",
    "## **F1 Score**\n",
    "L'**F1 Score** √® una metrica che bilancia **precision** e **recall**, ed √® particolarmente utile per dataset sbilanciati. √à definito dalla formula:\n",
    "\n",
    "\\[$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$\\]\n",
    "\n",
    "Dove:\n",
    "- **Precision** = \\($ \\frac{TP}{TP + FP}$ \\) (quanto il modello √® accurato quando predice positivo)\n",
    "- **Recall** = \\($ \\frac{TP}{TP + FN}$ \\) (quanto il modello cattura i veri positivi)\n",
    "\n",
    "### **Calcolo dell'F1 Score**\n",
    "```python\n",
    "# Calculate F1 score for both datasets\n",
    "balanced_f1 = f1_score(y_bal_test, y_bal_pred)\n",
    "imbalanced_f1 = f1_score(y_imb_test, y_imb_pred)\n",
    "```\n",
    "- `f1_score(y_bal_test, y_bal_pred)`: calcola l'**F1 Score** per il **dataset bilanciato**.\n",
    "- `f1_score(y_imb_test, y_imb_pred)`: calcola l'**F1 Score** per il **dataset sbilanciato**.\n",
    "\n",
    "```python\n",
    "print(f\"Balanced dataset F1 score: {balanced_f1:.4f}\")\n",
    "print(f\"Imbalanced dataset F1 score: {imbalanced_f1:.4f}\")\n",
    "```\n",
    "- Viene stampato il valore dell'F1 score con **4 decimali** per una migliore leggibilit√†.\n",
    "\n",
    "### **Output**\n",
    "```\n",
    "Balanced dataset F1 score: 0.8210\n",
    "Imbalanced dataset F1 score: 0.5945\n",
    "```\n",
    "- Il modello performa **meglio nel dataset bilanciato** (0.8210).\n",
    "- Il valore √® **molto pi√π basso nel dataset sbilanciato** (0.5945), indicando difficolt√† nel riconoscere la classe minoritaria.\n",
    "\n",
    "---\n",
    "\n",
    "## **Specificity**\n",
    "La **specificity** misura la capacit√† del modello di **identificare correttamente i casi negativi**. La formula √®:\n",
    "\n",
    "\\[$\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}$\n",
    "\\]\n",
    "\n",
    "Dove:\n",
    "- **TN (True Negative)**: Numero di negativi correttamente classificati.\n",
    "- **FP (False Positive)**: Numero di negativi classificati erroneamente come positivi.\n",
    "\n",
    "### **Calcolo della Specificity manualmente**\n",
    "```python\n",
    "# Calculate specificity manually (not directly available in sklearn)\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "```\n",
    "- `confusion_matrix(y_true, y_pred).ravel()` ottiene i valori **TN, FP, FN, TP**.\n",
    "- La funzione calcola la **specificity** usando la formula \\( $\\frac{TN}{TN + FP} $\\).\n",
    "\n",
    "```python\n",
    "balanced_specificity = specificity_score(y_bal_test, y_bal_pred)\n",
    "imbalanced_specificity = specificity_score(y_imb_test, y_imb_pred)\n",
    "```\n",
    "- `specificity_score(y_bal_test, y_bal_pred)`: calcola la **specificit√†** per il **dataset bilanciato**.\n",
    "- `specificity_score(y_imb_test, y_imb_pred)`: calcola la **specificit√†** per il **dataset sbilanciato**.\n",
    "\n",
    "```python\n",
    "print(f\"Balanced dataset specificity: {balanced_specificity:.4f}\")\n",
    "print(f\"Imbalanced dataset specificity: {imbalanced_specificity:.4f}\")\n",
    "```\n",
    "- Stampa i valori con **4 decimali**.\n",
    "\n",
    "### **Output**\n",
    "```\n",
    "Balanced dataset specificity: 0.8234\n",
    "Imbalanced dataset specificity: 0.9911\n",
    "```\n",
    "- Nel dataset **bilanciato**, la specificit√† √® **0.8234** ‚Üí Il modello identifica bene i negativi.\n",
    "- Nel dataset **sbilanciato**, la specificit√† √® **0.9911** ‚Üí Il modello √® **molto conservativo** e tende a classificare quasi tutto come negativo, ignorando la classe positiva.\n",
    "\n",
    "---\n",
    "\n",
    "## **Threshold-Based Evaluation**\n",
    "La **Threshold-Based Evaluation** analizza come la scelta della soglia influisce sulle prestazioni del modello.\n",
    "\n",
    "Di default, i modelli **predicono una probabilit√†** che un'osservazione appartenga alla classe positiva. Per assegnare una classe (`0` o `1`), si usa una soglia (di solito **0.5**).\n",
    "\n",
    "- **Bassa soglia (< 0.5)** ‚Üí Pi√π campioni classificati come positivi ‚Üí **Pi√π recall, meno precision**.\n",
    "- **Alta soglia (> 0.5)** ‚Üí Meno campioni classificati come positivi ‚Üí **Pi√π precision, meno recall**.\n",
    "\n",
    "### **Perch√© √® utile?**\n",
    "- Se il **costo degli errori √® asimmetrico**, possiamo **modificare la soglia** per **ottimizzare precision o recall**.\n",
    "- Per dataset sbilanciati, scegliere una soglia standard (0.5) potrebbe essere **non ottimale**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusione**\n",
    "- **L'F1 Score** bilancia precision e recall, evidenziando il problema dello sbilanciamento.\n",
    "- **La Specificity** aiuta a capire quanto bene il modello riconosce i **negativi**.\n",
    "- **Threshold-Based Evaluation** permette di **ottimizzare il modello** regolando la soglia di classificazione.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afbbe65-e360-4b4c-ae39-1a5634dba2a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## **Concetto di ROC Curve e AUC**\n",
    "- La **Receiver Operating Characteristic (ROC) curve** √® un grafico che mostra la relazione tra **True Positive Rate (TPR, Recall)** e **False Positive Rate (FPR, 1 - Specificit√†)** a diversi valori di soglia di classificazione.\n",
    "- L'**Area Under the Curve (AUC)** misura la qualit√† della classificazione. Un valore di AUC vicino a **1.0** indica un ottimo modello, mentre un valore di **0.5** indica un modello che fa previsioni casuali.\n",
    "\n",
    "---\n",
    "\n",
    "## **Funzione `plot_roc_curves`**\n",
    "Questa funzione genera e visualizza le **curve ROC** per pi√π modelli.\n",
    "\n",
    "```python\n",
    "def plot_roc_curves(models_data, title):\n",
    "```\n",
    "- **`models_data`**: un dizionario contenente i dati dei modelli, dove le chiavi sono i nomi dei modelli e i valori sono coppie `(y_true, y_prob)`, ovvero etichette reali e probabilit√† predette.\n",
    "- **`title`**: il titolo del grafico.\n",
    "\n",
    "```python\n",
    "    plt.figure(figsize=(6, 4))\n",
    "```\n",
    "- Crea una figura di **dimensioni 6x4 pollici** per il grafico.\n",
    "\n",
    "```python\n",
    "    for label, data in models_data.items():\n",
    "        y_true, y_prob = data\n",
    "```\n",
    "- Itera sui modelli passati alla funzione.\n",
    "- `label` √® il nome del modello (es. \"Logistic Regression\").\n",
    "- `data` √® una tupla contenente:\n",
    "  - `y_true`: etichette reali.\n",
    "  - `y_prob`: probabilit√† predette per la classe positiva (`1`).\n",
    "\n",
    "```python\n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "```\n",
    "- `roc_curve(y_true, y_prob)`: calcola **False Positive Rate (FPR)** e **True Positive Rate (TPR)** per diversi valori di soglia.\n",
    "- `roc_auc_score(y_true, y_prob)`: calcola l'**Area Under the Curve (AUC)**.\n",
    "\n",
    "```python\n",
    "        # Plot ROC curve\n",
    "        plt.plot(fpr, tpr, linewidth=2, label=f\"{label} (AUC = {auc:.3f})\")\n",
    "```\n",
    "- Disegna la curva ROC con:\n",
    "  - `fpr` sull'asse **x**.\n",
    "  - `tpr` sull'asse **y**.\n",
    "  - `label` include il nome del modello e il valore AUC.\n",
    "\n",
    "```python\n",
    "    # Add diagonal line (random classifier)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "```\n",
    "- Disegna una linea diagonale **dallo 0 al punto (1,1)**, rappresentando un **classificatore casuale** (AUC = 0.5).\n",
    "\n",
    "```python\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "```\n",
    "- **Etichetta gli assi**:\n",
    "  - **Asse X**: FPR (1 - Specificit√†).\n",
    "  - **Asse Y**: TPR (Recall).\n",
    "- **Imposta il titolo**.\n",
    "- **Aggiunge una legenda** in basso a destra.\n",
    "\n",
    "```python\n",
    "    plt.grid(True, alpha=0.3)\n",
    "```\n",
    "- Aggiunge una griglia leggera al grafico.\n",
    "\n",
    "```python\n",
    "    # Adjust axes\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "```\n",
    "- Imposta i limiti degli assi **tra -0.01 e 1.01**, per evitare tagli nei margini.\n",
    "\n",
    "```python\n",
    "    plt.annotate('Ideal Point\\n(FPR=0, TPR=1)', xy=(0.05, 0.95), xytext=(0.2, 0.8),\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05, width=1.5))\n",
    "```\n",
    "- **Annota il \"punto ideale\"** (FPR=0, TPR=1), cio√® **nessun falso positivo e tutti i veri positivi**.\n",
    "- Disegna una **freccia** per evidenziarlo.\n",
    "\n",
    "```python\n",
    "    plt.show()\n",
    "```\n",
    "- **Mostra il grafico**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Confronto tra modelli su dataset bilanciato e sbilanciato**\n",
    "\n",
    "Ora definiamo i modelli e chiamiamo la funzione:\n",
    "\n",
    "```python\n",
    "balanced_models = {\n",
    "    'Logistic Regression': (y_bal_test, y_bal_prob),\n",
    "    'Random Forest': (y_bal_test, y_bal_prob_rf)\n",
    "}\n",
    "```\n",
    "- **Dizionario `balanced_models`**:\n",
    "  - Contiene i dati dei modelli per il **dataset bilanciato**.\n",
    "  - `\"Logistic Regression\"`: `(y_bal_test, y_bal_prob)`.\n",
    "  - `\"Random Forest\"`: `(y_bal_test, y_bal_prob_rf)`.\n",
    "\n",
    "```python\n",
    "imbalanced_models = {\n",
    "    'Logistic Regression': (y_imb_test, y_imb_prob),\n",
    "    'Random Forest': (y_imb_test, y_imb_prob_rf)\n",
    "}\n",
    "```\n",
    "- **Dizionario `imbalanced_models`**:\n",
    "  - Contiene i dati dei modelli per il **dataset sbilanciato**.\n",
    "\n",
    "```python\n",
    "plot_roc_curves(balanced_models, \"ROC Curves - Balanced Dataset\")\n",
    "plot_roc_curves(imbalanced_models, \"ROC Curves - Imbalanced Dataset\")\n",
    "```\n",
    "- Chiama la funzione `plot_roc_curves` per generare e visualizzare le ROC curves:\n",
    "  - **Dataset bilanciato**.\n",
    "  - **Dataset sbilanciato**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusione**\n",
    "- Il codice genera **curve ROC** per diversi modelli.\n",
    "- **L'AUC** aiuta a confrontare le prestazioni tra modelli e dataset.\n",
    "- Un **modello migliore** avr√† una curva ROC che si avvicina all'angolo in alto a sinistra.\n",
    "- I modelli sui **dataset bilanciati** tendono ad avere performance pi√π stabili rispetto ai **dataset sbilanciati**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f307e5b-8c1d-411b-987e-92a24bdbea2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 1) ROC CURVE - BALANCED DATASET\n",
    "\n",
    "L'immagine mostra le curve ROC (Receiver Operating Characteristic) per due modelli di machine learning: **Logistic Regression** e **Random Forest**, valutati su un dataset bilanciato. La curva ROC confronta le prestazioni di classificazione di un modello tracciando il **True Positive Rate (Recall)** contro il **False Positive Rate (1 - Specificity)**. \n",
    "\n",
    "Ecco i dettagli principali:\n",
    "- **Linea tratteggiata**: Rappresenta un classificatore casuale (random), utilizzato come baseline.\n",
    "- **AUC (Area Under the Curve)**: Valuta l'efficacia del modello. Pi√π l'AUC si avvicina a 1, migliore √® il modello. In questo caso:\n",
    "  - Logistic Regression: **AUC = 0.898**\n",
    "  - Random Forest: **AUC = 0.978**\n",
    "\n",
    "### Interpretazione:\n",
    "- La curva del **Random Forest** si avvicina maggiormente all'angolo in alto a sinistra della griglia, indicando prestazioni superiori rispetto al Logistic Regression.\n",
    "- Un valore AUC maggiore (0.978 per Random Forest) segnala che questo modello ha una migliore capacit√† di distinguere tra le classi positive e negative rispetto al Logistic Regression.\n",
    "\n",
    "In sintesi, il grafico dimostra che il modello Random Forest √® pi√π efficace nel classificare correttamente i dati rispetto al modello Logistic Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d2f16c-6868-4178-90ef-e8b80d8dd22a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2) ROC CURVE - IMBALANCED DATASET\n",
    "\n",
    "L'immagine mostra le **curve ROC (Receiver Operating Characteristic)** di due modelli di machine learning: **Logistic Regression** (in viola) e **Random Forest** (in blu). Queste curve vengono utilizzate per valutare la capacit√† dei modelli di distinguere tra due classi, tracciando il **True Positive Rate (TPR)** (o Recall) contro il **False Positive Rate (FPR)** (1 - Specificity) per vari valori di soglia.\n",
    "\n",
    "### Punti principali:\n",
    "1. **AUC (Area Under the Curve)**:\n",
    "   - Logistic Regression: **AUC = 0.893**\n",
    "   - Random Forest: **AUC = 0.951**\n",
    "\n",
    "   Un valore AUC pi√π alto indica prestazioni migliori; in questo caso, il modello Random Forest supera Logistic Regression.\n",
    "\n",
    "2. **Linea diagonale tratteggiata**:\n",
    "   Rappresenta un classificatore casuale (AUC = 0.5). Entrambi i modelli si comportano significativamente meglio del caso casuale.\n",
    "\n",
    "3. **Punto ideale**:\n",
    "   L'angolo in alto a sinistra (TPR = 1, FPR = 0) rappresenta il punto ideale, dove non ci sono falsi positivi n√© falsi negativi. La curva del modello Random Forest si avvicina di pi√π a questo punto, dimostrando maggiore accuratezza.\n",
    "\n",
    "### Interpretazione:\n",
    "- Il modello **Random Forest** mostra prestazioni superiori, grazie a un'AUC pi√π alta e a una maggiore capacit√† di classificare correttamente le due classi.\n",
    "- Questi grafici sono particolarmente utili nei dataset sbilanciati, poich√© offrono una rappresentazione pi√π completa delle prestazioni di classificazione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911c44f-1d36-4a76-9758-a555f019559a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# PRECISION RECALL CURVE \n",
    "\n",
    "\n",
    "\n",
    "### **Definizione della funzione `plot_pr_curves`**\n",
    "```python\n",
    "def plot_pr_curves(models_data, title):\n",
    "```\n",
    "- Definiamo una funzione chiamata `plot_pr_curves` che prende due argomenti:\n",
    "  - `models_data`: un dizionario contenente le predizioni probabilistiche di pi√π modelli.\n",
    "  - `title`: il titolo del grafico.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    plt.figure(figsize=(10, 8))\n",
    "```\n",
    "- Creiamo una nuova figura (`figure`) con dimensioni **10x8 pollici**, per avere una visualizzazione chiara e leggibile.\n",
    "\n",
    "---\n",
    "\n",
    "### **Ciclo sui modelli per tracciare la curva PR**\n",
    "```python\n",
    "    for label, data in models_data.items():\n",
    "```\n",
    "- Iteriamo su ogni modello presente in `models_data`.\n",
    "- `label` rappresenta il nome del modello (es. \"Logistic Regression\", \"Random Forest\").\n",
    "- `data` √® una tupla contenente:\n",
    "  - `y_true`: i veri valori della classe (0 o 1).\n",
    "  - `y_prob`: le probabilit√† previste dal modello per la classe positiva.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        y_true, y_prob = data\n",
    "```\n",
    "- Estraiamo i due valori della tupla `data`:\n",
    "  - `y_true`: contiene le etichette reali (0 = negativo, 1 = positivo).\n",
    "  - `y_prob`: contiene la probabilit√† assegnata dal modello alla classe positiva.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "```\n",
    "- **Calcoliamo la Precision-Recall Curve** usando la funzione `precision_recall_curve`:\n",
    "  - `precision`: array contenente i valori della precisione per diversi threshold.\n",
    "  - `recall`: array contenente i valori del richiamo per gli stessi threshold.\n",
    "  - `thresholds`: valori di soglia usati per variare precisione e richiamo.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        ap = average_precision_score(y_true, y_prob)\n",
    "```\n",
    "- **Calcoliamo l'Average Precision (AP)**, una metrica che rappresenta l'area sotto la Precision-Recall Curve. Maggiore √® l'AP, migliore √® il modello.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        plt.plot(recall, precision, linewidth=2, label=f\"{label} (AP = {ap:.3f})\")\n",
    "```\n",
    "- **Disegniamo la Precision-Recall Curve** nel grafico:\n",
    "  - Sull'asse x mettiamo il **recall**.\n",
    "  - Sull'asse y mettiamo la **precision**.\n",
    "  - `linewidth=2` imposta lo spessore della linea.\n",
    "  - `label=f\"{label} (AP = {ap:.3f})\"` mostra il nome del modello e l'Average Precision.\n",
    "\n",
    "---\n",
    "\n",
    "### **Aggiunta della linea di riferimento per un modello casuale**\n",
    "```python\n",
    "    no_skill = sum(y_true) / len(y_true)  # Frequency of positive class\n",
    "```\n",
    "- Calcoliamo il **tasso di positivit√†** nel dataset:\n",
    "  - `sum(y_true)`: conta il numero di esempi positivi (dove `y_true = 1`).\n",
    "  - `len(y_true)`: conta il numero totale di esempi.\n",
    "  - `no_skill` rappresenta la **probabilit√† base** con cui un modello casuale predirebbe un positivo.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    plt.plot([0, 1], [no_skill, no_skill], 'k--', linewidth=1, label=f'No Skill (AP = {no_skill:.3f})')\n",
    "```\n",
    "- Disegniamo una **linea orizzontale tratteggiata** che rappresenta un modello senza abilit√† (\"No Skill\"):\n",
    "  - `[0, 1]` sull'asse x per coprire tutto il grafico.\n",
    "  - `[no_skill, no_skill]` fissa la linea orizzontale.\n",
    "  - `'k--'` specifica una linea **nera tratteggiata**.\n",
    "  - `label=f'No Skill (AP = {no_skill:.3f})'` aggiunge l'etichetta con il valore della baseline AP.\n",
    "\n",
    "---\n",
    "\n",
    "### **Etichette e miglioramenti grafici**\n",
    "```python\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "```\n",
    "- **Etichette degli assi**:\n",
    "  - `Recall` sull'asse x.\n",
    "  - `Precision` sull'asse y.\n",
    "\n",
    "```python\n",
    "    plt.title(title)\n",
    "```\n",
    "- **Imposta il titolo del grafico** usando il parametro `title`.\n",
    "\n",
    "```python\n",
    "    plt.legend(loc='best')\n",
    "```\n",
    "- **Aggiunge la legenda**, posizionandola automaticamente nel posto migliore (`loc='best'`).\n",
    "\n",
    "```python\n",
    "    plt.grid(True, alpha=0.3)\n",
    "```\n",
    "- **Aggiunge una griglia** con trasparenza `alpha=0.3` per una migliore leggibilit√†.\n",
    "\n",
    "---\n",
    "\n",
    "### **Regolazione degli assi e annotazione**\n",
    "```python\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "```\n",
    "- **Limiti degli assi** leggermente estesi oltre `[0,1]` per evitare tagli alle curve.\n",
    "\n",
    "```python\n",
    "    plt.annotate('Ideal Point\\n(Recall=1, Precision=1)', xy=(0.9, 0.95), xytext=(0.6, 0.9),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05, width=1.5))\n",
    "```\n",
    "- **Annotazione del punto ideale** in alto a destra del grafico (`Recall=1, Precision=1`):\n",
    "  - `xy=(0.9, 0.95)`: posizione del punto da evidenziare.\n",
    "  - `xytext=(0.6, 0.9)`: posizione del testo rispetto al punto.\n",
    "  - `arrowprops=dict(...)` disegna una **freccia nera** che collega il testo al punto.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    plt.show()\n",
    "```\n",
    "- **Mostra il grafico** a schermo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Chiamata alla funzione per entrambi i dataset**\n",
    "```python\n",
    "plot_pr_curves(balanced_models, \"Precision-Recall Curves - Balanced Dataset\")\n",
    "plot_pr_curves(imbalanced_models, \"Precision-Recall Curves - Imbalanced Dataset\")\n",
    "```\n",
    "- **Tracciamo le PR Curves** per:\n",
    "  1. Il dataset bilanciato (`balanced_models`).\n",
    "  2. Il dataset sbilanciato (`imbalanced_models`).\n",
    "- Ogni chiamata crea un grafico separato.\n",
    "\n",
    "---\n",
    "\n",
    "## **Riassunto**\n",
    "Questa funzione:\n",
    "1. **Calcola** Precision e Recall per ogni modello.\n",
    "2. **Disegna** la Precision-Recall Curve.\n",
    "3. **Aggiunge** una baseline per un classificatore casuale.\n",
    "4. **Formatta** il grafico con etichette, legenda e annotazioni.\n",
    "5. **Mostra** i risultati in un grafico chiaro e leggibile.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e1a590-9fbf-4fb3-908e-7cf402c6209b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "###  1) 1 IMMAGINE BALANCED DATA \n",
    "L'immagine mostra la **Precision-Recall Curve**, che √® uno strumento per valutare le prestazioni di due modelli di machine learning: **Logistic Regression** (in viola) e **Random Forest** (in blu). Questa curva √® particolarmente utile per comprendere le prestazioni dei modelli in dataset bilanciati o sbilanciati.\n",
    "\n",
    "### Dettagli principali:\n",
    "1. **Precision** (asse y): Misura la percentuale di risultati positivi previsti che sono effettivamente corretti.\n",
    "2. **Recall** (asse x): Misura la capacit√† del modello di identificare correttamente i casi positivi.\n",
    "3. **\"No Skill\" Linea tratteggiata nera**: Rappresenta un modello casuale con **Average Precision (AP) = 0.488**.\n",
    "4. **AP (Average Precision)**:\n",
    "   - Logistic Regression: **AP = 0.903**.\n",
    "   - Random Forest: **AP = 0.976**.\n",
    "\n",
    "### Interpretazione:\n",
    "- **Random Forest** si avvicina di pi√π all'\"Ideal Point\" (1,1), che rappresenta una classificazione perfetta (precisione e recall al massimo).\n",
    "- Un **AP pi√π alto** indica una migliore capacit√† di trovare correttamente i casi positivi senza perdere precisione. In questo caso, Random Forest supera Logistic Regression con un valore di AP pi√π elevato (0.976 contro 0.903).\n",
    "\n",
    "Questa visualizzazione √® particolarmente rilevante per confrontare i modelli in termini di capacit√† di gestire compromessi tra precisione e recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf534fc9-fb39-49df-865c-fde18356e7bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2 IMMAFINE - IMBALANCED DATA \n",
    "\n",
    "La **Precision-Recall Curve** nell'immagine confronta le prestazioni di due modelli di machine learning (**Logistic Regression** e **Random Forest**) su un dataset sbilanciato. Questo tipo di curva √® particolarmente utile in tali contesti, dove le metriche di accuratezza tradizionali possono essere fuorvianti. \n",
    "\n",
    "### Elementi principali della curva:\n",
    "1. **Precision** (asse y): Indica la proporzione di risultati positivi previsti corretti.\n",
    "2. **Recall** (asse x): Misura la capacit√† del modello di identificare correttamente i casi positivi.\n",
    "3. **Linee rappresentate**:\n",
    "   - Logistic Regression (**AP = 0.702**, linea viola): Prestazioni buone, ma inferiori rispetto al secondo modello.\n",
    "   - Random Forest (**AP = 0.840**, linea blu): Prestazioni superiori, con un valore di **Average Precision** (AP) maggiore.\n",
    "   - **Linea tratteggiata nera (\"No Skill\")**: Rappresenta un modello casuale con un **AP = 0.104**, utilizzato come baseline.\n",
    "\n",
    "### Interpretazione:\n",
    "- **Higher Area Under the Curve (AP)**: Random Forest si dimostra pi√π efficace, con un equilibrio migliore tra precision e recall, rispetto a Logistic Regression.\n",
    "- La curva del modello Random Forest si avvicina di pi√π all'angolo in alto a destra, che rappresenta il punto ideale di massima precision e recall.\n",
    "\n",
    "In sintesi, il modello **Random Forest** offre prestazioni migliori per il dataset sbilanciato, essendo pi√π affidabile nel distinguere i dati delle classi positive e negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a53ee0b-8708-436a-9bce-00a2bbcb964d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1.5 Tradeoffs in Classification Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6159dbf-e5cb-4566-9103-b29e54dd6639",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Funzione `analyze_thresholds`**\n",
    "\n",
    "```python\n",
    "def analyze_thresholds(y_true, y_prob, thresholds, model_name, dataset_name, num_points=10):\n",
    "```\n",
    "- Definizione della funzione `analyze_thresholds`, che serve ad analizzare le prestazioni del modello in base a diverse soglie di classificazione.\n",
    "- Parametri:\n",
    "  - `y_true`: etichette reali del test set.\n",
    "  - `y_prob`: probabilit√† previste dal modello per la classe positiva.\n",
    "  - `thresholds`: array contenente le soglie di decisione.\n",
    "  - `model_name`: nome del modello (per scopi descrittivi).\n",
    "  - `dataset_name`: nome del dataset (per scopi descrittivi).\n",
    "  - `num_points`: numero di soglie da analizzare (di default 10).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Select a subset of thresholds to analyze\n",
    "    indices = np.linspace(0, len(thresholds) - 1, num_points, dtype=int)\n",
    "    selected_thresholds = thresholds[indices]\n",
    "```\n",
    "- Usa `np.linspace` per selezionare `num_points` soglie equidistanti all'interno di `thresholds`.\n",
    "- `indices` contiene gli indici corrispondenti a queste soglie selezionate.\n",
    "- `selected_thresholds` estrae le soglie effettive da `thresholds`.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Calculate metrics for each threshold\n",
    "    results = []\n",
    "    for threshold in selected_thresholds:\n",
    "```\n",
    "- Crea una lista `results` che conterr√† i risultati per ogni soglia selezionata.\n",
    "- Inizia un ciclo `for` per iterare su ciascuna soglia selezionata.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "```\n",
    "- Converte le probabilit√† previste (`y_prob`) in previsioni binarie:\n",
    "  - Se `y_prob >= threshold`, predice 1 (classe positiva).\n",
    "  - Altrimenti predice 0 (classe negativa).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "```\n",
    "- Calcola la matrice di confusione confrontando `y_pred` con le etichette reali `y_true`.\n",
    "- Usa `.ravel()` per ottenere i singoli valori:\n",
    "  - `tn` (True Negatives): predizioni corrette della classe negativa.\n",
    "  - `fp` (False Positives): errori in cui la classe negativa √® stata predetta come positiva.\n",
    "  - `fn` (False Negatives): errori in cui la classe positiva √® stata predetta come negativa.\n",
    "  - `tp` (True Positives): predizioni corrette della classe positiva.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Recall/Sensitivity\n",
    "```\n",
    "- **True Positive Rate (TPR)** o **Recall**: misura quanti esempi positivi vengono identificati correttamente.\n",
    "  \\[$\n",
    "  \\text{Recall} = \\frac{TP}{TP + FN}$\n",
    "  \\]\n",
    "- Se `tp + fn == 0`, evita la divisione per zero e assegna `0`.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "```\n",
    "- **False Positive Rate (FPR)**: misura la percentuale di esempi negativi classificati erroneamente come positivi.\n",
    "  \\[$\n",
    "  \\text{FPR} = \\frac{FP}{FP + TN}$\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity\n",
    "```\n",
    "- **True Negative Rate (TNR)** o **Specificit√†**: misura quanti esempi negativi vengono identificati correttamente.\n",
    "  \\[$\n",
    "  \\text{Specificit√†} = \\frac{TN}{TN + FP}$\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "```\n",
    "- **Precision**: misura la proporzione di previsioni positive che sono effettivamente corrette.\n",
    "  \\[$\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP}$\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        f1 = 2 * precision * tpr / (precision + tpr) if (precision + tpr) > 0 else 0\n",
    "```\n",
    "- **F1 Score**: media armonica tra Precision e Recall.\n",
    "  \\[$\n",
    "  F1 = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "  \\]\n",
    "- Se `precision + tpr == 0`, assegna `0` per evitare la divisione per zero.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        results.append({\n",
    "            'Threshold': threshold,\n",
    "            'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn,\n",
    "            'Precision': precision,\n",
    "            'Recall (TPR)': tpr,\n",
    "            'Specificity (TNR)': tnr,\n",
    "            'FPR': fpr,\n",
    "            'F1 Score': f1\n",
    "        })\n",
    "```\n",
    "- Memorizza i risultati per la soglia corrente in `results`.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Convert to DataFrame for better display\n",
    "    results_df = pd.DataFrame(results)\n",
    "```\n",
    "- Converte `results` in un DataFrame Pandas per una migliore leggibilit√†.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    print(f\"Threshold analysis for {model_name} on {dataset_name}:\")\n",
    "    return results_df\n",
    "```\n",
    "- Stampa il nome del modello e il dataset analizzato.\n",
    "- Restituisce il DataFrame contenente i risultati.\n",
    "\n",
    "---\n",
    "\n",
    "### **Esecuzione dell'analisi per la Regressione Logistica sul dataset sbilanciato**\n",
    "```python\n",
    "precision, recall, thresholds = precision_recall_curve(imbalanced_models['Logistic Regression'][0], imbalanced_models['Logistic Regression'][1])\n",
    "```\n",
    "- `precision_recall_curve` calcola i valori di Precision e Recall per diverse soglie nel modello di Regressione Logistica sul dataset sbilanciato.\n",
    "- `thresholds` contiene le soglie usate per calcolare questi valori.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "log_reg_imb_thresholds = analyze_thresholds(\n",
    "    y_imb_test, \n",
    "    imbalanced_models['Logistic Regression'][1],\n",
    "    thresholds,\n",
    "    'Logistic Regression',\n",
    "    'Imbalanced Dataset'\n",
    ")\n",
    "```\n",
    "- Chiama `analyze_thresholds` per la regressione logistica sul dataset sbilanciato.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Display the results\n",
    "pd.set_option('display.precision', 3)\n",
    "display(log_reg_imb_thresholds[['Threshold', 'Precision', 'Recall (TPR)', 'Specificity (TNR)', 'F1 Score']])\n",
    "```\n",
    "- Imposta la precisione di visualizzazione su 3 decimali.\n",
    "- Mostra il DataFrame con i risultati principali.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretazione della tabella dei risultati**\n",
    "| Threshold | Precision | Recall (TPR) | Specificity (TNR) | F1 Score |\n",
    "|-----------|------------|-------------|-------------|-------------|\n",
    "| 0.0002186 | 0.104 | 1.000 | 0.000 | 0.188 |\n",
    "| 0.004931  | 0.114 | 0.981 | 0.121 | 0.205 |\n",
    "| 0.009824  | 0.129 | 0.969 | 0.244 | 0.228 |\n",
    "| 0.016600  | 0.150 | 0.965 | 0.368 | 0.260 |\n",
    "| 0.025630  | 0.173 | 0.927 | 0.487 | 0.291 |\n",
    "| 0.038990  | 0.210 | 0.903 | 0.608 | 0.341 |\n",
    "| 0.061710  | 0.267 | 0.861 | 0.727 | 0.408 |\n",
    "| 0.110300  | 0.370 | 0.795 | 0.843 | 0.505 |\n",
    "| 0.241300  | 0.602 | 0.649 | 0.950 | 0.625 |\n",
    "| 0.999100  | 1.000 | 0.004 | 1.000 | 0.008 |\n",
    "\n",
    "- **Basse soglie** ‚Üí Alto recall, ma bassa precisione.\n",
    "- **Alte soglie** ‚Üí Alta precisione, ma basso recall.\n",
    "- Il miglior compromesso tra Precision e Recall √® spesso individuato con l'F1-score pi√π alto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83950321-700f-414b-99df-89f64cdd214b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### **1. Definizione della funzione**\n",
    "```python\n",
    "def plot_threshold_tradeoffs(threshold_df):\n",
    "```\n",
    "- Qui definiamo una funzione chiamata `plot_threshold_tradeoffs` che accetta un solo argomento `threshold_df`, che si presume sia un DataFrame contenente i risultati dell'analisi dei threshold, con colonne come **Threshold**, **Precision**, **Recall (TPR)**, **Specificity (TNR)** e **F1 Score**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Creazione della figura**\n",
    "```python\n",
    "plt.figure(figsize=(10, 6))\n",
    "```\n",
    "- Qui creiamo una nuova figura per il grafico con dimensioni **10 pollici di larghezza e 6 pollici di altezza**, per garantire una buona leggibilit√† dei dati.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Traccia delle metriche in funzione del threshold**\n",
    "```python\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['Precision'], 'b-', label='Precision')\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['Recall (TPR)'], 'r-', label='Recall (TPR)')\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['Specificity (TNR)'], 'g-', label='Specificity (TNR)')\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['F1 Score'], 'y-', label='F1 Score')\n",
    "```\n",
    "- Qui vengono plottate **quattro curve**, ciascuna rappresentante l'andamento della rispettiva metrica al variare della soglia (`Threshold`):  \n",
    "  - **Precision** (in blu `'b-'`)\n",
    "  - **Recall (TPR)** (in rosso `'r-'`)\n",
    "  - **Specificity (TNR)** (in verde `'g-'`)\n",
    "  - **F1 Score** (in giallo `'y-'`)  \n",
    "- Ogni curva viene etichettata con un **label**, che verr√† poi usato nella legenda.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Linea verticale per il threshold di default (0.5)**\n",
    "```python\n",
    "plt.axvline(x=0.5, color='k', linestyle='--', alpha=0.3, label='Default Threshold (0.5)')\n",
    "```\n",
    "- **`plt.axvline(x=0.5, color='k', linestyle='--', alpha=0.3, label='Default Threshold (0.5)')`**  \n",
    "  - Traccia una **linea verticale nera tratteggiata** (`'--'`) in **x = 0.5** per evidenziare il threshold predefinito usato comunemente nei modelli di classificazione.\n",
    "  - `alpha=0.3` imposta la trasparenza al 30% per non rendere la linea troppo invadente.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Impostazioni degli assi e titolo**\n",
    "```python\n",
    "plt.xlabel('Classification Threshold')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('How Classification Metrics Change with Threshold')\n",
    "```\n",
    "- **`plt.xlabel('Classification Threshold')`** ‚Üí Imposta l‚Äôetichetta per l‚Äôasse X come **\"Classification Threshold\"** (cio√® la soglia di decisione per il modello).\n",
    "- **`plt.ylabel('Metric Value')`** ‚Üí Imposta l‚Äôetichetta per l‚Äôasse Y come **\"Metric Value\"** (poich√© le metriche vanno da 0 a 1).\n",
    "- **`plt.title('How Classification Metrics Change with Threshold')`** ‚Üí Imposta il titolo del grafico per indicare che stiamo analizzando come le metriche cambiano in funzione del threshold.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Aggiunta della legenda e griglia**\n",
    "```python\n",
    "plt.legend(loc='center right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "```\n",
    "- **`plt.legend(loc='center right')`** ‚Üí Posiziona la legenda nell'angolo **in alto a destra**.\n",
    "- **`plt.grid(True, alpha=0.3)`** ‚Üí Aggiunge una **griglia** con **trasparenza al 30%**, utile per facilitare la lettura del grafico.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Determinazione del threshold ottimale per F1 Score**\n",
    "```python\n",
    "f1_max_idx = threshold_df['F1 Score'].argmax()\n",
    "optimal_threshold = threshold_df.iloc[f1_max_idx]['Threshold']\n",
    "```\n",
    "- **`threshold_df['F1 Score'].argmax()`** trova l'indice della riga con il valore **massimo** di **F1 Score**.\n",
    "- **`threshold_df.iloc[f1_max_idx]['Threshold']`** estrae il valore di **Threshold** corrispondente a questo indice, che √® il **threshold ottimale** per bilanciare precisione e recall.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Evidenziazione del threshold ottimale nel grafico**\n",
    "```python\n",
    "plt.scatter(optimal_threshold, threshold_df.iloc[f1_max_idx]['F1 Score'], \n",
    "            s=100, c='black', marker='*')\n",
    "```\n",
    "- **`plt.scatter()`** aggiunge un **punto nero a forma di asterisco (`'*'`)** nel grafico per evidenziare la soglia ottimale.\n",
    "- Il parametro **s=100** imposta una grandezza del marker pari a 100 per renderlo ben visibile.\n",
    "- **`c='black'`** imposta il colore del marker a nero.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Annotazione del punto ottimale**\n",
    "```python\n",
    "plt.annotate(f'Optimal F1 Threshold: {optimal_threshold:.3f}', \n",
    "             xy=(optimal_threshold, threshold_df.iloc[f1_max_idx]['F1 Score']),\n",
    "             xytext=(optimal_threshold+0.2, threshold_df.iloc[f1_max_idx]['F1 Score']-0.1),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05, width=1.5))\n",
    "```\n",
    "- **`plt.annotate()`** aggiunge un‚Äôetichetta testuale vicino al punto ottimale.\n",
    "  - **`xy=(optimal_threshold, threshold_df.iloc[f1_max_idx]['F1 Score'])`** ‚Üí Specifica la posizione del testo (cio√® le coordinate del punto massimo di F1 Score).\n",
    "  - **`xytext=(optimal_threshold+0.2, threshold_df.iloc[f1_max_idx]['F1 Score']-0.1)`** ‚Üí Sposta il testo leggermente a destra e in basso rispetto al punto evidenziato.\n",
    "  - **`arrowprops=dict(facecolor='black', shrink=0.05, width=1.5)`** ‚Üí Aggiunge una **freccia nera** che punta al punto ottimale, con larghezza **1.5**.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Mostra il grafico**\n",
    "```python\n",
    "plt.show()\n",
    "```\n",
    "- **`plt.show()`** visualizza il grafico generato.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusione**\n",
    "Questa funzione `plot_threshold_tradeoffs()` aiuta a **visualizzare** come le metriche di classificazione (Precision, Recall, Specificity, F1 Score) variano al variare della soglia di decisione. Inoltre, evidenzia il **threshold ottimale per F1 Score**, utile per scegliere il valore migliore per il bilanciamento tra Precision e Recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4569f0-c137-4e98-866a-fa3bcea1babc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# SPIEGAZIONE IMMAGINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761bcc4-ce4c-4ebc-b99d-9446da3e4062",
   "metadata": {},
   "source": [
    "Il grafico intitolato **\"How Classification Metrics Change with Threshold\"** mostra come cambiano diverse metriche di classificazione variando la soglia di decisione di un modello di machine learning. Ecco una spiegazione dettagliata:\n",
    "\n",
    "### Assi e Componenti:\n",
    "- **Asse x**: Rappresenta la soglia di classificazione, con valori da **0.0 a 1.0**.\n",
    "- **Asse y**: Indica il valore delle metriche, che varia da **0.0 a 1.0**.\n",
    "\n",
    "### Metriche rappresentate:\n",
    "1. **Precision (linea blu)**: Misura la proporzione di previsioni positive corrette rispetto a tutte le previsioni positive.\n",
    "2. **Recall (TPR - linea rossa)**: Indica la capacit√† del modello di identificare correttamente i casi positivi reali.\n",
    "3. **Specificity (TNR - linea verde)**: Riflette la capacit√† del modello di identificare correttamente i casi negativi reali.\n",
    "4. **F1 Score (linea gialla)**: √à la media armonica tra Precision e Recall, utile per bilanciare le due metriche.\n",
    "\n",
    "### Punti chiave evidenziati:\n",
    "- **Soglia predefinita (0.5)**: Indicata da una linea tratteggiata verticale. √à il valore standard per molti modelli, ma non sempre garantisce le migliori prestazioni.\n",
    "- **Soglia ottimale per l'F1 Score (0.241)**: Indicato da una freccia nera sul grafico, √® il punto in cui il bilanciamento tra Precision e Recall massimizza l'F1 Score. Questo √® importante quando entrambe le metriche hanno uguale rilevanza.\n",
    "\n",
    "### Interpretazione:\n",
    "Il grafico dimostra che modificare la soglia pu√≤ influenzare significativamente le metriche:\n",
    "- Una **soglia pi√π bassa** (vicina a 0.0) aumenta Recall ma riduce Precision, poich√© il modello tende a classificare pi√π istanze come positive.\n",
    "- Una **soglia pi√π alta** (vicina a 1.0) migliora Precision a scapito di Recall, poich√© solo le istanze pi√π certe vengono classificate come positive.\n",
    "- Il **punto ottimale per l'F1 Score (0.241)** indica il miglior compromesso per bilanciare Precision e Recall.\n",
    "\n",
    "### Applicazione:\n",
    "Questa analisi √® essenziale per scegliere la soglia pi√π adatta agli obiettivi del modello. Ad esempio:\n",
    "- In ambito medico, dove √® cruciale rilevare ogni caso positivo, si potrebbe preferire una soglia che massimizza Recall.\n",
    "- In scenari finanziari, dove √® importante ridurre i falsi positivi, si potrebbe optare per una soglia che massimizza Precision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f743e018-4a2d-4a63-8f91-afb586392add",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### **1. Definizione del DataFrame `metrics_summary`**\n",
    "```python\n",
    "metrics_summary = pd.DataFrame({\n",
    "```\n",
    "- Qui stiamo creando un **DataFrame** chiamato `metrics_summary` utilizzando **Pandas**, che conterr√† informazioni su diverse **metriche di classificazione**, le loro **descrizioni** e i **casi in cui dovrebbero essere utilizzate**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Colonne del DataFrame**\n",
    "```python\n",
    "    'Metric': [\n",
    "        'Accuracy', 'Precision', 'Recall (Sensitivity)', 'Specificity', \n",
    "        'F1 Score', 'AUC-ROC', 'Average Precision (AP)', 'Balanced Accuracy'\n",
    "    ],\n",
    "```\n",
    "- La colonna `'Metric'` contiene una lista di **nomi delle metriche di classificazione**.\n",
    "  - **Accuracy**: La proporzione delle previsioni corrette.\n",
    "  - **Precision**: La proporzione delle previsioni positive corrette.\n",
    "  - **Recall (Sensitivity)**: La proporzione degli esempi positivi correttamente identificati.\n",
    "  - **Specificity**: La proporzione degli esempi negativi correttamente identificati.\n",
    "  - **F1 Score**: La media armonica tra precisione e recall.\n",
    "  - **AUC-ROC**: L'area sotto la curva ROC.\n",
    "  - **Average Precision (AP)**: L'area sotto la curva precision-recall.\n",
    "  - **Balanced Accuracy**: La media tra recall e specificity.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Descrizioni delle metriche**\n",
    "```python\n",
    "    'Description': [\n",
    "        'Proportion of correct predictions (TP+TN)/(TP+TN+FP+FN)',\n",
    "        'Proportion of positive predictions that are correct TP/(TP+FP)',\n",
    "        'Proportion of actual positives correctly identified TP/(TP+FN)',\n",
    "        'Proportion of actual negatives correctly identified TN/(TN+FP)',\n",
    "        'Harmonic mean of precision and recall 2*(P*R)/(P+R)',\n",
    "        'Area under the ROC curve (TPR vs FPR)',\n",
    "        'Area under the precision-recall curve',\n",
    "        'Average of recall and specificity (TPR+TNR)/2'\n",
    "    ],\n",
    "```\n",
    "- La colonna `'Description'` fornisce una **descrizione** per ogni metrica di classificazione. Ecco il dettaglio di ciascuna:\n",
    "  - **Accuracy**: La proporzione di previsioni corrette, cio√® la somma dei veri positivi (TP) e dei veri negativi (TN) divisa per il totale di esempi (TP+TN+FP+FN).\n",
    "  - **Precision**: La proporzione di previsioni positive corrette (TP / (TP + FP)), ovvero, quanto sono accurate le previsioni positive.\n",
    "  - **Recall (Sensitivity)**: La proporzione di veri positivi correttamente identificati (TP / (TP + FN)), cio√® quanto bene il modello riconosce i veri positivi.\n",
    "  - **Specificity**: La proporzione di veri negativi correttamente identificati (TN / (TN + FP)), cio√® quanto bene il modello riconosce i veri negativi.\n",
    "  - **F1 Score**: La media armonica tra precisione e recall, che cerca un equilibrio tra le due metriche (2 * (P * R) / (P + R)).\n",
    "  - **AUC-ROC**: L'area sotto la curva ROC, che misura la capacit√† di un modello di separare correttamente le classi positive e negative.\n",
    "  - **Average Precision (AP)**: L'area sotto la curva precision-recall, che √® utile nei dataset sbilanciati, per concentrarsi sulla performance della classe positiva.\n",
    "  - **Balanced Accuracy**: La media tra **recall** e **specificity**, utile per bilanciare l'importanza dei veri positivi e veri negativi.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Quando usare ciascuna metrica**\n",
    "```python\n",
    "    'When to Use': [\n",
    "        'Balanced datasets, equal misclassification costs',\n",
    "        'When false positives are costly (spam detection, content filtering)',\n",
    "        'When false negatives are costly (disease detection, fraud monitoring)',\n",
    "        'When correctly identifying negatives is important (medical screening)',\n",
    "        'When balance between precision and recall is needed, imbalanced datasets',\n",
    "        'Model comparison, balanced datasets, ranking performance',\n",
    "        'Imbalanced datasets, focus on positive class performance',\n",
    "        'Imbalanced datasets, when both classes are important'\n",
    "    ]\n",
    "```\n",
    "- La colonna `'When to Use'` specifica i **casi** in cui ciascuna metrica dovrebbe essere utilizzata:\n",
    "  - **Accuracy**: Adatta per dataset bilanciati e quando i costi di errore sono simili tra false positive e false negative.\n",
    "  - **Precision**: Utile quando i **falsi positivi** sono costosi, ad esempio nella rilevazione di spam o nella filtrazione dei contenuti.\n",
    "  - **Recall (Sensitivity)**: Utile quando i **falsi negativi** sono costosi, come nel caso della diagnosi di malattie o nel monitoraggio delle frodi.\n",
    "  - **Specificity**: Importante quando √® cruciale identificare correttamente i **negativi**, come nel caso dello screening medico.\n",
    "  - **F1 Score**: Utile quando √® necessario un buon equilibrio tra **precisione** e **recall**, specialmente in scenari con dataset sbilanciati.\n",
    "  - **AUC-ROC**: Buona per confrontare modelli, specialmente quando i dataset sono bilanciati e si vuole una panoramica delle performance.\n",
    "  - **Average Precision (AP)**: Particolarmente utile per **dataset sbilanciati**, concentrandosi sulle prestazioni della classe positiva.\n",
    "  - **Balanced Accuracy**: Quando entrambe le classi (positive e negative) sono importanti, e il dataset √® sbilanciato.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Visualizzazione del DataFrame**\n",
    "```python\n",
    "pd.set_option('display.max_colwidth',100)  \n",
    "display(metrics_summary)\n",
    "```\n",
    "- **`pd.set_option('display.max_colwidth', 100)`**: Imposta una **larghezza massima di visualizzazione** per le colonne del DataFrame a 100 caratteri, in modo da poter visualizzare interamente le descrizioni senza troncamenti.\n",
    "- **`display(metrics_summary)`**: Visualizza il **DataFrame** `metrics_summary`, che mostra le metriche di classificazione, le loro descrizioni e i contesti in cui dovrebbero essere utilizzate.\n",
    "\n",
    "---\n",
    "\n",
    "### **Risultato:**\n",
    "Il risultato finale √® una tabella che riassume le metriche di classificazione pi√π comuni e fornisce linee guida su quando usarle. Il **DataFrame** mostra:\n",
    "- Il nome della metrica\n",
    "- Una breve descrizione della metrica\n",
    "- Una guida su quando questa metrica √® pi√π utile, a seconda del tipo di problema che si sta affrontando.\n",
    "\n",
    "Questa tabella √® utile per capire quale metrica utilizzare in base al tipo di modello, dataset e agli obiettivi specifici del problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f452b971-fea2-4f0e-8ba1-a47af85f454e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Vediamo in dettaglio il significato di ciascun punto nella conclusione:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. No Single Perfect Metric**:\n",
    "- **Spiegazione**: Non esiste una metrica perfetta che sia adatta a tutti i problemi. La scelta della metrica di valutazione dipende dal contesto e dagli obiettivi specifici del problema che stai cercando di risolvere.\n",
    "- **Implicazioni**: A seconda del tipo di applicazione (es. rilevamento di malattie rare, analisi di frodi, ecc.), alcune metriche saranno pi√π rilevanti di altre. Ad esempio, in alcuni casi potresti privilegiare la **recall** per evitare falsi negativi, mentre in altri casi potresti preferire la **precisione** per evitare falsi positivi.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Understand Class Imbalance**:\n",
    "- **Spiegazione**: L'imbalance delle classi (ovvero quando una classe √® significativamente meno rappresentata nell'insieme di dati rispetto all'altra) pu√≤ influenzare la scelta delle metriche. In presenza di squilibrio, **l'accuratezza** (accurate rate) pu√≤ essere fuorviante.\n",
    "- **Implicazioni**: In un dataset sbilanciato, un modello che predice sempre la classe pi√π grande pu√≤ ottenere una **precisione elevata**, ma non sta realmente imparando a distinguere tra le classi. In questo caso, **recall**, **precisione** o **F1 Score** sono metriche migliori da considerare.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Consider the Cost of Errors**:\n",
    "- **Spiegazione**: Non tutti gli errori (falsi positivi o falsi negativi) hanno lo stesso costo nel mondo reale. Ad esempio, in un'applicazione medica, un falso negativo (ad esempio, non rilevare una malattia) potrebbe essere molto pi√π grave di un falso positivo (ad esempio, un test che segnala erroneamente una malattia).\n",
    "- **Implicazioni**: Le metriche dovrebbero essere selezionate in base ai costi reali degli errori. Se, per esempio, i falsi negativi sono pi√π costosi, √® importante concentrarsi sulla **recall** e ottimizzare per ridurre il numero di falsi negativi.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. ROC vs. PR Curves**:\n",
    "- **Spiegazione**:\n",
    "  - **ROC Curve (Receiver Operating Characteristic)**: √à utile per **dataset bilanciati** dove entrambe le classi sono ugualmente importanti. La curva traccia la **True Positive Rate** (TPR) rispetto alla **False Positive Rate** (FPR).\n",
    "  - **PR Curve (Precision-Recall)**: √à pi√π adatta per **dataset sbilanciati**, in cui la classe positiva √® pi√π importante. La curva traccia **precisione** contro **recall**.\n",
    "- **Implicazioni**: Se i dati sono **sbilanciati**, la **ROC curve** pu√≤ essere ottimista (mostrando performance elevate), ma la **PR curve** offre una visione pi√π accurata delle prestazioni del modello, specialmente quando l'accuratezza non √® il miglior indicatore.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Threshold Selection**:\n",
    "- **Spiegazione**: La scelta della soglia di classificazione (threshold) √® fondamentale, poich√© determina quando classificare una previsione come positiva o negativa. La selezione della soglia dipende dal trade-off tra le metriche (ad esempio, tra **precisione** e **recall**).\n",
    "- **Implicazioni**: Un threshold di 0.5 √® comunemente usato, ma non √® sempre il migliore. Puoi ottimizzare il threshold in base alle esigenze specifiche del problema, per esempio per minimizzare i falsi positivi o massimizzare la recall.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Beyond Binary Classification**:\n",
    "- **Spiegazione**: Le tecniche di valutazione per la classificazione **multiclasse** (quando ci sono pi√π di due classi) differiscono dalla classificazione binaria. √à necessario considerare media ponderata (ad esempio **macro**, **micro**, **weighted averages**) per aggregare le performance per ogni classe.\n",
    "- **Implicazioni**: Per i problemi con pi√π di due classi, √® importante usare metriche che considerino le prestazioni globali del modello, come la **matrice di confusione** multiclasse o **Cohen's Kappa**, che misura l'accordo tra le etichette predette e quelle reali.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Evaluate with Multiple Metrics**:\n",
    "- **Spiegazione**: Non dovresti mai basarti su una singola metrica per valutare il modello. Combinare diverse metriche ti permette di avere una visione pi√π completa delle prestazioni del modello.\n",
    "- **Implicazioni**: Per esempio, una **precisione elevata** potrebbe sembrare buona, ma potrebbe nascondere problemi legati alla **recall** bassa. Quindi, √® importante considerare **F1 score**, **recall**, **precisione** e **specificit√†** in combinazione per avere un quadro completo delle prestazioni.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Context Matters**:\n",
    "- **Spiegazione**: Le metriche devono essere sempre interpretate nel contesto specifico del problema che stai affrontando. Ad esempio, in alcuni scenari, un falso positivo pu√≤ avere conseguenze minori rispetto a un falso negativo, o viceversa.\n",
    "- **Implicazioni**: Ogni dominio applicativo ha una sua **tolleranza agli errori** e una sua **importanza relativa delle classi**. I modelli dovrebbero essere valutati sulla base del contesto specifico, non solo sulle metriche generali.\n",
    "\n",
    "---\n",
    "\n",
    "### **Riepilogo e Implicazioni Generali**:\n",
    "Questi 8 punti sottolineano che la scelta delle metriche di valutazione deve essere guidata dai **requisiti del problema**, dalla **distribuzione dei dati** e dalle **conseguenze reali degli errori**. √à essenziale evitare di ottimizzare per una singola metrica, ma piuttosto di considerare una combinazione di metriche, interpretando sempre i risultati nel contesto pratico dell'applicazione. In particolare, le **metriche per dataset sbilanciati** e la **scelta del threshold** sono critiche per una valutazione accurata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4676980-b891-4092-a4da-3c4ea8aefc2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1.7 Real World Scenario example: Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c2ae0-f3b6-45ea-8317-b22fbd5ccb07",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Il codice  simula un caso di **rilevamento delle frodi** in un contesto aziendale e dimostra come la scelta della soglia di decisione (threshold) influenzi il risultato dell'algoritmo in relazione ai costi aziendali. Vediamo il funzionamento riga per riga:\n",
    "\n",
    "### Funzione `threshold_business_impact_simulation`\n",
    "\n",
    "1. **Definizione della funzione**\n",
    "   La funzione simula un caso di rilevamento frodi e calcola l'impatto economico di diverse soglie (thresholds) di decisione. Accetta tre parametri:\n",
    "   - `avg_transaction`: l'importo medio di una transazione.\n",
    "   - `fraud_cost_multiple`: un moltiplicatore che indica quanto costa una frode non rilevata.\n",
    "   - `review_cost`: il costo per rivedere una transazione sospetta.\n",
    "\n",
    "2. **Creazione dei dati sintetici**\n",
    "   Vengono simulati dati per una popolazione di 100.000 transazioni:\n",
    "   - Si assume che il 5% delle transazioni siano fraudolente, quindi `n_fraud = 5% di 100,000 = 5,000`.\n",
    "   - Le probabilit√† di frode (`fraud_probs`) vengono generate tramite una distribuzione beta, che rappresenta la probabilit√† di essere fraudolente per ogni transazione.\n",
    "   - Le probabilit√† di transazioni legittime (`legitimate_probs`) vengono generate tramite un'altra distribuzione beta.\n",
    "   \n",
    "   Dopo aver generato le probabilit√† per entrambe le classi (fraudolenti e legittime), vengono combinate, e il risultato viene mischiato per simulare un dataset casuale.\n",
    "\n",
    "3. **Funzione `calculate_impact`**\n",
    "   Questa funzione calcola l'impatto aziendale di una data soglia di decisione, cio√® come cambiano le metriche di prestazione del modello (come precision, recall) e i costi aziendali (costi di frodi non rilevate e costi di revisione).\n",
    "   - `y_pred`: viene calcolato se la probabilit√† di una transazione √® maggiore o uguale alla soglia (`threshold`).\n",
    "   - La **matrice di confusione** (`tn`, `fp`, `fn`, `tp`) viene calcolata per determinare quante transazioni sono state correttamente identificate come fraudolente (True Positives), quante sono state erroneamente identificate come fraudolente (False Positives), ecc.\n",
    "   \n",
    "   Successivamente, vengono calcolati:\n",
    "   - **Costo delle frodi non rilevate** (`undetected_fraud_cost`): si moltiplica il numero di frodi non rilevate per l'importo medio della transazione e il moltiplicatore del costo.\n",
    "   - **Costo delle revisioni** (`review_cost_total`): si moltiplica il numero totale di transazioni da rivedere (True Positives + False Positives) per il costo di revisione.\n",
    "   - **Costo totale**: somma del costo delle frodi non rilevate e delle revisioni.\n",
    "\n",
    "4. **Calcolo degli impatti per diverse soglie**\n",
    "   La funzione calcola gli impatti per soglie che vanno da 0.1 a 0.9 (17 valori in totale) utilizzando il metodo `np.linspace`.\n",
    "\n",
    "5. **Creazione dei grafici**\n",
    "   - **Grafico 1**: Precisione e Recall vs Soglia. Viene tracciato come le due metriche (Precision e Recall) cambiano all'aumentare della soglia.\n",
    "     - Precisione aumenta quando si sceglie una soglia pi√π alta (si riducono i falsi positivi), ma il richiamo diminuisce.\n",
    "     - Un'**altra linea verticale** (`axvline`) indica la soglia ottimale per minimizzare il costo totale.\n",
    "   - **Grafico 2**: Costi vs Soglia. Si tracciano i costi di frodi non rilevate, i costi di revisione e il costo totale in funzione della soglia.\n",
    "     - Aumentando la soglia, i costi di frodi non rilevate aumentano (maggiori falsi negativi), mentre i costi di revisione diminuiscono.\n",
    "\n",
    "6. **Ricerca della soglia ottimale**\n",
    "   Viene identificata la soglia che minimizza il **costo totale** (somma di costi di frodi non rilevate e di revisione). Questa soglia viene utilizzata come punto di riferimento nei grafici e nelle analisi.\n",
    "\n",
    "7. **Analisi dei risultati**\n",
    "   Il codice stampa una **tabella comparativa** con:\n",
    "   - **Threshold**: Soglia.\n",
    "   - **Precision**: Percentuale di predizioni corrette tra le transazioni segnate come fraudolente.\n",
    "   - **Recall**: Percentuale di transazioni fraudolente effettivamente identificate.\n",
    "   - **False Positives**: Numero di transazioni legittime erroneamente identificate come frodi.\n",
    "   - **False Negatives**: Numero di transazioni fraudolente non identificate.\n",
    "   - **Total Cost**: Il costo totale associato alla scelta di quella soglia.\n",
    "\n",
    "8. **Confronto con le soglie F1-optimal e Cost-optimal**\n",
    "   - **Soglia ottimale F1 score**: Si calcola il valore ottimale per F1 score (un bilanciamento tra precisione e recall), ma si dimostra che questa non porta necessariamente al costo ottimale dal punto di vista aziendale.\n",
    "   - **Soglia ottimale costo**: Il punto in cui il costo totale (composto dai costi di frodi non rilevate e di revisione) √® minimo.\n",
    "\n",
    "9. **Conclusioni**\n",
    "   - Viene evidenziato che la soglia ottimale dal punto di vista dei costi non coincide con quella ottimale per il F1 score.\n",
    "   - La scelta della soglia dipende dai costi aziendali specifici (costi di frodi non rilevate e di revisione) e pu√≤ variare a seconda dei cambiamenti nei costi di questi fattori.\n",
    "\n",
    "### Risultato finale:\n",
    "Il codice visualizza i grafici che mostrano l'andamento delle metriche (precisione, recall, costi) in funzione della soglia. Inoltre, vengono forniti i dettagli numerici in una tabella, che confronta diverse soglie in termini di prestazioni e costi. \n",
    "\n",
    "Il messaggio chiave √® che **la scelta della soglia dovrebbe essere basata sulle necessit√† aziendali e sui costi associati agli errori di classificazione**, non solo su metriche come F1 score, che potrebbero non essere la scelta migliore per l'ottimizzazione dei costi aziendali."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05c3365-a69b-47ed-b4db-ccb46654d7b6",
   "metadata": {},
   "source": [
    "Il 1  grafico mostra l'evoluzione di **Precision** e **Recall** in base al valore della soglia di decisione di un modello di classificazione.\n",
    "\n",
    "### Dettagli principali:\n",
    "- **Asse x**: La soglia di decisione, variabile da **0.0 a 0.9**, rappresenta il limite oltre cui il modello classifica le istanze come positive.\n",
    "- **Asse y**: Il valore delle metriche, variabile da **0.0 a 1.0**, che rappresenta la Precision e il Recall.\n",
    "- **Linea verde (Precision)**: Mostra come la proporzione di previsioni positive corrette cambia aumentando la soglia.\n",
    "- **Linea rossa (Recall)**: Indica come la capacit√† del modello di rilevare correttamente i positivi varia in base alla soglia.\n",
    "- **Linea tratteggiata verticale (Soglia ottimale = 0.35)**: Evidenzia il punto in cui il bilanciamento tra Precision e Recall √® considerato ottimale dal punto di vista del costo o degli obiettivi.\n",
    "\n",
    "### Interpretazione:\n",
    "- **Precision**:\n",
    "  Aumenta con soglie pi√π alte. Questo perch√© il modello diventa pi√π selettivo, producendo meno falsi positivi. Tuttavia, ci√≤ riduce il numero complessivo di previsioni positive.\n",
    "- **Recall**:\n",
    "  Diminuisce con soglie pi√π alte. Quando il modello √® pi√π selettivo, pu√≤ \"perdere\" pi√π casi positivi, riducendo il tasso di rilevamento.\n",
    "- **Trade-off**: \n",
    "  Precision e Recall sono inversamente proporzionali. Un valore di soglia pi√π basso aumenta il Recall a scapito della Precision, e viceversa.\n",
    "\n",
    "### Significato della soglia ottimale (0.35):\n",
    "- Questo valore rappresenta un compromesso ideale tra Precision e Recall, spesso determinato in base alle esigenze specifiche di applicazione, come nel bilanciamento tra costi associati ai falsi positivi e falsi negativi.\n",
    "\n",
    "In sintesi, il grafico √® uno strumento utile per scegliere la soglia pi√π adatta in base agli obiettivi del problema di classificazione. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b34699-c36f-440a-a3b4-eb6c2cb44e1a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Il 2 grafico mostra la relazione tra i **costi aziendali** e il **valore della soglia** in un contesto di rilevamento di frodi o classificazione. L'obiettivo √® identificare il **valore di soglia ottimale** per minimizzare i costi totali. Vediamo i punti principali:\n",
    "\n",
    "### Assi e Curve:\n",
    "- **Asse X**: Valori di soglia, che vanno da 0.1 a 0.9. Rappresentano il limite oltre cui un caso viene classificato come positivo (ad esempio, frode rilevata).\n",
    "- **Asse Y**: I costi, misurati in migliaia di dollari.\n",
    "- **Curve rappresentate**:\n",
    "  1. **Undetected Fraud Cost (Curva rossa)**: Cresce all'aumentare della soglia. Un valore di soglia pi√π alto significa che si rischia di non rilevare pi√π frodi, aumentando cos√¨ i costi delle frodi non individuate.\n",
    "  2. **Review Cost (Curva verde)**: Diminuisce all'aumentare della soglia. Un valore di soglia pi√π basso comporta pi√π revisioni manuali, incrementando i costi.\n",
    "  3. **Total Cost (Curva blu)**: Rappresenta la somma dei costi di frodi non individuate e dei costi di revisione. Questa curva forma una \"U\", raggiungendo un **minimo** al valore di soglia ottimale.\n",
    "\n",
    "### Soglia Ottimale:\n",
    "- La **linea tratteggiata verticale** evidenzia la **soglia ottimale** (0.35), dove il **Total Cost** √® al suo minimo.\n",
    "- **Punto evidenziato sulla curva blu**: Indica il costo minimo totale, che bilancia in modo ideale i costi di revisione e i costi delle frodi non rilevate.\n",
    "\n",
    "### Analisi dell'Impatto Aziendale:\n",
    "Il testo sotto il grafico conferma che:\n",
    "- La soglia ottimale per minimizzare il costo totale √® **0.35**.\n",
    "- Questo bilanciamento √® fondamentale per ridurre al minimo le spese aziendali, mantenendo un'efficace rilevazione delle frodi.\n",
    "\n",
    "### Conclusione:\n",
    "Il grafico dimostra che scegliere la soglia giusta ha un impatto significativo sui costi aziendali. Un'analisi accurata come questa pu√≤ aiutare a ottimizzare le risorse e migliorare l'efficienza operativa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f077d-e2ed-44a4-828a-b54e43b2a61b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff21f9c7-ac8a-4e6a-a5e2-f5e8611704bb",
   "metadata": {},
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef4b78-7730-49ff-83e9-08045e824021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c033499e-6623-4467-9549-915e9cda1cdf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.1 Setting Up Our Environment\n",
    "\n",
    "**1. Importazione delle librerie principali per la manipolazione dei dati e la visualizzazione:**\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "```\n",
    "- **`numpy`**: libreria fondamentale per il calcolo numerico, utilizzata per operazioni su array.\n",
    "- **`pandas`**: libreria per la manipolazione e analisi dei dati, particolarmente utile per lavorare con DataFrame.\n",
    "- **`matplotlib.pyplot`**: libreria per creare visualizzazioni e grafici.\n",
    "- **`seaborn`**: libreria per la visualizzazione dei dati basata su `matplotlib`, ma con stili e funzionalit√† pi√π avanzate.\n",
    "\n",
    "**2. Importazione delle librerie per il machine learning:**\n",
    "```python\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, learning_curve\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "```\n",
    "- **`make_regression`**: funzione che genera dati sintetici per un problema di regressione.\n",
    "- **`train_test_split`**: funzione che suddivide i dati in set di addestramento, validazione e test.\n",
    "- **`cross_val_score`**, **`KFold`**, **`GridSearchCV`**: utilizzati per la validazione incrociata, la ricerca dei migliori iperparametri e la divisione dei dati in \"fold\" per la cross-validation.\n",
    "- **`LinearRegression`, `Ridge`**: modelli di regressione lineare, con `Ridge` che √® una versione regolarizzata della regressione lineare.\n",
    "- **`RandomForestRegressor`**: modello di regressione basato su una foresta di alberi decisionali (Random Forest).\n",
    "- **`DecisionTreeRegressor`**: modello di regressione che utilizza un albero decisionale.\n",
    "- **`make_pipeline`**: utile per concatenare pi√π passaggi in un solo flusso, come pre-processing e modello.\n",
    "- **`PolynomialFeatures`, `StandardScaler`**: strumenti per creare caratteristiche polinomiali e scalare i dati.\n",
    "\n",
    "**3. Importazione delle metriche per la valutazione della regressione:**\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "```\n",
    "- **`mean_squared_error`**: errore quadratico medio, una delle principali metriche per la regressione.\n",
    "- **`mean_absolute_error`**: errore assoluto medio, che misura la media delle differenze assolute tra le previsioni e i valori reali.\n",
    "- **`r2_score`**: coefficiente di determinazione, che misura la bont√† del modello (quanto bene il modello spiega la varianza dei dati).\n",
    "- **`mean_absolute_percentage_error`**: errore percentuale medio assoluto, che misura l'errore medio in termini percentuali.\n",
    "\n",
    "**4. Per visualizzare le grafiche in-linea nel notebook:**\n",
    "```python\n",
    "%matplotlib inline\n",
    "```\n",
    "- Comando specifico per Jupyter notebook che consente di visualizzare direttamente i grafici senza dover chiamare `plt.show()`.\n",
    "\n",
    "**5. Impostazione di un seme per la generazione dei numeri casuali per garantire la riproducibilit√†:**\n",
    "```python\n",
    "np.random.seed(42)\n",
    "```\n",
    "- Imposta un seme fisso per i numeri casuali, in modo che i risultati siano riproducibili ogni volta che il codice viene eseguito.\n",
    "\n",
    "**6. Impostazione dello stile delle grafiche:**\n",
    "```python\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "```\n",
    "- Imposta lo stile grafico `whitegrid` tramite `seaborn`, che applica uno sfondo bianco con griglia. \n",
    "- Modifica la dimensione predefinita delle figure per essere pi√π grandi (12x8 pollici).\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Creating Sample Data\n",
    "\n",
    "**7. Creazione di un dataset di regressione sintetico:**\n",
    "```python\n",
    "X, y = make_regression(n_samples=10000, n_features=20, n_informative=10, \n",
    "                        noise=20, random_state=42)\n",
    "```\n",
    "- **`make_regression`**: genera un dataset sintetico per la regressione. In questo caso:\n",
    "  - `n_samples=10000`: crea 10.000 campioni.\n",
    "  - `n_features=20`: 20 variabili indipendenti.\n",
    "  - `n_informative=10`: solo 10 delle 20 caratteristiche sono informative, le altre sono \"rumore\".\n",
    "  - `noise=20`: aggiunge un po' di rumore ai dati per simulare incertezza nei dati reali.\n",
    "  - `random_state=42`: imposta un seme fisso per garantire che il dataset sia generato in modo riproducibile.\n",
    "\n",
    "**8. Suddivisione dei dati in set di addestramento, validazione e test:**\n",
    "```python\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "- **`train_test_split`**: divide i dati in un set di addestramento e un set di test.\n",
    "  - `test_size=0.2`: il 20% dei dati viene destinato al set di test.\n",
    "  - `random_state=42`: imposta un seme per garantire che la divisione sia sempre la stessa.\n",
    "\n",
    "**9. Ulteriore suddivisione del set di addestramento/validazione in set di addestramento e validazione:**\n",
    "```python\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "```\n",
    "- **`train_test_split`**: divide ulteriormente il set di addestramento/validazione, assegnando il 25% del set a un set di validazione (questo porta ad un totale di 60% addestramento, 20% validazione, 20% test).\n",
    "\n",
    "**10. Stampa delle dimensioni dei set:**\n",
    "```python\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "```\n",
    "- Stampa il numero di campioni in ciascun set (addestramento, validazione, test).\n",
    "\n",
    "**11. Allenamento di un modello di regressione lineare:**\n",
    "```python\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "```\n",
    "- **`LinearRegression`**: crea un modello di regressione lineare.\n",
    "- **`fit(X_train, y_train)`**: allena il modello sui dati di addestramento.\n",
    "\n",
    "**12. Allenamento di un modello di Random Forest:**\n",
    "```python\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "```\n",
    "- **`RandomForestRegressor`**: crea un modello di regressione basato su Random Forest con 100 alberi.\n",
    "- **`fit(X_train, y_train)`**: allena il modello sui dati di addestramento.\n",
    "\n",
    "**13. Allenamento di un modello di Decision Tree:**\n",
    "```python\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "```\n",
    "- **`DecisionTreeRegressor`**: crea un modello di regressione basato su un albero decisionale.\n",
    "- **`fit(X_train, y_train)`**: allena il modello sui dati di addestramento.\n",
    "\n",
    "**14. Creazione di un dizionario per memorizzare i modelli:**\n",
    "```python\n",
    "models = {\n",
    "    'Linear Regression': linear_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'Decision Tree': dt_model\n",
    "}\n",
    "```\n",
    "- Crea un dizionario per memorizzare i modelli allenati (Regressione Lineare, Random Forest, e Decision Tree), in modo da poterli facilmente confrontare.\n",
    "\n",
    "In sintesi, il codice prepara l'ambiente di lavoro, crea un dataset di regressione sintetico, divide i dati in set di addestramento, validazione e test, allena tre modelli di regressione (lineare, Random Forest e Decision Tree) e infine memorizza questi modelli per usi successivi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31093897-057a-4bad-bbf5-8d426ee8b818",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### 2.3. Funzione `evaluate_model`\n",
    "\n",
    "```python\n",
    "def evaluate_model(model, X, y, model_name):\n",
    "    \"\"\"Evaluate a regression model using multiple metrics\"\"\"\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X)\n",
    "```\n",
    "Questa funzione si occupa di valutare un modello di regressione. La funzione prende in input un modello, un set di dati di input `X`, i valori reali di output `y`, e il nome del modello per la stampa dei risultati. La prima cosa che fa √® ottenere le previsioni del modello sui dati di input `X` utilizzando il metodo `.predict()`.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    # Calculate various metrics\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y, predictions)\n",
    "    r2 = r2_score(y, predictions)\n",
    "    mape = mean_absolute_percentage_error(y, predictions)\n",
    "```\n",
    "A questo punto, vengono calcolate varie metriche di valutazione per il modello:\n",
    "\n",
    "- **MSE (Mean Squared Error)**: L'errore quadratico medio, che misura la media dei quadrati degli errori tra le previsioni e i valori reali. Un valore pi√π basso √® migliore.\n",
    "- **RMSE (Root Mean Squared Error)**: La radice quadrata dell'MSE, che fornisce una misura dell'errore medio in termini di unit√† di misura originali. √à pi√π interpretabile rispetto all'MSE.\n",
    "- **MAE (Mean Absolute Error)**: L'errore assoluto medio, che calcola la media delle differenze assolute tra le previsioni e i valori reali.\n",
    "- **R¬≤ (R-squared)**: Il coefficiente di determinazione, che indica quanto bene il modello si adatta ai dati. Un valore pi√π vicino a 1 indica un buon adattamento.\n",
    "- **MAPE (Mean Absolute Percentage Error)**: L'errore medio assoluto in termini percentuali, che misura l'errore relativo in percentuale.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    print(f\"--- {model_name} Evaluation ---\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}\\n\")\n",
    "```\n",
    "Queste righe stampano i risultati di tutte le metriche calcolate per il modello. Il formato di stampa √® strutturato per mostrare ogni metrica e il suo valore con una specifica precisione decimale. Le metriche vengono visualizzate con i valori di errore (MSE, RMSE, MAE) e l'indice di adattamento (R¬≤ e MAPE).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "```\n",
    "Alla fine della funzione, viene restituito un dizionario con i risultati di valutazione, inclusi il nome del modello, le metriche calcolate e le previsioni effettuate dal modello. Questo dizionario permette di salvare e analizzare i risultati successivamente.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Valutazione dei modelli\n",
    "\n",
    "```python\n",
    "# Evaluate all models on the validation set\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    result = evaluate_model(model, X_val, y_val, name)\n",
    "    results.append(result)\n",
    "```\n",
    "Qui, viene eseguita la valutazione su un insieme di modelli. I modelli sono contenuti in un dizionario chiamato `models`, e per ciascun modello viene chiamata la funzione `evaluate_model`, passando i dati di validazione (`X_val` e `y_val`) e il nome del modello. I risultati per ogni modello vengono aggiunti alla lista `results`.\n",
    "\n",
    "---\n",
    "\n",
    "### Risultati della valutazione per ciascun modello\n",
    "\n",
    "1. **Valutazione del modello di Regressione Lineare (Linear Regression)**\n",
    "\n",
    "```\n",
    "--- Linear Regression Evaluation ---\n",
    "Mean Squared Error (MSE): 418.23\n",
    "Root Mean Squared Error (RMSE): 20.45\n",
    "Mean Absolute Error (MAE): 16.40\n",
    "R¬≤ Score: 0.9896\n",
    "Mean Absolute Percentage Error (MAPE): 0.5347\n",
    "```\n",
    "- **MSE**: 418.23, che indica che la media dei quadrati degli errori √® relativamente bassa, il che suggerisce che il modello si adatta bene ai dati.\n",
    "- **RMSE**: 20.45, che indica un errore medio di circa 20.45 unit√†.\n",
    "- **MAE**: 16.40, che indica che in media l'errore assoluto per previsione √® di circa 16.40 unit√†.\n",
    "- **R¬≤**: 0.9896, che indica che il modello spiega il 98.96% della varianza nei dati. Questo √® un eccellente indice di adattamento.\n",
    "- **MAPE**: 0.5347%, che indica un errore medio relativo molto basso, suggerendo una buona previsione in termini percentuali.\n",
    "\n",
    "2. **Valutazione del modello Random Forest**\n",
    "\n",
    "```\n",
    "--- Random Forest Evaluation ---\n",
    "Mean Squared Error (MSE): 5830.37\n",
    "Root Mean Squared Error (RMSE): 76.36\n",
    "Mean Absolute Error (MAE): 59.74\n",
    "R¬≤ Score: 0.8554\n",
    "Mean Absolute Percentage Error (MAPE): 1.3386\n",
    "```\n",
    "- **MSE**: 5830.37, che √® molto pi√π alto rispetto al modello di regressione lineare, suggerendo che il modello potrebbe non essere adattato perfettamente ai dati.\n",
    "- **RMSE**: 76.36, che √® pi√π alto rispetto al modello di regressione lineare, indicando un errore maggiore.\n",
    "- **MAE**: 59.74, anche questo √® pi√π alto rispetto alla regressione lineare, indicando errori maggiori.\n",
    "- **R¬≤**: 0.8554, che √® inferiore a quello della regressione lineare, ma comunque un buon valore che indica che il modello spiega l'85.54% della varianza.\n",
    "- **MAPE**: 1.3386%, che √® maggiore rispetto al modello di regressione lineare, indicando un errore relativo pi√π elevato.\n",
    "\n",
    "3. **Valutazione del modello Decision Tree**\n",
    "\n",
    "```\n",
    "--- Decision Tree Evaluation ---\n",
    "Mean Squared Error (MSE): 17532.23\n",
    "Root Mean Squared Error (RMSE): 132.41\n",
    "Mean Absolute Error (MAE): 104.39\n",
    "R¬≤ Score: 0.5652\n",
    "Mean Absolute Percentage Error (MAPE): 2.3300\n",
    "```\n",
    "- **MSE**: 17532.23, che √® molto pi√π alto rispetto agli altri modelli, indicando una scarsa previsione.\n",
    "- **RMSE**: 132.41, che √® molto alto, suggerendo che le previsioni sono imprecise.\n",
    "- **MAE**: 104.39, che √® significativamente pi√π alto rispetto agli altri modelli.\n",
    "- **R¬≤**: 0.5652, che √® basso e indica che solo il 56.52% della varianza viene spiegata dal modello. Questo √® un segno che il modello non si adatta bene ai dati.\n",
    "- **MAPE**: 2.3300%, che √® pi√π alto rispetto agli altri modelli, indicando una previsione meno accurata in termini relativi.\n",
    "\n",
    "---\n",
    "\n",
    "In sintesi, il modello di regressione lineare risulta essere il pi√π performante, con MSE, RMSE, MAE, R¬≤ e MAPE che indicano un buon adattamento ai dati, mentre gli altri modelli (Random Forest e Decision Tree) mostrano risultati peggiori, soprattutto in termini di errori assoluti e percentuali."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb867c2b-6b56-4633-8a2b-d2f5e27d75c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### 1. **Definizione dei metriche di regressione**\n",
    "\n",
    "Prima, ci viene fornita una panoramica teorica di ciascuna delle metriche di regressione:\n",
    "\n",
    "#### **1.1 Mean Squared Error (MSE)**\n",
    "```text\n",
    "- Calcola la media delle differenze quadrate tra le previsioni e i valori reali\n",
    "- Penalizza fortemente gli errori grandi a causa del quadrato\n",
    "- Valori pi√π bassi sono migliori, con 0 che rappresenta un punteggio perfetto\n",
    "- Formula: MSE = (1/n) * Œ£ (y_actual - y_predicted)¬≤\n",
    "```\n",
    "- **Cos'√®**: L‚Äôerrore quadratico medio (MSE) calcola la media delle differenze quadrate tra i valori predetti dal modello e i valori reali. I grandi errori sono penalizzati pi√π pesantemente a causa del quadrato, rendendo il MSE molto sensibile agli outliers.\n",
    "- **Interpretazione**: Valori di MSE pi√π bassi sono desiderabili, poich√© indicano che le previsioni sono vicine ai valori reali. MSE uguale a zero significa che il modello ha previsto perfettamente i dati.\n",
    "\n",
    "#### **1.2 Root Mean Squared Error (RMSE)**\n",
    "```text\n",
    "- Radice quadrata dell'MSE\n",
    "- Ha le stesse unit√† della variabile target, rendendolo pi√π interpretabile\n",
    "- Ancora sensibile agli outlier\n",
    "- Valori pi√π bassi sono migliori\n",
    "- Formula: RMSE = ‚àöMSE\n",
    "```\n",
    "- **Cos'√®**: La radice quadrata dell‚ÄôMSE fornisce un errore nella stessa unit√† di misura del target, che rende il valore pi√π interpretabile rispetto all'MSE. \n",
    "- **Interpretazione**: Come per l'MSE, valori pi√π bassi sono desiderabili. La differenza principale con l'MSE √® che il RMSE ha lo stesso significato delle unit√† originali, il che lo rende pi√π facilmente interpretabile.\n",
    "\n",
    "#### **1.3 Mean Absolute Error (MAE)**\n",
    "```text\n",
    "- Media delle differenze assolute tra le previsioni e i valori reali\n",
    "- Meno sensibile agli outlier rispetto a MSE/RMSE\n",
    "- Nella stessa unit√† della variabile target\n",
    "- Valori pi√π bassi sono migliori\n",
    "- Formula: MAE = (1/n) * Œ£ |y_actual - y_predicted|\n",
    "```\n",
    "- **Cos'√®**: Il MAE calcola la media delle differenze assolute tra i valori predetti e i valori reali. Poich√© non eleva al quadrato gli errori, √® meno sensibile agli outlier rispetto a MSE o RMSE.\n",
    "- **Interpretazione**: Valori pi√π bassi di MAE sono desiderabili, ma si trova spesso che MSE o RMSE vengano preferiti poich√© penalizzano maggiormente gli errori gravi.\n",
    "\n",
    "#### **1.4 R¬≤ Score (Coefficient of Determination)**\n",
    "```text\n",
    "- Rappresenta la proporzione della varianza del target che √® prevedibile dalle caratteristiche\n",
    "- I valori vanno generalmente da 0 a 1 (1 significa previsione perfetta)\n",
    "- Pu√≤ essere negativo se il modello √® peggiore di una linea orizzontale\n",
    "- Formula: R¬≤ = 1 - (Somma dei residui¬≤ / Somma totale dei quadrati)\n",
    "```\n",
    "- **Cos'√®**: L‚ÄôR¬≤ misura quanto bene le variabili indipendenti (le caratteristiche) spiegano la variabilit√† della variabile dipendente (target). Un valore R¬≤ vicino a 1 indica una buona previsione.\n",
    "- **Interpretazione**: Un R¬≤ di 0 significa che il modello non spiega nessuna della variabilit√† nel target. Un R¬≤ negativo indica che il modello √® peggiore di un modello che semplicemente predice la media.\n",
    "\n",
    "#### **1.5 Mean Absolute Percentage Error (MAPE)**\n",
    "```text\n",
    "- Media degli errori percentuali assoluti\n",
    "- Indipendente dalla scala, il che permette di fare confronti tra dataset differenti\n",
    "- Pu√≤ essere fuorviante quando i valori reali sono vicini allo zero\n",
    "- Valori pi√π bassi sono migliori\n",
    "- Formula: MAPE = (100% / n) * Œ£ |(y_actual - y_predicted) / y_actual|\n",
    "```\n",
    "- **Cos'√®**: MAPE calcola la media degli errori assoluti in percentuale. √à utile per confrontare errori tra modelli o dataset con diverse scale.\n",
    "- **Interpretazione**: Valori pi√π bassi di MAPE sono desiderabili, ma questa metrica pu√≤ essere fuorviante quando i valori reali sono molto piccoli (vicini a zero).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Visualizzazione delle previsioni rispetto ai valori reali**\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(15, 10))\n",
    "```\n",
    "- **Cos'√®**: Questa riga imposta la dimensione della figura del grafico, rendendola 15 pollici di larghezza e 10 pollici di altezza per una visualizzazione pi√π chiara.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Loop sui risultati per visualizzare le previsioni**\n",
    "```python\n",
    "for i, result in enumerate(results):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.scatter(y_val, result['predictions'], alpha=0.5)\n",
    "    plt.plot([-300, 300], [-300, 300], 'r--')  # Perfect prediction line\n",
    "    plt.title(f\"{result['model']} Predictions\")\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.xlim([-300, 300])\n",
    "    plt.ylim([-300, 300])\n",
    "```\n",
    "- **Cos'√®**: Viene eseguito un ciclo su ciascun modello presente in `results`. Per ogni modello, si crea un grafico di dispersione (scatter plot) dove le previsioni (`predictions`) vengono messe sull'asse delle ordinate (y) e i valori reali (`y_val`) sull'asse delle ascisse (x).\n",
    "- **Subplot**: Ogni grafico viene disegnato in una sottosezione di una griglia 2x2 (`plt.subplot(2, 2, i+1)`), quindi ci saranno 4 grafici totali.\n",
    "- **Alpha**: Imposta la trasparenza dei punti del grafico, in modo che i punti sovrapposti siano visibili.\n",
    "- **Linea rossa tratteggiata**: La linea rossa rappresenta una previsione perfetta, dove i valori predetti sono uguali ai valori reali. Se i punti si allineano a questa linea, significa che il modello ha fatto previsioni accurate.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Impostazione degli assi e titolo**\n",
    "```python\n",
    "    plt.title(f\"{result['model']} Predictions\")\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.xlim([-300, 300])\n",
    "    plt.ylim([-300, 300])\n",
    "```\n",
    "- **Cos'√®**: Imposta il titolo del grafico, le etichette degli assi (valori reali e valori predetti) e i limiti degli assi (da -300 a 300 in entrambi gli assi).\n",
    "- **Interpretazione**: Impostare limiti sugli assi consente di avere una scala uniforme per ogni grafico, facilitando il confronto visivo tra i modelli.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Layout del grafico**\n",
    "```python\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "- **Cos'√®**: `tight_layout()` assicura che i grafici non si sovrappongano tra loro e siano ben distanziati. `plt.show()` visualizza effettivamente i grafici.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusioni\n",
    "\n",
    "Questo codice fornisce una visualizzazione chiara di come le previsioni dei modelli si confrontano con i valori reali. Una linea retta ideale indica previsioni perfette. Pi√π i punti si discostano dalla linea, peggiore √® la previsione del modello. La visualizzazione permette di interpretare facilmente la qualit√† delle previsioni, in combinazione con le metriche numeriche (MSE, RMSE, MAE, R¬≤, MAPE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce722aae-f0aa-43ec-85cf-67e6fcaedf48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "L'immagine contiene tre grafici a dispersione che confrontano i valori previsti con quelli effettivi per tre modelli di machine learning: **Linear Regression**, **Random Forest**, e **Decision Tree**. Ogni grafico include una linea tratteggiata rossa che rappresenta lo scenario ideale, dove i valori previsti coincidono perfettamente con quelli effettivi (linea y = x).\n",
    "\n",
    "### Analisi dei grafici:\n",
    "\n",
    "1. **Linear Regression Predictions (in alto a sinistra):**\n",
    "   - Mostra una forte relazione lineare tra i valori previsti e quelli effettivi.\n",
    "   - I punti sono ben raggruppati intorno alla linea tratteggiata, suggerendo previsioni pi√π accurate rispetto agli altri modelli.\n",
    "\n",
    "2. **Random Forest Predictions (in alto a destra):**\n",
    "   - Anche qui √® presente una relazione lineare, ma con maggiore dispersione rispetto al modello di regressione lineare.\n",
    "   - Questo suggerisce che il modello √® meno preciso, pur mantenendo una certa coerenza nelle previsioni.\n",
    "\n",
    "3. **Decision Tree Predictions (in basso):**\n",
    "   - I punti sono notevolmente pi√π dispersi rispetto agli altri due modelli.\n",
    "   - Questo indica una minore accuratezza delle previsioni e una possibile tendenza al sovra-adattamento ai dati di addestramento.\n",
    "\n",
    "### Conclusioni:\n",
    "- **Linear Regression** sembra essere il modello pi√π accurato, con previsioni che si avvicinano maggiormente ai valori effettivi.\n",
    "- **Random Forest** mostra prestazioni accettabili ma leggermente inferiori in termini di precisione rispetto alla regressione lineare.\n",
    "- **Decision Tree** √® il meno performante, con una dispersione significativa che indica scarsa affidabilit√† nelle previsioni.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb573b2b-0675-4760-9e19-42f46831800f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### 1. **Creazione della figura**\n",
    "```python\n",
    "plt.figure(figsize=(15, 10))\n",
    "```\n",
    "- **Cos'√®**: Questa riga imposta la dimensione della figura del grafico. La figura avr√† una larghezza di 15 pollici e un'altezza di 10 pollici, garantendo che i grafici siano abbastanza grandi per una visualizzazione chiara.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Loop sui risultati per visualizzare i residui**\n",
    "```python\n",
    "for i, result in enumerate(results):\n",
    "```\n",
    "- **Cos'√®**: Questo ciclo `for` itera su ciascun risultato nel dizionario `results`, che contiene i risultati dei vari modelli. `i` √® l'indice del modello (0, 1, 2, ...), e `result` √® un dizionario contenente i dettagli di ciascun modello, incluse le previsioni (`predictions`).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Creazione del grafico dei residui**\n",
    "```python\n",
    "    plt.subplot(2, 2, i+1)\n",
    "```\n",
    "- **Cos'√®**: Questa riga crea una sottosezione della figura in una griglia 2x2. Il grafico corrente viene posizionato nella `i+1`-esima posizione della griglia, cos√¨ che ogni modello venga visualizzato in una posizione separata (ci saranno quindi 4 grafici in totale).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Calcolo dei residui**\n",
    "```python\n",
    "    residuals = y_val - result['predictions']\n",
    "```\n",
    "- **Cos'√®**: I residui sono la differenza tra i valori reali (`y_val`) e le previsioni fatte dal modello (`result['predictions']`). Ogni errore di previsione √® chiamato \"residuo\" e mostra quanto il modello si discosta dal valore reale.\n",
    "- **Interpretazione**: I residui indicano l'entit√† dell'errore del modello per ciascuna previsione. Se il modello fosse perfetto, i residui sarebbero tutti pari a zero.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Grafico a dispersione dei residui**\n",
    "```python\n",
    "    plt.scatter(result['predictions'], residuals, alpha=0.5)\n",
    "```\n",
    "- **Cos'√®**: Questo comando crea un grafico a dispersione (scatter plot) dove sull'asse delle ascisse vengono posizionate le previsioni (`result['predictions']`), e sull'asse delle ordinate vengono posizionati i residui (`residuals`).\n",
    "- **Alpha**: L'argomento `alpha=0.5` rende i punti semi-trasparenti, facilitando la visualizzazione di punti sovrapposti.\n",
    "- **Interpretazione**: Il grafico a dispersione dei residui aiuta a identificare eventuali pattern sistematici nei residui. Se i residui sono distribuiti casualmente, ci√≤ suggerisce che il modello √® appropriato. Se ci sono pattern (ad esempio, una curva), potrebbe indicare un modello non adatto.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Linea orizzontale a 0**\n",
    "```python\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "```\n",
    "- **Cos'√®**: Questa riga traccia una linea orizzontale sulla posizione `y=0`. La linea √® colorata di rosso (`color='r'`) e tratteggiata (`linestyle='--'`).\n",
    "- **Interpretazione**: La linea a `y=0` rappresenta il punto ideale per i residui, poich√© i residui nulli (zero) indicano previsioni perfette. I residui positivi (sopra la linea) indicano una sovrastima, mentre quelli negativi (sotto la linea) indicano una sottostima delle previsioni.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Impostazione del titolo e delle etichette degli assi**\n",
    "```python\n",
    "    plt.title(f\"{result['model']} Residuals\")\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "```\n",
    "- **Cos'√®**: Queste righe impostano il titolo del grafico (che include il nome del modello, ad esempio \"Linear Regression Residuals\") e le etichette degli assi. L'asse delle ascisse rappresenta i valori predetti, mentre l'asse delle ordinate rappresenta i residui.\n",
    "- **Interpretazione**: Questo rende il grafico chiaro e informativo, in modo che chiunque possa capire cosa viene rappresentato.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Layout del grafico**\n",
    "```python\n",
    "plt.tight_layout()\n",
    "```\n",
    "- **Cos'√®**: Questa funzione ottimizza la disposizione dei sottogruppi di grafici nella figura, evitando sovrapposizioni e garantendo che ogni grafico abbia abbastanza spazio.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Visualizzazione del grafico**\n",
    "```python\n",
    "plt.show()\n",
    "```\n",
    "- **Cos'√®**: Questa funzione visualizza effettivamente i grafici sullo schermo. √à l'ultimo passo che rende i grafici visibili all'utente.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusioni\n",
    "Il codice crea una serie di grafici a dispersione (uno per ogni modello) che mostrano i residui delle previsioni. I residui sono le differenze tra i valori predetti e quelli reali. Un buon modello avr√† residui distribuiti casualmente attorno alla linea orizzontale a `y=0`. Se i residui mostrano un pattern, potrebbe essere un segno che il modello non √® adatto o che c'√® qualche altro fattore che il modello non ha catturato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22dac4-efb8-4953-bc56-eb4013ffbcef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "I grafici mostrano le distribuzioni dei **residui** (differenza tra i valori previsti e quelli effettivi) rispetto ai **valori previsti** per tre modelli di regressione: **Linear Regression**, **Random Forest** e **Decision Tree**. L'obiettivo √® analizzare la performance di ciascun modello osservando il comportamento dei residui.\n",
    "\n",
    "### Struttura dei grafici:\n",
    "- **Asse X**: Valori previsti dai modelli.\n",
    "- **Asse Y**: Residui, che rappresentano l'errore (valore previsto meno valore effettivo).\n",
    "- **Linea rossa tratteggiata (y = 0)**: Indica l'assenza di errore. Residui vicini a questa linea indicano previsioni accurate.\n",
    "\n",
    "### Analisi dei modelli:\n",
    "1. **Linear Regression**:\n",
    "   - I residui sembrano ben distribuiti intorno alla linea rossa (y = 0), con una dispersione moderata.\n",
    "   - √à evidente una tendenza lineare: il modello funziona bene per previsioni semplici.\n",
    "\n",
    "2. **Random Forest**:\n",
    "   - I residui mostrano una distribuzione meno regolare, ma pi√π uniforme rispetto alla Linear Regression.\n",
    "   - Questo indica che il modello cattura relazioni pi√π complesse, riducendo potenzialmente gli errori per alcuni intervalli.\n",
    "\n",
    "3. **Decision Tree**:\n",
    "   - I residui sono molto pi√π dispersi, con alcuni valori notevolmente lontani dalla linea rossa.\n",
    "   - Questo suggerisce una tendenza al **sovra-adattamento** (overfitting), ovvero il modello si adatta eccessivamente ai dati di addestramento e perde generalizzazione.\n",
    "\n",
    "### Conclusioni:\n",
    "- **Linear Regression** offre previsioni pi√π consistenti e mostra meno errori globali.\n",
    "- **Random Forest** √® pi√π versatile e cattura relazioni non lineari, ma con un livello di errore leggermente maggiore in alcuni punti.\n",
    "- **Decision Tree** presenta la performance peggiore, con alti residui e una chiara evidenza di overfitting.\n",
    "\n",
    "Questo tipo di analisi √® fondamentale per scegliere il modello pi√π adatto in base agli obiettivi del progetto e alla natura dei dati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c3713-fbf0-44cf-9cdf-8a521dd5a585",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### **Introduzione ai concetti di errore nel Machine Learning**\n",
    "Nella regressione, abbiamo diverse metriche per valutare la qualit√† di un modello. Le metriche pi√π comuni sono:\n",
    "\n",
    "1. **MSE/RMSE**: Utilizzate quando gli errori pi√π grandi sono considerati pi√π significativi degli errori piccoli, come nei casi di previsioni finanziarie o quando si vuole dare maggiore importanza agli outliers.\n",
    "2. **MAE**: Utile quando ogni errore deve contribuire in modo proporzionale all'errore totale, ed √® adatto quando i dati non contengono outliers o non si desidera penalizzare pesantemente gli outliers.\n",
    "3. **R¬≤**: Utilizzato per comprendere quanto il modello sia migliore rispetto alla media dei valori, molto utile per comunicare i risultati a stakeholder non tecnici.\n",
    "4. **MAPE**: Utile quando si vogliono confrontare modelli che operano su scale differenti, o quando √® pi√π significativo lavorare con errori percentuali piuttosto che assoluti. Viene spesso usato nelle previsioni e nei contesti aziendali.\n",
    "\n",
    "### **Bias-Variance Tradeoff**\n",
    "Il *tradeoff bias-variance* √® un concetto centrale nell'apprendimento automatico e riguarda l'equilibrio tra due fonti di errore che impediscono agli algoritmi di apprendimento supervisionato di generalizzare oltre il set di dati di addestramento:\n",
    "\n",
    "- **Bias**: Errore dovuto a ipotesi troppo semplicistiche nell'algoritmo di apprendimento. Un modello con alto bias pu√≤ non riuscire a catturare le relazioni rilevanti tra le caratteristiche e l'output (sottodimensionamento o *underfitting*).\n",
    "- **Variance**: Errore dovuto a una sensibilit√† eccessiva alle piccole fluttuazioni nel set di addestramento. Un modello con alta varianza pu√≤ adattarsi troppo ai rumori casuali nei dati di addestramento anzich√© ai veri output (sovradimensionamento o *overfitting*).\n",
    "\n",
    "L'obiettivo √® trovare un punto di equilibrio che minimizzi sia il bias che la varianza, ottenendo cos√¨ la miglior performance di generalizzazione.\n",
    "\n",
    "Un'altra interpretazione della struttura dell'errore √® la seguente:\n",
    "\n",
    "- **Bias¬≤**: Indica quanto le previsioni del modello si discostano in media dai valori corretti.\n",
    "- **Variance**: Indica quanto le previsioni variano per un dato punto attraverso diverse realizzazioni del modello.\n",
    "- **Irreducible Error**: √à il rumore intrinseco nella relazione vera che non pu√≤ essere modellato. √à un errore che non pu√≤ essere ridotto con nessuna modifica al modello.\n",
    "\n",
    "---\n",
    "\n",
    "### **Generazione di Dati Sintetici**\n",
    "\n",
    "Il blocco successivo di codice genera dati sintetici con rumore e li visualizza. Questo √® il cuore della sezione che esplora il *bias-variance tradeoff*.\n",
    "\n",
    "#### 1. **Definizione della funzione vera da imparare**\n",
    "```python\n",
    "def true_function(X):\n",
    "    \"\"\"La funzione sottostante che stiamo cercando di imparare\"\"\"\n",
    "    return np.sin(1.5 * X)\n",
    "```\n",
    "- **Cos'√®**: Qui viene definita la funzione vera che cercheremo di approssimare con il nostro modello. In questo caso, la funzione √® il seno moltiplicato per un fattore di 1.5, ossia \\( \\sin(1.5X) \\).\n",
    "\n",
    "#### 2. **Generazione dei dati sintetici con rumore**\n",
    "```python\n",
    "X = np.sort(np.random.uniform(0, 2*np.pi, 40))\n",
    "y = true_function(X) + np.random.normal(0, 0.2, X.shape[0])\n",
    "```\n",
    "- **Cos'√®**: Qui stiamo generando un insieme di dati sintetici.\n",
    "  - `X` √® un array di 40 valori casuali, uniformemente distribuiti nell'intervallo da 0 a \\(2\\pi\\) (circa 6.28). L'uso di `np.sort` ordina questi valori in modo crescente.\n",
    "  - `y` √® il risultato della funzione `true_function(X)`, cio√® i valori generati dalla funzione \\( \\sin(1.5X) \\), con l'aggiunta di un po' di rumore normale (gaussiano) con media 0 e deviazione standard 0.2. Questo rappresenta la naturale variabilit√† nei dati reali.\n",
    "\n",
    "#### 3. **Ristrutturazione di X per scikit-learn**\n",
    "```python\n",
    "X_reshaped = X.reshape(-1, 1)\n",
    "```\n",
    "- **Cos'√®**: Qui, il vettore `X` viene riformattato per essere compatibile con scikit-learn, una libreria di Python per il machine learning. `X.reshape(-1, 1)` trasforma `X` in una matrice a una colonna, che √® la forma richiesta per i modelli di regressione.\n",
    "\n",
    "#### 4. **Suddivisione dei dati in set di addestramento e di test**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "- **Cos'√®**: Qui i dati vengono divisi in un set di addestramento (`X_train`, `y_train`) e un set di test (`X_test`, `y_test`). La dimensione del set di test √® il 30% dei dati totali, e il parametro `random_state=42` garantisce che la divisione sia riproducibile.\n",
    "\n",
    "#### 5. **Creazione di una griglia fine per la visualizzazione**\n",
    "```python\n",
    "X_grid = np.linspace(0, 2*np.pi, 1000).reshape(-1, 1)\n",
    "```\n",
    "- **Cos'√®**: Qui viene creata una griglia di 1000 punti uniformemente distribuiti nell'intervallo \\( [0, 2\\pi] \\), che sar√† utilizzata per visualizzare la funzione vera (senza rumore) su una scala fine.\n",
    "\n",
    "#### 6. **Creazione del grafico**\n",
    "```python\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', s=30, label='Training data')\n",
    "plt.scatter(X_test, y_test, color='red', s=30, label='Test data')\n",
    "plt.plot(X_grid, true_function(X_grid), color='green', linestyle='-', label='True function')\n",
    "plt.title('Synthetic Dataset with Noise')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "- **Cos'√®**: Viene creato un grafico che visualizza i dati sintetici.\n",
    "  - I dati di addestramento vengono mostrati come punti blu (`scatter`).\n",
    "  - I dati di test sono mostrati come punti rossi.\n",
    "  - La funzione vera \\( \\sin(1.5X) \\) √® tracciata come una linea verde.\n",
    "  - Viene anche aggiunto un titolo, le etichette degli assi e una legenda per rendere il grafico facilmente leggibile.\n",
    "\n",
    "### **Interpretazione del grafico**\n",
    "Il grafico mostra i dati sintetici (con rumore) e la vera funzione sottostante. I punti blu rappresentano i dati di addestramento e i punti rossi i dati di test. La linea verde √® la funzione vera che il modello sta cercando di apprendere. Questo grafico serve come base per analizzare il tradeoff tra bias e varianza, poich√© possiamo vedere come il modello si adatta ai dati e quanto si discosta dalla funzione vera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0507246-440d-4e35-a926-abf08e3dc803",
   "metadata": {},
   "source": [
    "Il grafico intitolato **\"Synthetic Dataset with Noise\"** rappresenta l'andamento di una funzione vera con dati di addestramento e test, con l'aggiunta di rumore. Vediamo i dettagli:\n",
    "\n",
    "### Elementi del grafico:\n",
    "1. **Curva verde (True function)**:\n",
    "   - Questa rappresenta la funzione matematica originale che descrive la relazione tra la variabile indipendente (X) e quella dipendente (y). √à il riferimento ideale che il modello dovrebbe approssimare.\n",
    "\n",
    "2. **Punti blu (Training data)**:\n",
    "   - Sono i dati utilizzati per addestrare il modello. La loro distribuzione intorno alla curva verde mostra che sono affetti da rumore, ovvero variazioni casuali che allontanano i dati dal valore ideale.\n",
    "\n",
    "3. **Punti rossi (Test data)**:\n",
    "   - Sono dati indipendenti che non vengono utilizzati durante l'addestramento, ma servono per valutare la capacit√† del modello di generalizzare e adattarsi a nuovi input. Anche questi dati mostrano una certa dispersione rispetto alla funzione vera.\n",
    "\n",
    "### Cosa mostra il grafico:\n",
    "Il grafico evidenzia come il rumore nei dati influenzi il processo di modellazione. L'obiettivo di un modello di machine learning √® approssimare la **True function** nonostante la presenza di tale rumore, senza sovra-adattarsi ai dati di addestramento. \n",
    "\n",
    "### Concetti utili:\n",
    "- **Bias**: Se il modello non si avvicina abbastanza alla curva verde, potrebbe esserci un errore sistematico (bias elevato).\n",
    "- **Varianza**: Se il modello si adatta troppo ai punti blu, potrebbe sovra-adattarsi, riducendo le prestazioni sui dati di test.\n",
    "  \n",
    "Questo grafico √® spesso utilizzato per spiegare il compromesso tra bias e varianza e l'importanza della capacit√† del modello di generalizzare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca67aa80-2915-4e42-b071-84d9f3a463b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Questo blocco di codice illustra come creare e allenare modelli di regressione polinomiale di diversi gradi (1, 3 e 15), analizzando il comportamento del modello in relazione al *bias-variance tradeoff*.\n",
    "\n",
    "### 1. **Definizione dei gradi dei polinomi**\n",
    "```python\n",
    "degrees = [1, 3, 15]  # Linear, cubic, and high degree polynomial\n",
    "```\n",
    "- **Cos'√®**: Vengono definiti tre gradi polinomiali da utilizzare:\n",
    "  - **1**: un modello lineare, che rappresenta un polinomio di primo grado.\n",
    "  - **3**: un modello cubico, che rappresenta un polinomio di terzo grado.\n",
    "  - **15**: un modello ad alto grado, con un polinomio di quindicesimo grado.\n",
    "\n",
    "### 2. **Creazione e addestramento del modello**\n",
    "```python\n",
    "for i, degree in enumerate(degrees):\n",
    "    # Create and train the model\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "```\n",
    "- **Cos'√®**: Per ogni grado di polinomio definito (1, 3, 15), viene creato un modello di regressione polinomiale. Il codice utilizza `make_pipeline` per concatenare due fasi:\n",
    "  - **PolynomialFeatures(degree)**: Trasforma i dati di input in caratteristiche polinomiali di un determinato grado.\n",
    "  - **LinearRegression()**: Applica la regressione lineare ai dati trasformati.\n",
    "  \n",
    "  Il modello viene quindi addestrato sui dati di addestramento (`X_train` e `y_train`).\n",
    "\n",
    "### 3. **Predizioni**\n",
    "```python\n",
    "train_pred = model.predict(X_train)\n",
    "test_pred = model.predict(X_test)\n",
    "grid_pred = model.predict(X_grid)\n",
    "```\n",
    "- **Cos'√®**: Dopo aver addestrato il modello, vengono fatte delle previsioni:\n",
    "  - **train_pred**: Previsioni sui dati di addestramento (`X_train`).\n",
    "  - **test_pred**: Previsioni sui dati di test (`X_test`).\n",
    "  - **grid_pred**: Previsioni sui dati generati dalla griglia fine (`X_grid`) per visualizzare la curva del modello.\n",
    "\n",
    "### 4. **Calcolo degli errori**\n",
    "```python\n",
    "train_mse = mean_squared_error(y_train, train_pred)\n",
    "test_mse = mean_squared_error(y_test, test_pred)\n",
    "```\n",
    "- **Cos'√®**: Vengono calcolati gli errori di previsione utilizzando la metrica **MSE** (Errore Quadratico Medio) per il set di addestramento e il set di test:\n",
    "  - **train_mse**: MSE sui dati di addestramento.\n",
    "  - **test_mse**: MSE sui dati di test.\n",
    "  \n",
    "  L'errore MSE ci dice quanto le previsioni si discostano dai valori reali. Valori pi√π bassi indicano previsioni migliori.\n",
    "\n",
    "### 5. **Visualizzazione dei risultati**\n",
    "```python\n",
    "plt.subplot(1, 3, i+1)\n",
    "plt.scatter(X_train, y_train, color='blue', s=30, label='Training data')\n",
    "plt.scatter(X_test, y_test, color='red', s=30, label='Test data')\n",
    "plt.plot(X_grid, true_function(X_grid), color='green', linestyle='-', label='True function')\n",
    "plt.plot(X_grid, grid_pred, color='purple', linestyle='--', label=f'Degree {degree} polynomial')\n",
    "```\n",
    "- **Cos'√®**: Qui viene creato un grafico per ogni modello, mostrando:\n",
    "  - **Dati di addestramento (blu)** e **dati di test (rosso)**, per visualizzare dove si trovano i punti reali.\n",
    "  - La **funzione vera** (linea verde) rappresenta la relazione sottostante che il modello sta cercando di approssimare.\n",
    "  - Le **previsioni del modello (linea viola)** mostrano come il modello polinomiale si adatta ai dati.\n",
    "\n",
    "### 6. **Aggiunta delle informazioni sull'errore**\n",
    "```python\n",
    "plt.title(f'Polynomial Degree {degree}\\nTrain MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "```\n",
    "- **Cos'√®**: Il titolo del grafico viene aggiornato per mostrare il grado del polinomio e gli errori MSE per i dati di addestramento e di test.\n",
    "\n",
    "### 7. **Aggiunta di annotazioni sul bias e sulla varianza**\n",
    "```python\n",
    "if i == 0:\n",
    "    plt.text(3, -0.7, \"High Bias (Underfitting)\")\n",
    "elif i == 1:\n",
    "    plt.text(3, -0.7, \"Good Balance\")\n",
    "else:\n",
    "    plt.text(3, -0.7, \"High Variance (Overfitting)\")\n",
    "```\n",
    "- **Cos'√®**: In base al grado del polinomio, viene aggiunta un'annotazione per spiegare il comportamento del modello:\n",
    "  - **High Bias (Underfitting)**: Quando il grado del polinomio √® basso (1), il modello non riesce a catturare la complessit√† dei dati, e quindi ha un *alto bias* (sottodimensionamento o *underfitting*).\n",
    "  - **Good Balance**: Quando il grado del polinomio √® moderato (3), il modello ha un buon equilibrio tra bias e varianza, adattandosi bene ai dati.\n",
    "  - **High Variance (Overfitting)**: Quando il grado del polinomio √® molto alto (15), il modello si adatta troppo ai dati di addestramento, cogliendo anche il rumore, causando un *alta varianza* (sovradimensionamento o *overfitting*).\n",
    "\n",
    "### 8. **Visualizzazione finale**\n",
    "```python\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "- **Cos'√®**: Viene regolato il layout dei grafici per assicurarsi che non ci siano sovrapposizioni e poi il grafico finale viene mostrato.\n",
    "\n",
    "### **Interpretazione dei grafici**\n",
    "- **Grado 1 (lineare)**: Il modello non riesce a catturare la curva dei dati (alta bias, underfitting).\n",
    "- **Grado 3 (cubo)**: Il modello si adatta abbastanza bene ai dati, mostrando un buon equilibrio tra bias e varianza.\n",
    "- **Grado 15 (alto)**: Il modello si adatta perfettamente ai dati di addestramento, ma potrebbe non generalizzare bene ai dati di test (alta varianza, overfitting).\n",
    "\n",
    "In sintesi, il codice mostra come un modello polinomiale pu√≤ adattarsi ai dati in modo diverso a seconda del grado del polinomio e come questo influenzi l'errore e la generalizzazione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2febb5ca-2967-488c-84c4-4f869e343684",
   "metadata": {},
   "source": [
    "L'immagine illustra come il grado di un modello di regressione polinomiale influenzi la capacit√† di adattamento ai dati e il compromesso tra **bias** e **varianza**. Ecco l'analisi dettagliata dei tre modelli mostrati:\n",
    "\n",
    "### Modello 1: **Grado 1 (Linear Regression)** ‚Äì Underfitting\n",
    "- **Osservazione:** La linea viola tratteggiata rappresenta una regressione lineare (grado 1). Mostra un'elevata discrepanza rispetto alla funzione reale (curva verde).\n",
    "- **MSE:**  \n",
    "   - Train MSE: 0.4824  \n",
    "   - Test MSE: 0.3792  \n",
    "   Questi valori elevati indicano che il modello non cattura adeguatamente la complessit√† dei dati.\n",
    "- **Conclusione:** L'underfitting si verifica quando il modello √® troppo semplice per rappresentare la relazione sottostante.\n",
    "\n",
    "### Modello 2: **Grado 3 (Cubic Regression)** ‚Äì Buon equilibrio\n",
    "- **Osservazione:** La curva viola tratteggiata si avvicina meglio alla funzione reale, dimostrando un equilibrio tra adattamento ai dati di training e generalizzazione sui dati di test.\n",
    "- **MSE:**  \n",
    "   - Train MSE: 0.3034  \n",
    "   - Test MSE: 0.2685  \n",
    "   I valori sono pi√π bassi e molto vicini tra loro, segnalando una buona capacit√† predittiva.\n",
    "- **Conclusione:** Questo modello rappresenta il compromesso ideale tra bias e varianza, adattandosi bene sia ai dati di training che di test.\n",
    "\n",
    "### Modello 3: **Grado 15 (High-Degree Polynomial)** ‚Äì Overfitting\n",
    "- **Osservazione:** La curva viola tratteggiata passa attraverso quasi tutti i punti di training (blu), ma devia significativamente dalla funzione reale e dai dati di test.\n",
    "- **MSE:**  \n",
    "   - Train MSE: 0.0173 (molto basso)  \n",
    "   - Test MSE: 0.0490 (pi√π alto rispetto al modello di grado 3).  \n",
    "   Questo suggerisce che il modello memorizza i dati di training a scapito della generalizzazione.\n",
    "- **Conclusione:** L'overfitting si verifica quando il modello √® troppo complesso e si adatta eccessivamente ai dati di training, risultando meno efficace su nuovi dati.\n",
    "\n",
    "### Concetti chiave:\n",
    "- **Bias**: L'errore sistematico del modello (elevato nel modello di grado 1).  \n",
    "- **Varianza**: La sensibilit√† del modello alle variazioni nei dati di training (elevata nel modello di grado 15).\n",
    "- **Trade-off bias-varianza**: Il modello di grado 3 evidenzia il miglior equilibrio, con errore contenuto e buona generalizzazione.\n",
    "\n",
    "Questa visualizzazione aiuta a comprendere l'importanza della scelta del grado del polinomio per ottenere modelli predittivi efficaci. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d4b15-b5f8-4f75-816c-b0fb601d6366",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Questo blocco di codice esplora l'uso della **regolarizzazione Ridge** per controllare il *bias-variance tradeoff* in un modello di regressione polinomiale di alto grado (15). La regolarizzazione Ridge aggiunge un termine di penalizzazione alla funzione di costo, con lo scopo di prevenire il sovra-adattamento (overfitting) dei dati.\n",
    "\n",
    "### **Cos'√® la regolarizzazione Ridge**\n",
    "La regolarizzazione Ridge modifica la funzione obiettivo della regressione lineare ordinaria (OLS) aggiungendo un termine di penalizzazione sui coefficienti del modello. Questo penalizza i modelli con coefficienti troppo grandi, contribuendo a evitare l'overfitting.\n",
    "\n",
    "La funzione obiettivo della regressione Ridge √®:\n",
    "\n",
    "$$ \\text{minimizza: } \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2 $$\n",
    "\n",
    "Dove:\n",
    "- Il primo termine √® la funzione di errore standard (somma dei quadrati degli errori).\n",
    "- Il secondo termine √® la penalizzazione dei coefficienti, che li riduce verso zero.\n",
    "- $\\alpha$ √® il parametro di regolarizzazione, che controlla l'intensit√† della penalizzazione.\n",
    "- $\\beta_j$ sono i coefficienti del modello.\n",
    "\n",
    "### **Obiettivo del Codice**\n",
    "Il codice mostra l'effetto di vari valori di $\\alpha$ (da 0 a 100) su un modello polinomiale di 15¬∞ grado, applicando Ridge Regression. L'obiettivo √® osservare come la regolarizzazione influenzi l'adattamento del modello ai dati, controllando il bias e la varianza.\n",
    "\n",
    "### **Dettaglio del Codice**\n",
    "\n",
    "#### 1. **Definizione del grado del polinomio e dei valori di $\\alpha$**\n",
    "```python\n",
    "degree = 15  # High degree polynomial\n",
    "alphas = [0, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "```\n",
    "- **Cos'√®**: Il modello utilizza un polinomio di **15¬∞ grado**. I valori di **$\\alpha$** variano da **0** (senza regolarizzazione, equivalente alla regressione OLS) a **100**, passando per valori intermedi. Ogni valore di $\\alpha$ corrisponde a una diversa intensit√† di penalizzazione sui coefficienti del modello.\n",
    "\n",
    "#### 2. **Creazione e addestramento del modello**\n",
    "```python\n",
    "model = make_pipeline(\n",
    "    PolynomialFeatures(degree),\n",
    "    Ridge(alpha=alpha)\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "- **Cos'√®**: Per ciascun valore di $\\alpha$, viene creato un **modello Ridge** con un polinomio di 15¬∞ grado. Si utilizza `make_pipeline` per concatenare due fasi:\n",
    "  1. **PolynomialFeatures(degree)**: Trasforma i dati di input in caratteristiche polinomiali.\n",
    "  2. **Ridge(alpha=alpha)**: Applica la regressione Ridge con il parametro $\\alpha$ definito.\n",
    "  \n",
    "  Poi, il modello viene addestrato sui dati di addestramento (`X_train` e `y_train`).\n",
    "\n",
    "#### 3. **Previsioni e calcolo degli errori**\n",
    "```python\n",
    "train_pred = model.predict(X_train)\n",
    "test_pred = model.predict(X_test)\n",
    "grid_pred = model.predict(X_grid[:-30])\n",
    "```\n",
    "- **Cos'√®**: Dopo l'addestramento, vengono fatte delle previsioni:\n",
    "  - **train_pred**: Previsioni sui dati di addestramento (`X_train`).\n",
    "  - **test_pred**: Previsioni sui dati di test (`X_test`).\n",
    "  - **grid_pred**: Previsioni su una griglia di valori (da `X_grid`), utilizzata per tracciare la curva del modello.\n",
    "  \n",
    "  Successivamente, vengono calcolati gli **errori MSE** (Mean Squared Error) sia per i dati di addestramento che per i dati di test, per valutare la qualit√† delle previsioni.\n",
    "\n",
    "#### 4. **Visualizzazione dei risultati**\n",
    "```python\n",
    "plt.scatter(X_train, y_train, color='blue', s=30, alpha=0.6, label='Training data')\n",
    "plt.plot(X_grid[:-30], true_function(X_grid[:-30]), color='green', linestyle='-', label='True function')\n",
    "plt.plot(X_grid[:-30], grid_pred, color='red', linestyle='--', label=f'Ridge (Œ±={alpha})')\n",
    "```\n",
    "- **Cos'√®**: Per ciascun valore di $\\alpha$, viene generato un grafico che mostra:\n",
    "  - I **dati di addestramento** in blu.\n",
    "  - La **funzione vera** in verde, che rappresenta la relazione che il modello sta cercando di approssimare.\n",
    "  - La **curva del modello Ridge** in rosso, che mostra come il modello si adatta ai dati con un determinato valore di $\\alpha$.\n",
    "\n",
    "#### 5. **Aggiunta delle informazioni sugli errori**\n",
    "```python\n",
    "plt.title(f'Œ±={alpha_text}\\nTrain MSE: {train_mse:.4f}\\nTest MSE: {test_mse:.4f}')\n",
    "```\n",
    "- **Cos'√®**: Ogni grafico include il valore di $\\alpha$ e gli errori **MSE** (Train e Test), per evidenziare l'impatto della regolarizzazione sui dati di addestramento e di test.\n",
    "\n",
    "#### 6. **Leggenda e annotazioni**\n",
    "```python\n",
    "if i == 0:  # Only show legend for the first plot\n",
    "    plt.legend()\n",
    "```\n",
    "- **Cos'√®**: La **legenda** viene mostrata solo per il primo grafico, evitando che le legende si sovrappongano nei grafici successivi.\n",
    "\n",
    "#### 7. **Layout del grafico**\n",
    "```python\n",
    "plt.suptitle('Effect of Ridge Regularization (Œ±) on a Degree 15 Polynomial Model', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "```\n",
    "- **Cos'√®**: Il titolo del grafico principale (`suptitle`) spiega l'effetto della regolarizzazione Ridge con vari valori di $\\alpha$. `tight_layout` ottimizza lo spazio tra i grafici per una visualizzazione chiara.\n",
    "\n",
    "### **Interpretazione dei Risultati**\n",
    "1. **$\\alpha = 0$ (OLS)**: Il modello √® equivalente alla regressione lineare ordinaria (senza penalizzazione), quindi si adatta perfettamente ai dati di addestramento, ma potrebbe sovradattarsi (overfitting).\n",
    "2. **$\\alpha > 0$**: Con l'aumentare di $\\alpha$, la regolarizzazione riduce l'intensit√† dei coefficienti del modello, impedendo il sovradattamento. Ci√≤ porta a un **maggiore bias e minore varianza**, ma migliora la generalizzazione sui dati di test.\n",
    "3. **$\\alpha$ molto grande**: Quando $\\alpha$ √® molto grande, il modello diventa molto semplice, riducendo fortemente l'adattamento ai dati (possibile **underfitting**).\n",
    "\n",
    "### **Conclusione**\n",
    "La regolarizzazione Ridge √® utile per prevenire il sovra-adattamento, specialmente nei modelli complessi. Aumentando $\\alpha$, si controlla meglio il tradeoff tra bias e varianza, trovando il giusto equilibrio per un modello che generalizza bene sui dati non visti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59112f-f85b-4da3-9a29-583ec00a1b9e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "L'immagine mostra come la **Ridge Regularization** (controllata dal parametro Œ±) influenzi un modello di regressione polinomiale di grado 15. Il parametro Œ± rappresenta la forza della penalizzazione: valori pi√π alti riducono la complessit√† del modello, evitando sovra-adattamenti.\n",
    "\n",
    "### Elementi principali:\n",
    "1. **Œ±=0 (Ordinary Least Squares - OLS):**\n",
    "   - **Train MSE = 0.0162** | **Test MSE = 0.0633**\n",
    "   - Il modello cattura tutti i dettagli e il rumore dei dati di addestramento, risultando in **overfitting**. La linea tratteggiata rossa segue esattamente i punti di training, ma si allontana dalla funzione reale (verde).\n",
    "\n",
    "2. **Œ±=0.001 e Œ±=0.01:**\n",
    "   - Per valori molto bassi di Œ±:\n",
    "     - **Train MSE** rimane basso (~0.0173 e 0.0174), e il **Test MSE** migliora rispetto a OLS (0.0469 e 0.0447).\n",
    "     - Questi valori suggeriscono che il modello inizia a generalizzare meglio, ma mantiene ancora un'elevata flessibilit√†.\n",
    "\n",
    "3. **Œ±=0.1 e Œ±=1:**\n",
    "   - Qui si osserva un **equilibrio ottimale**.\n",
    "   - Ad esempio, per **Œ±=1**:\n",
    "     - **Train MSE = 0.0214** | **Test MSE = 0.0340**\n",
    "     - Il modello riduce la complessit√†, migliorando la generalizzazione senza perdere troppa accuratezza nei dati di training. La curva rossa approssima bene sia i punti blu che la funzione verde.\n",
    "\n",
    "4. **Œ±=10 e Œ±=100:**\n",
    "   - **Train MSE** e **Test MSE** crescono gradualmente (es. Œ±=10: **Train MSE = 0.0367**, **Test MSE = 0.0356**).\n",
    "   - Con l'aumento di Œ±, il modello diventa troppo rigido, perdendo la capacit√† di catturare la struttura dei dati. Si osserva un effetto di **underfitting**.\n",
    "\n",
    "### Conclusioni:\n",
    "- **Modelli non regolarizzati (Œ±=0)** mostrano overfitting, con elevata varianza e scarse prestazioni sui dati di test.\n",
    "- **Valori intermedi di Œ± (es. 1)** rappresentano il compromesso ideale, con un buon bilanciamento tra bias e varianza.\n",
    "- **Valori molto alti di Œ± (es. 100)** portano a underfitting, dove il modello diventa troppo semplice e incapace di catturare i dettagli della funzione reale.\n",
    "\n",
    "Questo esperimento dimostra l'importanza della regolarizzazione per ottenere modelli robusti e generalizzabili. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54bcdd-512a-4bbb-b257-8b8bb041ad08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2491f4e-98dc-4069-b049-1459b9da11e4",
   "metadata": {},
   "source": [
    "BREAK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed361f5-27a6-4fd9-b6ec-53a8eecbab7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07b63dbb-d87c-40b6-8411-e09788467c54",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Questo blocco di codice esplora diverse **tecniche di validazione** per valutare come i modelli generalizzano su nuovi dati. Si concentra in particolare sulla **validazione incrociata (K-Fold Cross-Validation)**, confrontandola con la **validazione hold-out**.\n",
    "\n",
    "### **Obiettivo del Codice**\n",
    "Il codice esegue una **5-fold cross-validation** per diversi modelli (presumibilmente gi√† definiti come `models`), calcolando e visualizzando le metriche di prestazione come **MSE (Mean Squared Error)**, **RMSE (Root Mean Squared Error)** e **R¬≤** (coefficiente di determinazione).\n",
    "\n",
    "### **Dettaglio del Codice**\n",
    "\n",
    "#### 1. **Definizione della K-Fold Cross-Validation**\n",
    "```python\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "```\n",
    "- **Cos'√®**: Qui, viene definita una **K-fold cross-validation** con `k=5`, il che significa che il dataset sar√† suddiviso in 5 parti (fold). I dati vengono mescolati prima della divisione (`shuffle=True`) per garantire che la divisione sia casuale. `random_state=42` garantisce che i risultati siano riproducibili.\n",
    "- **Funzione**: `KFold` √® una funzione che suddivide il dataset in `k` parti e poi esegue il training del modello su `k-1` di esse, utilizzando la parte rimanente come set di validazione. Questo processo viene ripetuto `k` volte, ognuna con un diverso set di validazione.\n",
    "\n",
    "#### 2. **Creazione del Dizionario per Memorizzare i Risultati**\n",
    "```python\n",
    "cv_results = {}\n",
    "```\n",
    "- **Cos'√®**: Qui viene creato un dizionario vuoto chiamato `cv_results` dove verranno memorizzati i risultati delle diverse metriche di valutazione per ogni modello.\n",
    "\n",
    "#### 3. **Loop sui Modelli**\n",
    "```python\n",
    "for name, model in models.items():\n",
    "```\n",
    "- **Cos'√®**: Questo ciclo `for` itera su ogni modello presente nel dizionario `models`. Presumibilmente, `models` √® un dizionario che contiene i modelli di machine learning (come regressori o classificatori). Ogni iterazione restituir√† il nome del modello (`name`) e il modello stesso (`model`).\n",
    "\n",
    "#### 4. **Calcolo dei MSE e R¬≤ tramite Cross-Validation**\n",
    "```python\n",
    "mse_scores = -cross_val_score(model, X_train_val, y_train_val, \n",
    "                               scoring='neg_mean_squared_error', \n",
    "                               cv=kf, n_jobs=-1)\n",
    "```\n",
    "- **Cos'√®**: Qui viene eseguita una **cross-validation** calcolando il **MSE (Mean Squared Error)**. La funzione `cross_val_score` calcola la metrica desiderata (in questo caso MSE) per ogni fold. `scoring='neg_mean_squared_error'` √® usato per restituire il MSE come valore negativo (perch√© `cross_val_score` vuole una metrica di massimizzazione, quindi il valore negativo permette di minimizzare MSE).\n",
    "- `n_jobs=-1` significa che il codice utilizza tutte le risorse di calcolo disponibili per parallelizzare il processo.\n",
    "  \n",
    "```python\n",
    "r2_scores = cross_val_score(model, X_train_val, y_train_val, \n",
    "                             scoring='r2', \n",
    "                             cv=kf, n_jobs=-1)\n",
    "```\n",
    "- **Cos'√®**: Viene calcolato anche il punteggio **R¬≤ (coefficiente di determinazione)** tramite la stessa funzione `cross_val_score`, ma con `scoring='r2'`. Questo punteggio misura quanto bene il modello spiega la varianza dei dati di output.\n",
    "\n",
    "#### 5. **Memorizzazione dei Risultati per Ogni Modello**\n",
    "```python\n",
    "cv_results[name] = {\n",
    "    'MSE': mse_scores,\n",
    "    'RMSE': np.sqrt(mse_scores),\n",
    "    'R2': r2_scores\n",
    "}\n",
    "```\n",
    "- **Cos'√®**: I risultati di MSE, RMSE (che √® la radice quadrata di MSE) e R¬≤ per ogni modello vengono memorizzati nel dizionario `cv_results`. Ogni modello avr√† un proprio sotto-dizionario contenente queste metriche.\n",
    "\n",
    "#### 6. **Visualizzazione dei Risultati**\n",
    "```python\n",
    "plt.figure(figsize=(14, 6))\n",
    "```\n",
    "- **Cos'√®**: Crea una figura per il grafico con una dimensione di 14x6 pollici, che √® abbastanza grande per visualizzare chiaramente i risultati.\n",
    "\n",
    "#### 7. **Grafico RMSE**\n",
    "```python\n",
    "plt.subplot(1, 2, 1)\n",
    "rmse_means = [cv_results[model]['RMSE'].mean() for model in models.keys()]\n",
    "rmse_stds = [cv_results[model]['RMSE'].std() for model in models.keys()]\n",
    "plt.bar(models.keys(), rmse_means, yerr=rmse_stds, capsize=10, alpha=0.7)\n",
    "plt.title('Cross-Validation RMSE by Model')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks(rotation=45)\n",
    "```\n",
    "- **Cos'√®**: Crea il primo grafico (un **bar plot**) per visualizzare il **RMSE medio** per ciascun modello. `rmse_means` calcola la media dei valori RMSE per ogni modello, mentre `rmse_stds` calcola la deviazione standard. Vengono anche mostrati gli **errori standard** (come barre verticali) utilizzando `yerr=rmse_stds`.\n",
    "- `capsize=10` aggiunge dei \"caps\" alle estremit√† delle barre di errore, mentre `alpha=0.7` imposta la trasparenza delle barre.\n",
    "- `plt.xticks(rotation=45)` ruota le etichette sull'asse x per renderle leggibili.\n",
    "\n",
    "#### 8. **Grafico R¬≤**\n",
    "```python\n",
    "plt.subplot(1, 2, 2)\n",
    "r2_means = [cv_results[model]['R2'].mean() for model in models.keys()]\n",
    "r2_stds = [cv_results[model]['R2'].std() for model in models.keys()]\n",
    "plt.bar(models.keys(), r2_means, yerr=r2_stds, capsize=10, alpha=0.7)\n",
    "plt.title('Cross-Validation R¬≤ by Model')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.xticks(rotation=45)\n",
    "```\n",
    "- **Cos'√®**: Crea il secondo grafico (un **bar plot**) per visualizzare la **media del punteggio R¬≤** per ciascun modello. Vengono anche mostrati gli errori standard, come nel grafico RMSE.\n",
    "\n",
    "#### 9. **Ottimizzazione del Layout e Visualizzazione**\n",
    "```python\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "- **Cos'√®**: `tight_layout()` ottimizza la disposizione dei grafici, evitando che le etichette e i titoli si sovrappongano. `plt.show()` visualizza effettivamente il grafico.\n",
    "\n",
    "### **Interpretazione dei Risultati**\n",
    "Ogni modello sar√† valutato utilizzando la **K-fold cross-validation**:\n",
    "- **MSE (Mean Squared Error)**: Una metrica di errore che penalizza gli errori pi√π grandi in modo quadratico.\n",
    "- **RMSE (Root Mean Squared Error)**: La radice quadrata del MSE, che ha le stesse unit√† della variabile target, rendendola pi√π interpretativa.\n",
    "- **R¬≤ (Coefficiente di determinazione)**: Misura la proporzione della varianza spiegata dal modello. Un valore vicino a 1 indica che il modello spiega bene la variabilit√† dei dati, mentre un valore vicino a 0 indica che il modello non spiega la varianza dei dati.\n",
    "\n",
    "### **Conclusione**\n",
    "Questa procedura di **K-Fold Cross-Validation** consente di ottenere una stima pi√π robusta delle prestazioni di un modello, particolarmente utile quando si lavora con piccoli dataset. La visualizzazione aiuta a confrontare l'efficacia dei vari modelli rispetto alle diverse metriche."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2d15e-9611-4ae0-8528-a27b65d80316",
   "metadata": {},
   "source": [
    "L'immagine confronta le prestazioni di tre modelli di machine learning ‚Äî **Linear Regression**, **Random Forest** e **Decision Tree** ‚Äî utilizzando due metriche fondamentali: **RMSE (Root Mean Squared Error)** e **R¬≤ (coefficiente di determinazione)**.\n",
    "\n",
    "### Analisi dei grafici:\n",
    "#### **Grafico di sinistra: RMSE (Root Mean Squared Error)**\n",
    "- **Linear Regression**: Mostra il valore **pi√π basso** di RMSE, circa 20. Questo indica che √® il modello pi√π accurato in termini di errori medi rispetto ai dati reali.\n",
    "- **Random Forest**: Ha un RMSE maggiore, attorno a 70. Questo suggerisce una minore accuratezza rispetto alla regressione lineare.\n",
    "- **Decision Tree**: Presenta l'**RMSE pi√π alto**, circa 120, indicando che √® il modello meno preciso.\n",
    "\n",
    "#### **Grafico di destra: R¬≤ (coefficiente di determinazione)**\n",
    "- **Linear Regression**: Ottiene il valore pi√π alto di R¬≤, vicino a **1.0**, il che significa che spiega quasi perfettamente la variazione nei dati.\n",
    "- **Random Forest**: Ha un valore di R¬≤ pi√π basso, circa **0.85**, dimostrando una discreta capacit√† di adattarsi ai dati ma inferiore alla regressione lineare.\n",
    "- **Decision Tree**: Ottiene il punteggio R¬≤ pi√π basso, circa **0.6**, evidenziando una scarsa capacit√† di spiegare i dati in modo coerente.\n",
    "\n",
    "### Interpretazione:\n",
    "1. **Linear Regression**: \n",
    "   - Il miglior modello in termini di accuratezza generale. Ha il RMSE pi√π basso e il R¬≤ pi√π alto, risultando ideale se la relazione tra le variabili √® lineare.\n",
    "   \n",
    "2. **Random Forest**:\n",
    "   - Offre prestazioni accettabili, ma meno efficaci rispetto alla regressione lineare. Potrebbe essere preferito in situazioni con relazioni pi√π complesse tra i dati.\n",
    "\n",
    "3. **Decision Tree**:\n",
    "   - Mostra le peggiori prestazioni tra i tre modelli, con un RMSE elevato e un R¬≤ basso. Questo indica problemi di generalizzazione o sovra-adattamento ai dati di training.\n",
    "\n",
    "### Conclusione:\n",
    "**Linear Regression** risulta essere la scelta migliore in termini di errore minimo e capacit√† predittiva, mentre **Random Forest** potrebbe essere utile per scenari meno lineari. **Decision Tree**, invece, √® il meno indicato, con risultati significativamente peggiori. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70244b7d-512c-494e-960f-e8b9b3819196",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Questa sezione esplora **tecniche di validazione avanzate** che possono essere utilizzate in situazioni particolari. Ogni tecnica ha dei vantaggi specifici, a seconda della natura dei dati e delle esigenze del modello.\n",
    "\n",
    "### **1. Stratified K-Fold**\n",
    "```python\n",
    "Stratified K-Fold: Ensures that each fold has approximately the same proportion of each target value. More commonly used for classification but can be adapted for regression by binning the target values.\n",
    "```\n",
    "- **Cos'√®**: La **Stratified K-Fold** √® una variazione della K-fold cross-validation che assicura che ogni \"fold\" (suddivisione) del dataset contenga la stessa proporzione di ciascun valore del target (variabile di output). \n",
    "  - **In classificazione**: Questa tecnica √® particolarmente utile quando si ha un dataset sbilanciato, cio√® quando alcune classi sono molto pi√π numerose di altre. Ad esempio, se nel dataset ci sono 90% di campioni della classe A e 10% della classe B, una suddivisione casuale potrebbe far s√¨ che alcuni fold non abbiano abbastanza campioni della classe B per una valutazione corretta. Con lo **Stratified K-Fold**, la proporzione delle classi in ciascun fold rimane simile a quella del dataset originale.\n",
    "  - **In regressione**: Anche se originariamente progettata per la classificazione, questa tecnica pu√≤ essere adattata alla regressione attraverso un processo chiamato **binning**, che consiste nel raggruppare i valori continui del target in intervalli (ad esempio, creare bin di valori per la variabile target).\n",
    "  \n",
    "### **2. Time Series Cross-Validation**\n",
    "```python\n",
    "Time Series Cross-Validation: For time-dependent data, where future samples shouldn't be used to predict past observations.\n",
    "```\n",
    "- **Cos'√®**: La **Time Series Cross-Validation** √® una tecnica specifica per i dati **temporalmente dipendenti**, come i dati di serie storiche. Nei dati di questo tipo, √® fondamentale rispettare l'ordine cronologico, evitando che le informazioni future vengano utilizzate per predire eventi passati. \n",
    "  - **Funzionamento**: In pratica, i dati vengono suddivisi in diversi blocchi temporali (folds), ma anzich√© fare un'estrazione casuale come nella K-fold tradizionale, i fold vengono costruiti in modo tale che, per ogni iterazione, il modello venga allenato su un periodo di tempo precedente e validato su un periodo successivo. Ad esempio, se il dataset copre il periodo 2010-2020, la cross-validation pu√≤ allenare il modello sui dati dal 2010 al 2015 e validarlo sui dati dal 2016 al 2020, e cos√¨ via, spostando la finestra temporale ad ogni iterazione.\n",
    "\n",
    "### **3. Leave-One-Out Cross-Validation (LOOCV)**\n",
    "```python\n",
    "Leave-One-Out Cross-Validation (LOOCV): A special case of k-fold where k equals the number of samples. Computationally expensive but useful for very small datasets.\n",
    "```\n",
    "- **Cos'√®**: La **Leave-One-Out Cross-Validation** (LOOCV) √® un caso speciale della K-fold cross-validation in cui **k** √® uguale al numero di campioni nel dataset. \n",
    "  - **Funzionamento**: In altre parole, per ogni iterazione, il modello √® addestrato utilizzando tutti i dati tranne uno, e il campione rimanente viene usato come set di validazione. Questo processo viene ripetuto per ogni campione del dataset. Se ci sono 100 campioni, il modello sar√† allenato 100 volte, ogni volta su 99 campioni e testato su 1 campione.\n",
    "  - **Vantaggio**: √à particolarmente utile per **dataset molto piccoli**, in cui ogni singolo campione √® importante e il numero di dati disponibili √® limitato.\n",
    "  - **Svantaggio**: √à **computazionalmente costoso**, perch√© richiede di addestrare il modello un numero di volte pari al numero di campioni nel dataset, il che pu√≤ essere molto dispendioso in termini di tempo per dataset grandi.\n",
    "\n",
    "### **4. Nested Cross-Validation**\n",
    "```python\n",
    "Nested Cross-Validation: Uses an outer loop for performance estimation and an inner loop for hyperparameter tuning. Provides an unbiased estimate of the model's performance, especially when hyperparameter tuning is involved.\n",
    "```\n",
    "- **Cos'√®**: La **Nested Cross-Validation** √® una tecnica che utilizza due cicli di cross-validation: uno esterno e uno interno.\n",
    "  - **Ciclo esterno (outer loop)**: In questo ciclo, viene utilizzato un set di dati di validazione per valutare le prestazioni del modello, proprio come nella cross-validation tradizionale.\n",
    "  - **Ciclo interno (inner loop)**: All'interno di ogni iterazione del ciclo esterno, un ciclo interno viene utilizzato per **ottimizzare gli iperparametri** del modello. In altre parole, all'interno del ciclo esterno, si esegue un'altra cross-validation sui dati di addestramento per trovare i migliori iperparametri.\n",
    "  - **Vantaggio**: Questo approccio √® particolarmente utile quando si fa **iperparametro tuning** (ad esempio, scegliere i migliori parametri di un modello come il numero di alberi in una foresta casuale). Il vantaggio √® che fornisce una stima **non distorta** delle prestazioni del modello, evitando il rischio di **overfitting** che potrebbe verificarsi se i dati di test venissero usati per ottimizzare gli iperparametri.\n",
    "  - **Funzionamento**: Questo tipo di validazione √® pi√π costoso in termini di tempo computazionale rispetto alla semplice cross-validation, poich√© richiede di eseguire pi√π cicli di cross-validation per la ricerca degli iperparametri.\n",
    "\n",
    "### **Sintesi delle Tecniche**\n",
    "Ogni tecnica di validazione √® pensata per risolvere specifici problemi che si possono verificare durante l'addestramento e la validazione dei modelli:\n",
    "\n",
    "- **Stratified K-Fold**: Utile per dataset sbilanciati o con classi rare.\n",
    "- **Time Series Cross-Validation**: Fondamentale per i dati temporali, dove l'ordine cronologico √® cruciale.\n",
    "- **LOOCV**: Ideale per dataset molto piccoli, ma dispendioso in termini di calcolo.\n",
    "- **Nested Cross-Validation**: Essenziale quando c'√® bisogno di una stima non distorta delle prestazioni del modello, specialmente quando si eseguono ottimizzazioni degli iperparametri.\n",
    "\n",
    "Ogni tecnica ha i suoi pro e contro, e la scelta di quale utilizzare dipende dal tipo di dati e dal problema specifico che si sta cercando di risolvere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84019d38-9b70-426f-ac35-5bb065506b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f924a74-f865-4fa9-8eaa-42222d1064e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5d457ec-66ae-479e-8cba-d48ca8dcdfed",
   "metadata": {},
   "source": [
    "STOP 07-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f352bcf7-2c1f-4fad-9052-41b6213bb658",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc0e7a55-74df-4b16-8a96-a62d56e27067",
   "metadata": {},
   "source": [
    "CONTINUE 10-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e57b5-3154-4e56-b627-8014441cb679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84155647-8b33-49e6-a7f2-e66474354039",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    " **learning curves** (curve di apprendimento) \n",
    "\n",
    "---\n",
    "\n",
    "## **Che cosa sono le Learning Curves?**\n",
    "Le learning curves sono uno strumento utilizzato per analizzare le prestazioni di un modello di machine learning man mano che aumenta la quantit√† di dati di addestramento. Queste curve mostrano:\n",
    "1. **Training Score (accuratezza sul set di addestramento):** Quanto bene il modello si adatta ai dati di addestramento.\n",
    "2. **Cross-validation Score (accuratezza su dati di test o validazione incrociata):** Quanto bene il modello generalizza su nuovi dati non visti durante l‚Äôaddestramento.\n",
    "\n",
    "### **Perch√© sono utili?**\n",
    "- **Rilevare underfitting o overfitting:** \n",
    "  - **Underfitting:** Quando il modello non √® sufficientemente complesso e non riesce ad adattarsi ai dati.\n",
    "  - **Overfitting:** Quando il modello si adatta troppo ai dettagli del training set, perdendo capacit√† di generalizzazione.\n",
    "- **Capire l'impatto dei dati:** Permettono di valutare se aggiungere pi√π dati di addestramento potrebbe migliorare le prestazioni del modello.\n",
    "\n",
    "---\n",
    "\n",
    "## **Passo 1: Caricamento dei dati**\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "X, y = load_iris(return_X_y=True)\n",
    "```\n",
    "1. **Dataset Iris:** √à un dataset incorporato in scikit-learn usato per problemi di classificazione. Contiene tre classi (specie di fiori) e 150 campioni con 4 feature (es. lunghezza del petalo, larghezza del sepalo).\n",
    "2. **DecisionTreeClassifier:** Un modello di classificazione basato su alberi decisionali.\n",
    "3. `random_state=42`: Garantisce la riproducibilit√† del modello, generando sempre lo stesso risultato.\n",
    "\n",
    "---\n",
    "\n",
    "## **Passo 2: Calcolo delle Learning Curves**\n",
    "```python\n",
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    dt, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "```\n",
    "\n",
    "### **Spiegazione dei parametri:**\n",
    "- **`learning_curve`**: Funzione di scikit-learn che calcola le performance di un modello al variare della dimensione del training set.\n",
    "  - `dt`: Il modello (DecisionTreeClassifier).\n",
    "  - `X, y`: Dati di input e target.\n",
    "  - `cv=5`: Usa una validazione incrociata con 5 suddivisioni.\n",
    "  - `train_sizes=np.linspace(0.1, 1.0, 10)`: Crea 10 diverse dimensioni per il set di addestramento, da 10% al 100% dei dati totali.\n",
    "  \n",
    "### **Risultati Restituiti:**\n",
    "1. **`train_sizes`:** Le dimensioni effettive dei training set usati in ogni step.\n",
    "2. **`train_scores`:** Accuratezze ottenute sul training set per ogni dimensione.\n",
    "3. **`test_scores`:** Accuratezze ottenute durante la validazione incrociata.\n",
    "\n",
    "---\n",
    "\n",
    "## **Passo 3: Analisi Statistica**\n",
    "```python\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "```\n",
    "Per ogni dimensione del set di addestramento:\n",
    "1. **`train_mean` e `test_mean`:** Calcolano le medie degli score per ogni step della learning curve.\n",
    "2. **`train_std` e `test_std`:** Calcolano la deviazione standard degli score, per indicare la variabilit√† nelle prestazioni.\n",
    "\n",
    "---\n",
    "\n",
    "## **Passo 4: Visualizzazione delle Learning Curves**\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Learning Curves')\n",
    "plt.plot(train_sizes, train_mean, label='Training Score')\n",
    "plt.plot(train_sizes, test_mean, label='Cross-validation Score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **Cosa fa questo codice:**\n",
    "1. **Grafico delle curve:**\n",
    "   - `plt.plot(train_sizes, train_mean, label='Training Score')`: Traccia la curva dell'accuratezza sul training set.\n",
    "   - `plt.plot(train_sizes, test_mean, label='Cross-validation Score')`: Traccia la curva dell'accuratezza in validazione.\n",
    "\n",
    "2. **Aree di incertezza (ombrate):**\n",
    "   - `plt.fill_between`: Riempie l'area attorno a ogni curva (medie ¬± deviazioni standard) per mostrare la variabilit√† delle prestazioni.\n",
    "\n",
    "3. **Etichette:**\n",
    "   - `plt.xlabel` e `plt.ylabel`: Definiscono gli assi (dimensione del training set e accuratezza).\n",
    "   - `plt.legend`: Aggiunge la legenda per distinguere le due curve.\n",
    "\n",
    "---\n",
    "\n",
    "## **Come interpretare le Learning Curves**\n",
    "- **Curva di Training (Training Score):** Mostra quanto bene il modello si adatta ai dati di addestramento. Una curva piatta e alta (vicina a 1) indica che il modello si adatta perfettamente ai dati di training.\n",
    "- **Curva di Validazione (Cross-validation Score):** Indica quanto bene il modello generalizza su dati non visti. Una curva stabile vicino al valore massimo indica un buon modello generalizzante.\n",
    "\n",
    "### **Esempi pratici:**\n",
    "1. **Overfitting:**\n",
    "   - La curva di training √® alta (vicino a 1.0).\n",
    "   - La curva di validazione √® bassa, con un ampio gap tra le due.\n",
    "   - Soluzione: Ridurre la complessit√† del modello (es. limitare la profondit√† dell‚Äôalbero decisionale).\n",
    "\n",
    "2. **Underfitting:**\n",
    "   - Entrambe le curve (training e validazione) sono basse.\n",
    "   - Soluzione: Aumentare la complessit√† del modello o aggiungere pi√π feature.\n",
    "\n",
    "3. **Buon Modello:**\n",
    "   - La curva di training e quella di validazione convergono a un punteggio elevato.\n",
    "\n",
    "---\n",
    "\n",
    "## **Considerazioni Finali**\n",
    "Grazie a questa procedura, puoi:\n",
    "- Diagnosticare problemi di overfitting e underfitting.\n",
    "- Valutare se aumentare i dati di addestramento migliorer√† le prestazioni del modello.\n",
    "- Determinare se il modello √® sufficientemente complesso per il problema.\n",
    "\n",
    "Fammi sapere se desideri un'ulteriore spiegazione o esempi su specifiche situazioni!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284017f5-7b56-49f8-9f80-5ace9ad0ed9a",
   "metadata": {},
   "source": [
    "#\n",
    "\n",
    "---\n",
    "\n",
    "## **4.1 Grid Search Cross-Validation**\n",
    "\n",
    "**Cos'√® il Grid Search?**\n",
    "Il Grid Search √® una tecnica per trovare i parametri ottimali per un modello testando sistematicamente tutte le combinazioni di parametri definiti in una griglia. Usa la validazione incrociata per valutare ogni combinazione, scegliendo infine quella che massimizza una metrica (ad esempio, l'accuratezza).\n",
    "\n",
    "### **Passaggi chiave nel codice:**\n",
    "1. **Importazione delle librerie e caricamento del dataset.**\n",
    "   ```python\n",
    "   from sklearn.model_selection import GridSearchCV\n",
    "   from sklearn.svm import SVC\n",
    "\n",
    "   # Load dataset\n",
    "   X, y = load_iris(return_X_y=True)\n",
    "   ```\n",
    "   - Usiamo il dataset **Iris**, che contiene 150 esempi, 3 classi e 4 feature.\n",
    "   - Il modello selezionato √® **SVC** (Support Vector Classifier).\n",
    "\n",
    "2. **Definizione della griglia di parametri.**\n",
    "   ```python\n",
    "   param_grid = {\n",
    "       'C': [0.1, 1, 10],\n",
    "       'kernel': ['linear', 'rbf'],\n",
    "       'gamma': ['scale', 'auto']\n",
    "   }\n",
    "   ```\n",
    "   - `C`: Controlla la forza di regolarizzazione (valori pi√π alti riducono l'overfitting).\n",
    "   - `kernel`: Specifica il tipo di kernel (lineare o RBF).\n",
    "   - `gamma`: Parametro del kernel RBF; controlla l'influenza di un singolo esempio di addestramento.\n",
    "\n",
    "3. **Esecuzione del Grid Search.**\n",
    "   ```python\n",
    "   grid_search = GridSearchCV(\n",
    "       SVC(), param_grid, cv=5, \n",
    "       scoring='accuracy', n_jobs=-1\n",
    "   )\n",
    "   grid_search.fit(X, y)\n",
    "   ```\n",
    "   - `cv=5`: Suddivide i dati in 5 fold per validazione incrociata.\n",
    "   - `scoring='accuracy'`: Valuta le combinazioni di parametri in base all'accuratezza.\n",
    "   - `n_jobs=-1`: Usa tutti i core disponibili per velocizzare il calcolo.\n",
    "\n",
    "4. **Risultati.**\n",
    "   ```python\n",
    "   print(\"Best Parameters:\", grid_search.best_params_)\n",
    "   print(\"Best Cross-validation Score:\", grid_search.best_score_)\n",
    "   ```\n",
    "   - Stampa i migliori parametri trovati e l'accuratezza corrispondente.\n",
    "\n",
    "---\n",
    "\n",
    "## **4.2 Definire una suite di modelli per il confronto**\n",
    "\n",
    "**Perch√© confrontare modelli diversi?**\n",
    "Ogni modello ha punti di forza e debolezze. Confrontare pi√π modelli ci consente di identificare quello pi√π adatto al nostro problema e ai nostri dati.\n",
    "\n",
    "### **Passaggi chiave nel codice:**\n",
    "1. **Definizione dei modelli.**\n",
    "   ```python\n",
    "   from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "   from sklearn.tree import DecisionTreeRegressor\n",
    "   from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "   from sklearn.svm import SVR\n",
    "   from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "   models = {\n",
    "       'Linear Regression': LinearRegression(),\n",
    "       'Ridge Regression': Ridge(),\n",
    "       'Lasso Regression': Lasso(),\n",
    "       'ElasticNet': ElasticNet(),\n",
    "       'Decision Tree': DecisionTreeRegressor(),\n",
    "       'Random Forest': RandomForestRegressor(),\n",
    "       'Gradient Boosting': GradientBoostingRegressor(),\n",
    "       'SVR': SVR(),\n",
    "       'KNN': KNeighborsRegressor()\n",
    "   }\n",
    "   ```\n",
    "   - Ogni modello ha caratteristiche uniche:\n",
    "     - **Linear Regression**: Modello base per relazioni lineari.\n",
    "     - **Ridge, Lasso, ElasticNet**: Varianti della regressione lineare con regolarizzazione.\n",
    "     - **Decision Tree**: Modello interpretabile per dati non lineari.\n",
    "     - **Random Forest, Gradient Boosting**: Ensemble di alberi decisionali.\n",
    "     - **SVR**: Support Vector Regressor per relazioni complesse.\n",
    "     - **KNN**: Modello basato sulla distanza tra i punti dati.\n",
    "\n",
    "2. **Conteggio dei modelli.**\n",
    "   ```python\n",
    "   print(f\"Number of models to compare: {len(models)}\")\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.3 Creare una funzione di valutazione del modello**\n",
    "\n",
    "**Perch√© √® importante?**\n",
    "Una funzione standard consente di valutare ogni modello utilizzando metriche consistenti, semplificando il confronto.\n",
    "\n",
    "### **Passaggi chiave:**\n",
    "1. **Definizione della funzione.**\n",
    "   ```python\n",
    "   def evaluate_model(model, X, y, cv=5):\n",
    "       kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "       neg_mse_scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "       r2_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "       mae_scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_absolute_error')\n",
    "       ev_scores = cross_val_score(model, X, y, cv=kf, scoring='explained_variance')\n",
    "\n",
    "       mse_scores = -neg_mse_scores\n",
    "       rmse_scores = np.sqrt(mse_scores)\n",
    "       mae_scores = -mae_scores\n",
    "\n",
    "       results = {\n",
    "           'RMSE': {'mean': rmse_scores.mean(), 'std': rmse_scores.std()},\n",
    "           'MAE': {'mean': mae_scores.mean(), 'std': mae_scores.std()},\n",
    "           'R¬≤': {'mean': r2_scores.mean(), 'std': r2_scores.std()},\n",
    "           'Explained Variance': {'mean': ev_scores.mean(), 'std': ev_scores.std()}\n",
    "       }\n",
    "       return results\n",
    "   ```\n",
    "   - **KFold:** Divide il dataset in pi√π fold per garantire valutazioni consistenti.\n",
    "   - **Metriche calcolate:**\n",
    "     - RMSE: Radice dell'errore quadratico medio.\n",
    "     - MAE: Errore assoluto medio.\n",
    "     - R¬≤: Coefficiente di determinazione (adattamento del modello).\n",
    "     - Explained Variance: Percentuale di varianza spiegata dal modello.\n",
    "\n",
    "---\n",
    "\n",
    "## **4.4 Valutare tutti i modelli**\n",
    "\n",
    "**Passaggi chiave:**\n",
    "1. **Ciclo sui modelli.**\n",
    "   ```python\n",
    "   all_results = {}\n",
    "\n",
    "   for name, model in models.items():\n",
    "       print(f\"Evaluating {name}...\")\n",
    "       pipeline = Pipeline([\n",
    "           ('scaler', StandardScaler()),\n",
    "           ('model', model)\n",
    "       ])\n",
    "       all_results[name] = evaluate_model(pipeline, X_train, y_train, cv=5)\n",
    "   print(\"\\nEvaluation complete!\")\n",
    "   ```\n",
    "   - **Pipeline:** Applica uno scaler (standardizzazione dei dati) prima di addestrare ogni modello.\n",
    "   - **Evaluate_model:** Usa la funzione per calcolare tutte le metriche.\n",
    "   - **all_results:** Memorizza i risultati per ogni modello.\n",
    "\n",
    "2. **Risultati.**\n",
    "   ```python\n",
    "   Evaluating Linear Regression...\n",
    "   Evaluating Ridge Regression...\n",
    "   ...\n",
    "   Evaluation complete!\n",
    "   ```\n",
    "   - Questo fornisce un confronto diretto tra i modelli.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusione**\n",
    "\n",
    "Questa strategia di selezione del modello:\n",
    "1. Trova i migliori parametri tramite Grid Search.\n",
    "2. Confronta pi√π modelli usando cross-validation e metriche standard.\n",
    "3. Standardizza la valutazione con una funzione robusta.\n",
    "\n",
    "Se desideri approfondire una specifica metrica o modello, fammi sapere!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31176ede-3045-4d93-be15-dc2fb4288c00",
   "metadata": {},
   "source": [
    "### Lezione Dettagliata: Confronto dei Modelli Utilizzando Metriche Multiple\n",
    "\n",
    "In questa parte, impariamo a **confrontare modelli di machine learning** utilizzando diverse metriche di valutazione. Creeremo visualizzazioni che consentono un confronto chiaro e tabelle riassuntive per analizzare i risultati.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Obiettivo del Confronto**\n",
    "\n",
    "Confrontare le prestazioni dei modelli ci aiuta a:\n",
    "- Identificare quale modello √® il pi√π adatto al problema.\n",
    "- Valutare i modelli in base a **metriche chiave** (ad esempio, errore, accuratezza o varianza spiegata).\n",
    "- Scegliere il miglior compromesso tra prestazioni e complessit√†.\n",
    "\n",
    "Le metriche utilizzate per i modelli di regressione sono:\n",
    "- **RMSE (Root Mean Squared Error):** Valuta l'entit√† degli errori. Pi√π basso √®, meglio √®.\n",
    "- **MAE (Mean Absolute Error):** Simile al RMSE, ma misura l'errore medio assoluto.\n",
    "- **R¬≤ (Coefficient of Determination):** Misura quanto bene il modello spiega la varianza. Pi√π alto √®, meglio √®.\n",
    "- **Explained Variance:** Percentuale di varianza spiegata dal modello.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Creazione della Funzione per il Confronto**\n",
    "\n",
    "### **a. Funzione per il grafico del confronto**\n",
    "\n",
    "La funzione `plot_model_comparison` crea un grafico a barre che confronta i modelli in base a una metrica specifica.\n",
    "\n",
    "```python\n",
    "def plot_model_comparison(results, metric):\n",
    "    \"\"\"\n",
    "    Crea un grafico a barre per confrontare i modelli su una specifica metrica.\n",
    "    \n",
    "    Parametri:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Dizionario che contiene i risultati di valutazione di tutti i modelli.\n",
    "    metric : str\n",
    "        La metrica da plottare ('RMSE', 'MAE', 'R¬≤', 'Explained Variance').\n",
    "    \"\"\"\n",
    "    # Estrazione dei valori medi e deviazioni standard\n",
    "    means = [results[model][metric]['mean'] for model in results]\n",
    "    stds = [results[model][metric]['std'] for model in results]\n",
    "    model_names = list(results.keys())\n",
    "    \n",
    "    # Ordinamento (dal migliore al peggiore)\n",
    "    if metric in ['RMSE', 'MAE']:\n",
    "        sorted_indices = np.argsort(means)  # Pi√π basso √® meglio\n",
    "        title_text = f\"Confronto Modelli per {metric} (Minore √® Meglio)\"\n",
    "    else:\n",
    "        sorted_indices = np.argsort(means)[::-1]  # Pi√π alto √® meglio\n",
    "        title_text = f\"Confronto Modelli per {metric} (Maggiore √® Meglio)\"\n",
    "    \n",
    "    # Ordinamento dei modelli e metriche\n",
    "    sorted_means = [means[i] for i in sorted_indices]\n",
    "    sorted_stds = [stds[i] for i in sorted_indices]\n",
    "    sorted_names = [model_names[i] for i in sorted_indices]\n",
    "    \n",
    "    # Creazione del grafico a barre\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.bar(sorted_names, sorted_means, yerr=sorted_stds, capsize=10,\n",
    "                  color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    # Aggiunta delle etichette e titolo\n",
    "    plt.xlabel('Modello')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(title_text)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Valori sopra le barre\n",
    "    for bar, value in zip(bars, sorted_means):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02 * max(sorted_means),\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "### **Cosa fa questa funzione?**\n",
    "1. **Estrazione dei dati:** Raccoglie le medie e le deviazioni standard per una specifica metrica.\n",
    "2. **Ordinamento:** Ordina i modelli in base ai valori della metrica (ad esempio, RMSE in ordine crescente).\n",
    "3. **Grafico a barre:** Mostra un grafico con le barre ordinate, includendo errori standard (con `yerr`).\n",
    "4. **Titoli e annotazioni:** Aggiunge etichette e i valori delle metriche sopra le barre.\n",
    "\n",
    "---\n",
    "\n",
    "### **b. Funzione per creare una tabella riassuntiva**\n",
    "\n",
    "La funzione `create_comparison_table` crea una tabella che riepiloga le prestazioni di tutti i modelli utilizzando le metriche definite.\n",
    "\n",
    "```python\n",
    "def create_comparison_table(results):\n",
    "    \"\"\"\n",
    "    Crea una tabella riepilogativa con le performance dei modelli.\n",
    "    \n",
    "    Parametri:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Dizionario con i risultati di valutazione per tutti i modelli.\n",
    "        \n",
    "    Ritorna:\n",
    "    --------\n",
    "    DataFrame : Tabella riepilogativa delle performance dei modelli.\n",
    "    \"\"\"\n",
    "    # Inizializzazione delle liste per i dati\n",
    "    models = []\n",
    "    rmse_means = []\n",
    "    rmse_stds = []\n",
    "    mae_means = []\n",
    "    mae_stds = []\n",
    "    r2_means = []\n",
    "    r2_stds = []\n",
    "    ev_means = []\n",
    "    ev_stds = []\n",
    "    \n",
    "    # Estrazione dei dati dai risultati\n",
    "    for model_name, model_results in results.items():\n",
    "        models.append(model_name)\n",
    "        rmse_means.append(model_results['RMSE']['mean'])\n",
    "        rmse_stds.append(model_results['RMSE']['std'])\n",
    "        mae_means.append(model_results['MAE']['mean'])\n",
    "        mae_stds.append(model_results['MAE']['std'])\n",
    "        r2_means.append(model_results['R¬≤']['mean'])\n",
    "        r2_stds.append(model_results['R¬≤']['std'])\n",
    "        ev_means.append(model_results['Explained Variance']['mean'])\n",
    "        ev_stds.append(model_results['Explained Variance']['std'])\n",
    "    \n",
    "    # Creazione del DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Model': models,\n",
    "        'RMSE (mean)': rmse_means,\n",
    "        'RMSE (std)': rmse_stds,\n",
    "        'MAE (mean)': mae_means,\n",
    "        'MAE (std)': mae_stds,\n",
    "        'R¬≤ (mean)': r2_means,\n",
    "        'R¬≤ (std)': r2_stds,\n",
    "        'Explained Variance (mean)': ev_means,\n",
    "        'Explained Variance (std)': ev_stds\n",
    "    })\n",
    "    \n",
    "    # Formattazione del DataFrame\n",
    "    for col in df.columns:\n",
    "        if '(mean)' in col or '(std)' in col:\n",
    "            df[col] = df[col].round(4)\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "### **Cosa fa questa funzione?**\n",
    "1. **Estrae i risultati:** Recupera le metriche per ogni modello.\n",
    "2. **Crea un DataFrame:** Organizza le informazioni in formato tabellare per facilitare l'analisi.\n",
    "3. **Formattazione:** Arrotonda i numeri a 4 decimali per una presentazione pi√π leggibile.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Confronto Completo dei Modelli**\n",
    "\n",
    "1. **Visualizzare i grafici per ogni metrica.**\n",
    "   ```python\n",
    "   for metric in ['RMSE', 'MAE', 'R¬≤', 'Explained Variance']:\n",
    "       plot_model_comparison(all_results, metric)\n",
    "   ```\n",
    "\n",
    "2. **Creare e ordinare la tabella riassuntiva.**\n",
    "   ```python\n",
    "   comparison_table = create_comparison_table(all_results)\n",
    "   comparison_table.sort_values(by='RMSE (mean)')\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusione**\n",
    "\n",
    "Grazie a queste funzioni, possiamo confrontare facilmente le prestazioni dei modelli:\n",
    "- **Grafico a barre:** Confronto visivo per singola metrica.\n",
    "- **Tabella riassuntiva:** Analisi dettagliata di tutte le metriche.\n",
    "\n",
    "Questo approccio strutturato aiuta a identificare il miglior modello per il problema specifico. Se hai bisogno di ulteriori chiarimenti o implementazioni, fammi sapere!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee095755-d479-4ccc-955d-d5e45fa70e9d",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "### **Obiettivo di questa fase**\n",
    "Quando si confrontano modelli di machine learning, √® utile analizzare le loro prestazioni su pi√π metriche (ad esempio, **RMSE**, **MAE**, **R¬≤**, e **Explained Variance**). Tuttavia, ogni metrica pu√≤ avere importanza diversa a seconda del contesto. L'obiettivo di questa sezione √®:\n",
    "- **Classificare i modelli** basandosi su un punteggio composito.\n",
    "- Considerare pi√π metriche con pesi personalizzabili.\n",
    "- Normalizzare le metriche per confrontarle correttamente.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Comprendere il Punteggio Composito**\n",
    "\n",
    "Un **punteggio composito** combina i risultati di pi√π metriche:\n",
    "1. **Normalizzazione dei valori delle metriche:** Le metriche sono scalate su un intervallo da 0 a 1, dove 1 rappresenta il risultato migliore.\n",
    "   - Per metriche come **RMSE** e **MAE**, valori pi√π bassi indicano prestazioni migliori.\n",
    "   - Per metriche come **R¬≤** e **Explained Variance**, valori pi√π alti sono migliori.\n",
    "2. **Assegnazione dei pesi:** I pesi definiscono l'importanza di ciascuna metrica.\n",
    "   - Ad esempio, possiamo dare un peso maggiore a **R¬≤** se ci interessa pi√π la varianza spiegata.\n",
    "3. **Calcolo del punteggio totale:** Il punteggio totale √® una media ponderata dei punteggi normalizzati.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Spiegazione del Codice**\n",
    "\n",
    "### **a. Funzione `rank_models`**\n",
    "\n",
    "La funzione implementa il processo descritto sopra. Ecco i passaggi principali:\n",
    "\n",
    "```python\n",
    "def rank_models(results, weights=None):\n",
    "    \"\"\"\n",
    "    Classifica i modelli basandosi su pi√π metriche con pesi opzionali.\n",
    "    \"\"\"\n",
    "    # Definizione dei pesi predefiniti (uguale importanza per ogni metrica)\n",
    "    if weights is None:\n",
    "        weights = {'RMSE': 0.25, 'MAE': 0.25, 'R¬≤': 0.25, 'Explained Variance': 0.25}\n",
    "    \n",
    "    # Controllo che la somma dei pesi sia 1\n",
    "    weight_sum = sum(weights.values())\n",
    "    if abs(weight_sum - 1.0) > 1e-10:  # Per evitare errori di precisione floating-point\n",
    "        raise ValueError(f\"I pesi devono sommare a 1, ma attualmente sommano {weight_sum}\")\n",
    "```\n",
    "\n",
    "1. **Definizione dei Pesi Predefiniti:** Se non sono forniti pesi, ogni metrica riceve un peso uguale (0.25).\n",
    "2. **Validazione dei Pesi:** Controlla che la somma dei pesi sia esattamente 1.\n",
    "\n",
    "---\n",
    "\n",
    "### **b. Normalizzazione delle Metriche**\n",
    "Le metriche sono normalizzate su una scala da 0 a 1:\n",
    "- Per metriche come RMSE e MAE (valori pi√π bassi sono migliori):\n",
    "  ```python\n",
    "  normalized = [1 - (val - min_val) / (max_val - min_val) for val in metric_values]\n",
    "  ```\n",
    "- Per metriche come R¬≤ e Explained Variance (valori pi√π alti sono migliori):\n",
    "  ```python\n",
    "  normalized = [(val - min_val) / (max_val - min_val) for val in metric_values]\n",
    "  ```\n",
    "\n",
    "Ecco un esempio:\n",
    "```python\n",
    "# Normalizzazione delle metriche\n",
    "if metric in ['RMSE', 'MAE']:  # Metriche dove valori pi√π bassi sono migliori\n",
    "    min_val = min(metric_values)\n",
    "    max_val = max(metric_values)\n",
    "    if max_val == min_val:\n",
    "        normalized = [1.0 for _ in metric_values]  # Evita la divisione per 0\n",
    "else:  # Metriche dove valori pi√π alti sono migliori\n",
    "    normalized = [(val - min_val) / (max_val - min_val) for val in metric_values]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **c. Calcolo del Punteggio Composito**\n",
    "Ogni modello riceve un punteggio calcolato come media ponderata delle metriche normalizzate.\n",
    "\n",
    "```python\n",
    "# Aggiungi il punteggio ponderato normalizzato a ciascun modello\n",
    "for i, model in enumerate(model_names):\n",
    "    normalized_scores[model] += weight * normalized[i]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **d. Creazione del DataFrame Ordinato**\n",
    "I risultati sono organizzati in un `DataFrame`, ordinati per punteggio composito in ordine decrescente.\n",
    "\n",
    "```python\n",
    "rank_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Composite Score': [normalized_scores[model] for model in model_names]\n",
    "})\n",
    "rank_df = rank_df.sort_values('Composite Score', ascending=False).reset_index(drop=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Applicazione della Funzione**\n",
    "La funzione √® chiamata sui risultati di valutazione dei modelli. Ecco come appare l'output:\n",
    "\n",
    "```python\n",
    "# Classifica i modelli con pesi predefiniti\n",
    "default_ranking = rank_models(all_results)\n",
    "print(\"Classifica dei modelli con pesi uguali:\")\n",
    "display(default_ranking)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Interpretazione del Risultato**\n",
    "\n",
    "### Tabella dei Risultati\n",
    "| **Rank** | **Model**              | **Composite Score** |\n",
    "|----------|-------------------------|----------------------|\n",
    "| 1        | Random Forest          | 1.000               |\n",
    "| 2        | SVR                    | 0.964               |\n",
    "| 3        | Gradient Boosting      | 0.956               |\n",
    "| 4        | Decision Tree          | 0.950               |\n",
    "| 5        | KNN                    | 0.887               |\n",
    "| 6        | Lasso Regression       | 0.113               |\n",
    "| 7        | ElasticNet             | 0.113               |\n",
    "| 8        | Ridge Regression       | 0.006               |\n",
    "| 9        | Linear Regression      | 0.000               |\n",
    "\n",
    "### Analisi\n",
    "1. **Modelli Migliori:** `Random Forest`, `SVR`, e `Gradient Boosting` si distinguono con punteggi compositi molto alti.\n",
    "2. **Modelli Peggiori:** `Linear Regression` e `Ridge Regression` ottengono risultati bassi a causa delle loro limitate capacit√† su dataset non lineari o complessi.\n",
    "3. **Distribuzione Equa:** I pesi sono distribuiti equamente tra le metriche; tuttavia, cambiando i pesi, √® possibile influenzare la classifica.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Personalizzazione dei Pesi**\n",
    "\n",
    "Possiamo modificare i pesi per dare maggiore importanza a metriche specifiche:\n",
    "```python\n",
    "custom_weights = {'RMSE': 0.4, 'MAE': 0.3, 'R¬≤': 0.2, 'Explained Variance': 0.1}\n",
    "custom_ranking = rank_models(all_results, weights=custom_weights)\n",
    "print(\"Classifica dei modelli con pesi personalizzati:\")\n",
    "display(custom_ranking)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusione**\n",
    "Questa tecnica fornisce un approccio strutturato per classificare i modelli utilizzando:\n",
    "1. **Metriche multiple:** Include sia metriche di errore che di spiegazione della varianza.\n",
    "2. **Pesi personalizzati:** Permette di enfatizzare alcune metriche a seconda dell'obiettivo.\n",
    "3. **Punteggio composito:** Combina tutto in un valore unico, semplificando la comparazione.\n",
    "\n",
    "Fammi sapere se desideri ulteriori esempi o approfondimenti!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe88fa-dfb6-42ac-82ae-edf7448aa4d8",
   "metadata": {},
   "source": [
    "### **Lezione dettagliata: Valutazione delle previsioni delle serie temporali**\n",
    "\n",
    "Questa lezione offre una panoramica completa su come generare, analizzare e valutare previsioni di serie temporali utilizzando metriche appropriate e considera i dati campione visualizzati nel grafico allegato. Procediamo per sezioni.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Introduzione alla valutazione delle serie temporali**\n",
    "\n",
    "Le serie temporali differiscono da altre applicazioni di machine learning per via della loro **dipendenza dal tempo**, che introduce sfide uniche. √à cruciale scegliere le metriche di valutazione giuste, poich√© queste influenzano:\n",
    "- La **selezione del modello**.\n",
    "- La **taratura dei parametri**.\n",
    "\n",
    "#### **Domande chiave per la scelta delle metriche**\n",
    "1. **Sensibilit√† alla scala:** La metrica √® influenzata dall'ampiezza dei valori?\n",
    "2. **Sensibilit√† agli outlier:** Come gestisce valori estremi?\n",
    "3. **Valori piccoli o zero:** La metrica fallisce se i dati includono zero?\n",
    "4. **Interpretabilit√†:** La metrica √® comprensibile per i decisori aziendali?\n",
    "5. **Direzionalit√†:** √à pi√π importante la direzione del valore (sovra o sottostima) rispetto alla magnitudine?\n",
    "6. **Costi asimmetrici:** Gli errori di sovra-stima e sotto-stima hanno lo stesso impatto?\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Configurazione dei dati di esempio**\n",
    "\n",
    "Il codice genera dati sintetici per simulare una serie temporale reale con trend, stagionalit√† e rumore, e aggiunge diverse versioni di previsioni per confrontarle.\n",
    "\n",
    "#### **Codice completo**\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "```\n",
    "- **`numpy`, `pandas` e `matplotlib.pyplot`**: Per manipolare i dati e creare grafici.\n",
    "- **`sklearn.metrics`**: Fornisce metriche per calcolare l'errore delle previsioni.\n",
    "- **`statsmodels.tsa.api`**: Contiene modelli comuni di serie temporali (non utilizzati in questo esempio ma disponibili per futuri approfondimenti).\n",
    "- **`warnings.filterwarnings`**: Ignora avvisi durante l'analisi.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Funzione `generate_sample_data`**\n",
    "```python\n",
    "def generate_sample_data(n=100, with_trend=True, with_seasonality=True, with_noise=True):\n",
    "    ...\n",
    "    return df\n",
    "```\n",
    "1. **Indice temporale (`time_idx`):** Genera 100 punti giornalieri partendo dal 1¬∞ gennaio 2023.\n",
    "2. **Componenti della serie temporale:**\n",
    "   - **Base (`base`):** Livello iniziale della serie (100).\n",
    "   - **Trend (`trend`):** Crescita lineare in 100 giorni (se abilitata).\n",
    "   - **Stagionalit√† (`seasonality`):** Oscillazione settimanale con ampiezza di 15.\n",
    "   - **Rumore (`noise`):** Aggiunge variazioni casuali (media = 0, deviazione standard = 5).\n",
    "3. **Previsioni sintetiche:**\n",
    "   - **`forecast_good`:** Previsione accurata con aggiunta di piccoli errori.\n",
    "   - **`forecast_biased`:** Previsione con bias (deviazione sistematica).\n",
    "   - **`forecast_no_seasonality`:** Mancanza del componente stagionale.\n",
    "   - **`forecast_scaled`:** Sovrastima sistematica (valori 20% pi√π alti).\n",
    "4. **Casi speciali:** Introduce valori di **zero** (giorni 10-15) e un **outlier** (giorno 20).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Visualizzazione dei dati generati**\n",
    "\n",
    "Il grafico allegato mostra:\n",
    "- La serie temporale originale (**Actual**, linea nera).\n",
    "- Diverse versioni di previsioni:\n",
    "  - **Good Forecast (blu):** Segue fedelmente l'andamento reale.\n",
    "  - **Biased Forecast (rosso):** Mostra un bias sistematico.\n",
    "  - **No Seasonality (verde):** Non cattura il componente stagionale.\n",
    "  - **Scaled Forecast (giallo):** Sovrastima sistematica.\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(data['ds'], data['actual'], 'k-', label='Actual')\n",
    "plt.plot(data['ds'], data['forecast_good'], 'b-', alpha=0.7, label='Good Forecast')\n",
    "plt.plot(data['ds'], data['forecast_biased'], 'r-', alpha=0.7, label='Biased Forecast')\n",
    "plt.plot(data['ds'], data['forecast_no_seasonality'], 'g-', alpha=0.7, label='No Seasonality')\n",
    "plt.plot(data['ds'], data['forecast_scaled'], 'y-', alpha=0.7, label='Scaled Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Synthetic Time Series Data with Various Forecasts')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **Osservazioni dal grafico:**\n",
    "1. La **linea nera** rappresenta i dati originali (con trend, stagionalit√† e rumore).\n",
    "2. Le previsioni forniscono confronti visivi:\n",
    "   - Alcune seguono l'andamento reale (Good Forecast).\n",
    "   - Altre commettono errori evidenti (Biased Forecast, No Seasonality).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Implicazioni e prossimi passi**\n",
    "\n",
    "**Obiettivo:**\n",
    "Valutare quale previsione √® la migliore utilizzando metriche specifiche. Nei passaggi successivi, analizzeremo come calcolare:\n",
    "- **Errore medio assoluto (MAE):** Penalizza gli errori uniformemente.\n",
    "- **Errore quadratico medio (MSE/RMSE):** Penalizza maggiormente gli errori grandi.\n",
    "- **Mean Absolute Percentage Error (MAPE):** Calcola l'errore relativo alla magnitudine.\n",
    "- **Symmetric Mean Absolute Percentage Error (sMAPE):** Versione pi√π robusta rispetto al MAPE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f875754-6fa8-4b71-89ae-8b4f51169560",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7717238d-6b3f-4732-9d59-0169ef6befaa",
   "metadata": {},
   "source": [
    "### **Lezione dettagliata: Metriche Dipendenti dalla Scala per la Valutazione delle Serie Temporali**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Introduzione**\n",
    "Le metriche **dipendenti dalla scala** sono tra le pi√π semplici e comuni per valutare la qualit√† delle previsioni nelle serie temporali. Calcolano l'errore tra i valori previsti e quelli reali nella loro scala originale. Tuttavia, non possono essere usate per confrontare previsioni provenienti da serie con **scale o unit√† diverse**.\n",
    "\n",
    "Queste metriche includono:\n",
    "1. **MAE (Mean Absolute Error):** Misura l'errore assoluto medio.\n",
    "2. **MSE (Mean Squared Error):** Penalizza gli errori grandi con il quadrato delle differenze.\n",
    "3. **RMSE (Root Mean Squared Error):** Porta l'MSE alla stessa scala dei dati.\n",
    "\n",
    "Andiamo rigo per rigo attraverso ogni metrica, il codice e i risultati ottenuti.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.1 Mean Absolute Error (MAE)**\n",
    "\n",
    "#### **Definizione**\n",
    "La **MAE** rappresenta la media delle differenze assolute tra i valori previsti e quelli reali, calcolando la distanza media tra i due insiemi di valori.\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "Dove:\n",
    "- $n$: numero totale di osservazioni.\n",
    "- $y_i$: valore reale.\n",
    "- $\\hat{y}_i$: valore previsto.\n",
    "\n",
    "#### **Codice**\n",
    "Il seguente codice implementa la MAE:\n",
    "\n",
    "```python\n",
    "def calculate_mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "```\n",
    "\n",
    "#### **Calcolo per ogni previsione**\n",
    "Applichiamo la funzione a tutte le previsioni:\n",
    "\n",
    "```python\n",
    "mae_good = calculate_mae(data['actual'], data['forecast_good'])\n",
    "mae_biased = calculate_mae(data['actual'], data['forecast_biased'])\n",
    "mae_no_seasonality = calculate_mae(data['actual'], data['forecast_no_seasonality'])\n",
    "mae_scaled = calculate_mae(data['actual'], data['forecast_scaled'])\n",
    "```\n",
    "\n",
    "#### **Output**\n",
    "Stampiamo i risultati:\n",
    "\n",
    "```python\n",
    "print(f\"MAE (Good Forecast): {mae_good:.2f}\")\n",
    "print(f\"MAE (Biased Forecast): {mae_biased:.2f}\")\n",
    "print(f\"MAE (No Seasonality): {mae_no_seasonality:.2f}\")\n",
    "print(f\"MAE (Scaled Forecast): {mae_scaled:.2f}\")\n",
    "```\n",
    "\n",
    "**Risultati:**\n",
    "- **Good Forecast:** 15.80\n",
    "- **Biased Forecast:** 21.63\n",
    "- **No Seasonality:** 19.27\n",
    "- **Scaled Forecast:** 31.33\n",
    "\n",
    "#### **Osservazioni**\n",
    "- La previsione \"Good Forecast\" ha il **valore pi√π basso di MAE**, indicando che ha l'errore assoluto medio pi√π ridotto.\n",
    "- La previsione \"Scaled Forecast\" ha il valore pi√π alto, mostrando errori significativi.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2 Mean Squared Error (MSE)**\n",
    "\n",
    "#### **Definizione**\n",
    "La **MSE** rappresenta la media dei quadrati delle differenze tra i valori reali e quelli previsti. Penalizza maggiormente gli errori grandi, rendendola particolarmente sensibile agli **outlier**.\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "#### **Codice**\n",
    "Il codice per calcolare la MSE √®:\n",
    "\n",
    "```python\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    return np.mean(np.square(y_true - y_pred))\n",
    "```\n",
    "\n",
    "#### **Calcolo per ogni previsione**\n",
    "Applichiamo la funzione per calcolare la MSE:\n",
    "\n",
    "```python\n",
    "mse_good = calculate_mse(data['actual'], data['forecast_good'])\n",
    "mse_biased = calculate_mse(data['actual'], data['forecast_biased'])\n",
    "mse_no_seasonality = calculate_mse(data['actual'], data['forecast_no_seasonality'])\n",
    "mse_scaled = calculate_mse(data['actual'], data['forecast_scaled'])\n",
    "```\n",
    "\n",
    "#### **Output**\n",
    "Stampiamo i risultati:\n",
    "\n",
    "```python\n",
    "print(f\"MSE (Good Forecast): {mse_good:.2f}\")\n",
    "print(f\"MSE (Biased Forecast): {mse_biased:.2f}\")\n",
    "print(f\"MSE (No Seasonality): {mse_no_seasonality:.2f}\")\n",
    "print(f\"MSE (Scaled Forecast): {mse_scaled:.2f}\")\n",
    "```\n",
    "\n",
    "**Risultati:**\n",
    "- **Good Forecast:** 1510.39\n",
    "- **Biased Forecast:** 1692.43\n",
    "- **No Seasonality:** 1747.15\n",
    "- **Scaled Forecast:** 2105.69\n",
    "\n",
    "#### **Osservazioni**\n",
    "- L'MSE evidenzia chiaramente l'impatto degli errori grandi, con valori pi√π alti per previsioni meno accurate (es. \"Scaled Forecast\").\n",
    "- Penalizza pi√π severamente gli errori rispetto alla MAE.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.3 Root Mean Squared Error (RMSE)**\n",
    "\n",
    "#### **Definizione**\n",
    "La **RMSE** √® semplicemente la radice quadrata della MSE. Riporta l'errore alla scala originale dei dati, rendendolo pi√π interpretabile rispetto alla MSE.\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "#### **Codice**\n",
    "Il codice per calcolare la RMSE √®:\n",
    "\n",
    "```python\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    return np.sqrt(calculate_mse(y_true, y_pred))\n",
    "```\n",
    "\n",
    "#### **Calcolo per ogni previsione**\n",
    "Applichiamo la funzione per calcolare la RMSE:\n",
    "\n",
    "```python\n",
    "rmse_good = calculate_rmse(data['actual'], data['forecast_good'])\n",
    "rmse_biased = calculate_rmse(data['actual'], data['forecast_biased'])\n",
    "rmse_no_seasonality = calculate_rmse(data['actual'], data['forecast_no_seasonality'])\n",
    "rmse_scaled = calculate_rmse(data['actual'], data['forecast_scaled'])\n",
    "```\n",
    "\n",
    "#### **Output**\n",
    "Stampiamo i risultati:\n",
    "\n",
    "```python\n",
    "print(f\"RMSE (Good Forecast): {rmse_good:.2f}\")\n",
    "print(f\"RMSE (Biased Forecast): {rmse_biased:.2f}\")\n",
    "print(f\"RMSE (No Seasonality): {rmse_no_seasonality:.2f}\")\n",
    "print(f\"RMSE (Scaled Forecast): {rmse_scaled:.2f}\")\n",
    "```\n",
    "\n",
    "**Risultati:**\n",
    "- **Good Forecast:** 38.86\n",
    "- **Biased Forecast:** 41.14\n",
    "- **No Seasonality:** 41.80\n",
    "- **Scaled Forecast:** 45.89\n",
    "\n",
    "#### **Osservazioni**\n",
    "La **Good Forecast** si conferma ancora come la pi√π accurata, mentre la \"Scaled Forecast\" √® la peggiore.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Confronto delle Metriche**\n",
    "\n",
    "Raccogliamo tutte le metriche in una tabella per confrontarle:\n",
    "\n",
    "```python\n",
    "scale_dependent_metrics = pd.DataFrame({\n",
    "    'Forecast Type': ['Good', 'Biased', 'No Seasonality', 'Scaled'],\n",
    "    'MAE': [mae_good, mae_biased, mae_no_seasonality, mae_scaled],\n",
    "    'MSE': [mse_good, mse_biased, mse_no_seasonality, mse_scaled],\n",
    "    'RMSE': [rmse_good, rmse_biased, rmse_no_seasonality, rmse_scaled]\n",
    "})\n",
    "\n",
    "scale_dependent_metrics.set_index('Forecast Type', inplace=True)\n",
    "scale_dependent_metrics\n",
    "```\n",
    "\n",
    "**Tabella dei Risultati:**\n",
    "\n",
    "| **Forecast Type**   | **MAE**  | **MSE**      | **RMSE**   |\n",
    "|----------------------|----------|--------------|------------|\n",
    "| **Good**            | 15.80    | 1510.39      | 38.86      |\n",
    "| **Biased**          | 21.63    | 1692.43      | 41.14      |\n",
    "| **No Seasonality**  | 19.27    | 1747.15      | 41.80      |\n",
    "| **Scaled**          | 31.33    | 2105.69      | 45.89      |\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Conclusioni**\n",
    "\n",
    "- **La \"Good Forecast\" √® la pi√π precisa** su tutte le metriche.\n",
    "- La **\"Scaled Forecast\" √® la peggiore**, con errori significativi.\n",
    "- Le metriche MSE e RMSE penalizzano maggiormente gli errori grandi rispetto alla MAE.\n",
    "- Queste metriche sono ideali per analisi a scala unica ma non adatte per confrontare serie con scale diverse.\n",
    "\n",
    "Se vuoi esplorare metriche **indipendenti dalla scala** (come il MAPE), fammi sapere!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c07e8c-ca48-466a-8af2-4dd8415c5dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88be028a-9727-4ad4-b3d8-2f3810541f51",
   "metadata": {},
   "source": [
    "**Percentage Errors** e **Scaled Errors** per la valutazione delle previsioni di serie temporali. Approfondiremo ogni metrica e ne analizzeremo implementazione, applicazione e interpretazione.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Percentage Errors**\n",
    "\n",
    "Le percentuali d'errore esprimono l'errore come una proporzione dei valori reali, rendendole **indipendenti dalla scala**. Questo le rende utili per confrontare previsioni su serie temporali con diverse unit√† o scale. Tuttavia, presentano problemi con valori prossimi a zero.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1 Mean Absolute Percentage Error (MAPE)**\n",
    "\n",
    "#### **Definizione**\n",
    "La **MAPE** misura l'errore medio assoluto in percentuale rispetto ai valori reali. √à facilmente comprensibile ma pu√≤:\n",
    "- **Non essere definita** per valori reali uguali a zero.\n",
    "- Essere **distorta** verso previsioni pi√π basse.\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{MAPE} = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|$$\n",
    "\n",
    "Dove:\n",
    "- $y_i$: valore reale.\n",
    "- $\\hat{y}_i$: valore previsto.\n",
    "- $n$: numero di osservazioni.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Codice**\n",
    "Il codice per calcolare la MAPE gestisce valori zero con un filtro e un termine aggiuntivo (`epsilon`) per prevenire divisioni per zero:\n",
    "\n",
    "```python\n",
    "def calculate_mape(y_true, y_pred, epsilon=1e-10):\n",
    "    # Filtra i valori zero\n",
    "    mask = y_true != 0\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    \n",
    "    if len(y_true_filtered) == 0:\n",
    "        return np.nan  # Ritorna NaN se tutti i valori reali sono zero\n",
    "    \n",
    "    return 100 * np.mean(np.abs((y_true_filtered - y_pred_filtered) / (y_true_filtered + epsilon)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Calcolo per ogni previsione**\n",
    "Applichiamo la funzione a tutte le previsioni:\n",
    "\n",
    "```python\n",
    "mape_good = calculate_mape(data['actual'], data['forecast_good'])\n",
    "mape_biased = calculate_mape(data['actual'], data['forecast_biased'])\n",
    "mape_no_seasonality = calculate_mape(data['actual'], data['forecast_no_seasonality'])\n",
    "mape_scaled = calculate_mape(data['actual'], data['forecast_scaled'])\n",
    "```\n",
    "\n",
    "#### **Output**\n",
    "Stampiamo i risultati:\n",
    "\n",
    "```python\n",
    "print(f\"MAPE (Good Forecast): {mape_good:.2f}%\")\n",
    "print(f\"MAPE (Biased Forecast): {mape_biased:.2f}%\")\n",
    "print(f\"MAPE (No Seasonality): {mape_no_seasonality:.2f}%\")\n",
    "print(f\"MAPE (Scaled Forecast): {mape_scaled:.2f}%\")\n",
    "```\n",
    "\n",
    "**Risultati:**\n",
    "- **Good Forecast:** 7.35%\n",
    "- **Biased Forecast:** 12.09%\n",
    "- **No Seasonality:** 10.00%\n",
    "- **Scaled Forecast:** 20.53%\n",
    "\n",
    "---\n",
    "\n",
    "#### **Osservazioni**\n",
    "- La \"Good Forecast\" ha la **MAPE pi√π bassa**, confermando la sua accuratezza.\n",
    "- La \"Scaled Forecast\" √® la peggiore, con il doppio dell'errore rispetto alla \"No Seasonality\".\n",
    "\n",
    "---\n",
    "\n",
    "### **4.2 Symmetric Mean Absolute Percentage Error (SMAPE)**\n",
    "\n",
    "#### **Definizione**\n",
    "La **SMAPE** √® una versione simmetrica della MAPE. Considera la media aritmetica tra il valore reale e quello previsto, trattando **sovrastima e sottostima** in modo equo. √à limitata tra **0% e 200%**.\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{SMAPE} = \\frac{200\\%}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{|y_i| + |\\hat{y}_i|}$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Codice**\n",
    "Anche in questo caso, gestiamo i valori zero con un filtro:\n",
    "\n",
    "```python\n",
    "def calculate_smape(y_true, y_pred, epsilon=1e-10):\n",
    "    # Gestisce i casi in cui sia y_true che y_pred sono zero\n",
    "    mask = (np.abs(y_true) + np.abs(y_pred)) != 0\n",
    "    \n",
    "    if np.sum(mask) == 0:\n",
    "        return 0.0  # Nessun errore se tutti i valori sono zero\n",
    "    \n",
    "    return 200 * np.mean(np.abs(y_pred[mask] - y_true[mask]) / (np.abs(y_true[mask]) + np.abs(y_pred[mask]) + epsilon))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Calcolo per ogni previsione**\n",
    "Applichiamo la funzione a tutte le previsioni:\n",
    "\n",
    "```python\n",
    "smape_good = calculate_smape(data['actual'], data['forecast_good'])\n",
    "smape_biased = calculate_smape(data['actual'], data['forecast_biased'])\n",
    "smape_no_seasonality = calculate_smape(data['actual'], data['forecast_no_seasonality'])\n",
    "smape_scaled = calculate_smape(data['actual'], data['forecast_scaled'])\n",
    "```\n",
    "\n",
    "#### **Output**\n",
    "Stampiamo i risultati:\n",
    "\n",
    "```python\n",
    "print(f\"SMAPE (Good Forecast): {smape_good:.2f}%\")\n",
    "print(f\"SMAPE (Biased Forecast): {smape_biased:.2f}%\")\n",
    "print(f\"SMAPE (No Seasonality): {smape_no_seasonality:.2f}%\")\n",
    "print(f\"SMAPE (Scaled Forecast): {smape_scaled:.2f}%\")\n",
    "```\n",
    "\n",
    "**Risultati:**\n",
    "- **Good Forecast:** 19.30%\n",
    "- **Biased Forecast:** 23.34%\n",
    "- **No Seasonality:** 21.65%\n",
    "- **Scaled Forecast:** 29.97%\n",
    "\n",
    "---\n",
    "\n",
    "#### **Osservazioni**\n",
    "- La SMAPE considera sia valori reali che previsti. √à pi√π **robusta rispetto a outlier**.\n",
    "- Mentre la **Good Forecast** resta la migliore, la **Scaled Forecast** √® meno penalizzata rispetto alla MAPE.\n",
    "\n",
    "---\n",
    "\n",
    "### **Confronto delle metriche percentuali**\n",
    "\n",
    "Raccogliamo i risultati in un DataFrame:\n",
    "\n",
    "```python\n",
    "percentage_metrics = pd.DataFrame({\n",
    "    'Forecast Type': ['Good', 'Biased', 'No Seasonality', 'Scaled'],\n",
    "    'MAPE (%)': [mape_good, mape_biased, mape_no_seasonality, mape_scaled],\n",
    "    'SMAPE (%)': [smape_good, smape_biased, smape_no_seasonality, smape_scaled]\n",
    "})\n",
    "\n",
    "percentage_metrics.set_index('Forecast Type', inplace=True)\n",
    "percentage_metrics\n",
    "```\n",
    "\n",
    "**Tabella finale:**\n",
    "\n",
    "| **Forecast Type**   | **MAPE (%)** | **SMAPE (%)** |\n",
    "|----------------------|--------------|---------------|\n",
    "| **Good**            | 7.35%        | 19.30%        |\n",
    "| **Biased**          | 12.09%       | 23.34%        |\n",
    "| **No Seasonality**  | 10.00%       | 21.65%        |\n",
    "| **Scaled**          | 20.53%       | 29.97%        |\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Scaled Errors**\n",
    "\n",
    "Le scaled errors confrontano gli errori del modello con un metodo base (come un forecast na√Øve), fornendo un'indicazione relativa della performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.1 Mean Absolute Scaled Error (MASE)**\n",
    "\n",
    "#### **Definizione**\n",
    "La **MASE** confronta la MAE di una previsione con la MAE di un forecast na√Øve (ad esempio, il valore precedente o lo stesso periodo della stagione precedente).\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{MASE} = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|}{\\frac{1}{T-m} \\sum_{t=m+1}^{T} |y_t - y_{t-m}|}$$\n",
    "\n",
    "Dove:\n",
    "- $m$: periodo stagionale.\n",
    "- $T$: numero di osservazioni nel dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Codice**\n",
    "La funzione per calcolare la MASE:\n",
    "\n",
    "```python\n",
    "def calculate_mase(y_true, y_pred, seasonal_period=1):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Errori della previsione\n",
    "    forecast_errors = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # Errori del forecast na√Øve\n",
    "    naive_errors = np.abs(y_true[seasonal_period:] - y_true[:-seasonal_period])\n",
    "    \n",
    "    if len(naive_errors) == 0 or np.sum(naive_errors) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return np.mean(forecast_errors) / np.mean(naive_errors)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Interpretazione**\n",
    "- **MASE < 1:** La previsione √® migliore del benchmark na√Øve.\n",
    "- **MASE = 1:** La previsione √® uguale al benchmark na√Øve.\n",
    "- **MASE > 1:** La previsione √® peggiore del benchmark na√Øve.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5a4ec-1b72-433d-bbd5-b6aafcf86197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40fc64bd-21cd-4a05-9af1-6dce98aaf7fd",
   "metadata": {},
   "source": [
    "Ecco una spiegazione dettagliata **rigo per rigo** sul calcolo e l'utilizzo della metrica **Continuous Ranked Probability Score (CRPS)** per la valutazione delle previsioni probabilistiche. Procediamo analizzando il codice, il concetto matematico e i grafici allegati.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduzione: Metrica CRPS**\n",
    "\n",
    "La **Continuous Ranked Probability Score (CRPS)** √® una metrica che confronta una distribuzione di probabilit√† prevista (cumulative distribution function o CDF) con un valore osservato reale.\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{CRPS} = \\int_{-\\infty}^{\\infty} (F(y) - \\mathbf{1}\\{y \\geq y^o\\})^2 dy$$\n",
    "\n",
    "Dove:\n",
    "- $F(y)$ √® la CDF della distribuzione prevista.\n",
    "- $\\mathbf{1}\\{y \\geq y^o\\}$ √® una funzione indicatrice (vale 1 se $y \\geq y^o$, altrimenti 0).\n",
    "- $y^o$ √® il valore osservato.\n",
    "\n",
    "### **Propriet√†**\n",
    "- √à una **regola di scoring appropriata**: premia modelli che producono distribuzioni probabilistiche affidabili.\n",
    "- **Riduce alla MAE** per previsioni deterministiche.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Implementazione del CRPS (semplificata)**\n",
    "\n",
    "### **a. Calcolo tramite Monte Carlo**\n",
    "\n",
    "Questo approccio approssima il CRPS generando campioni dalla distribuzione normale prevista:\n",
    "\n",
    "```python\n",
    "def calculate_crps(actual, forecast_mean, forecast_std, n_samples=1000):\n",
    "    \"\"\"Calcola il CRPS tramite campioni Monte Carlo\"\"\"\n",
    "    crps_values = []\n",
    "    \n",
    "    for i in range(len(actual)):  # Itera su ogni punto temporale\n",
    "        # Genera campioni dalla distribuzione normale\n",
    "        samples = np.random.normal(forecast_mean[i], forecast_std[i], n_samples)\n",
    "        samples.sort()  # Ordina i campioni per calcolare la CDF empirica\n",
    "        \n",
    "        # Inizia a calcolare il CRPS per l'osservazione attuale\n",
    "        crps_i = 0\n",
    "        for j in range(n_samples-1):  # Itera sui campioni successivi\n",
    "            width = samples[j+1] - samples[j]  # Larghezza dell'intervallo\n",
    "            cdf_value = (j+1) / n_samples  # Valore CDF empirico\n",
    "            indicator = 1.0 if samples[j] >= actual[i] else 0.0\n",
    "            height = (cdf_value - indicator) ** 2  # Differenza tra CDF e indicatore\n",
    "            crps_i += width * height  # Aggiungi l'area all'integrale\n",
    "        \n",
    "        crps_values.append(crps_i)\n",
    "    \n",
    "    return np.mean(crps_values)  # Restituisci la media su tutti i punti\n",
    "```\n",
    "\n",
    "**Spiegazione del codice:**\n",
    "- **Generazione dei campioni:** I campioni sono generati usando una distribuzione normale centrata sul valore previsto con una deviazione standard.\n",
    "- **CDF empirica:** La CDF √® approssimata ordinando i campioni e calcolando i valori cumulativi.\n",
    "- **Indicator function:** Per ogni valore, determina se √® maggiore o uguale all'osservazione reale.\n",
    "- **CRPS finale:** Calcolato come media degli errori su tutti i punti temporali.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Applicazione e risultati**\n",
    "\n",
    "### **a. Simulazione dei dati**\n",
    "I dati di input includono le osservazioni reali (`actual`) e una previsione media (`forecast_good`). La deviazione standard delle previsioni √® stimata come il **10%** dei valori previsti:\n",
    "\n",
    "```python\n",
    "forecast_std = data['forecast_good'] * 0.1\n",
    "```\n",
    "\n",
    "### **b. Calcolo del CRPS**\n",
    "Applichiamo la funzione:\n",
    "\n",
    "```python\n",
    "crps = calculate_crps(data['actual'], data['forecast_good'], forecast_std)\n",
    "print(f\"CRPS: {crps:.4f}\")\n",
    "```\n",
    "\n",
    "**Risultato:**  \n",
    "- **CRPS = 6.9416**\n",
    "\n",
    "#### **Interpretazione**\n",
    "Un valore di CRPS pi√π basso indica che la distribuzione probabilistica prevista √® pi√π compatibile con i valori osservati.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Visualizzazione: Previsioni probabilistiche**\n",
    "\n",
    "### **a. Previsioni con intervallo di confidenza (90%)**\n",
    "\n",
    "#### **Codice**\n",
    "Il grafico mostra le previsioni medie con un intervallo di confidenza del **90%**:\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(data['actual'], label='Actual')\n",
    "plt.plot(data['forecast_good'], label='Forecast Mean')\n",
    "\n",
    "upper_90 = data['forecast_good'] + 1.645 * forecast_std\n",
    "lower_90 = data['forecast_good'] - 1.645 * forecast_std\n",
    "plt.fill_between(range(len(data['forecast_good'])), lower_90, upper_90, alpha=0.2, color='blue', label='90% Prediction Interval')\n",
    "\n",
    "plt.title('Probabilistic Forecast with 90% Prediction Interval')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### **Grafico interpretato**\n",
    "- La **linea arancione** rappresenta la previsione media.\n",
    "- La **zona azzurra** indica l'intervallo di confidenza del 90%.\n",
    "- La **linea blu scura** rappresenta i valori reali.\n",
    "\n",
    "**Osservazione:**  \n",
    "Le osservazioni reali spesso rientrano nell'intervallo di confidenza, confermando la validit√† del modello probabilistico.\n",
    "\n",
    "---\n",
    "\n",
    "### **b. Empirical CDF a pi√π intervalli temporali**\n",
    "\n",
    "#### **Codice**\n",
    "Per analizzare specifiche previsioni, tracciamo la **CDF empirica** della distribuzione prevista rispetto al valore osservato per alcuni punti temporali:\n",
    "\n",
    "```python\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate([0, 5, 10, 30]):\n",
    "    samples = np.random.normal(data['forecast_good'][idx], forecast_std[idx], 1000)\n",
    "    samples.sort()\n",
    "    cdf = np.arange(1, len(samples) + 1) / len(samples)\n",
    "    axes[i].plot(samples, cdf, label='Forecast CDF')\n",
    "    \n",
    "    x_values = np.linspace(min(samples), max(samples), 1000)\n",
    "    y_values = np.where(x_values >= data['actual'][idx], 1, 0)\n",
    "    axes[i].plot(x_values, y_values, 'r--', label='Observed Value')\n",
    "    \n",
    "    axes[i].axvline(data['actual'][idx], color='r', linestyle=':')\n",
    "    axes[i].set_title(f'Time {idx}: Forecast vs. Actual')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Cumulative Probability')\n",
    "    axes[i].grid(True)\n",
    "    axes[i].legend()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Grafici interpretati**\n",
    "1. Ogni grafico mostra:\n",
    "   - La **CDF empirica** della previsione (linea blu).\n",
    "   - Il valore osservato come funzione indicatrice (linea tratteggiata rossa).\n",
    "   - La linea verticale rossa indica il valore osservato.\n",
    "\n",
    "2. **Confronto:**  \n",
    "Questi grafici evidenziano quanto la distribuzione prevista si allinei al valore osservato per punti temporali specifici.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Conclusioni**\n",
    "\n",
    "- **CRPS:** Valuta l'intera distribuzione probabilistica. Un CRPS pi√π basso indica migliori previsioni probabilistiche.\n",
    "- **Intervalli di confidenza:** Offrono una rappresentazione visiva dell'incertezza nelle previsioni.\n",
    "- **CDF empirica:** Confronta la distribuzione prevista con l'osservazione reale, fornendo intuizioni dettagliate.\n",
    "\n",
    "Se hai bisogno di ulteriori dettagli o di un confronto con altre metriche probabilistiche, fammi sapere!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f97b9-f581-4ea3-a0d1-cf29bf739227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fab99130-a0df-492c-b689-e4ddf74f5b09",
   "metadata": {},
   "source": [
    " metrica **Weighted MAPE (WMAPE)**, \n",
    "---\n",
    "\n",
    "## **7.1 Weighted MAPE**\n",
    "\n",
    "### **Che cos'√® il Weighted MAPE (WMAPE)?**\n",
    "Il **Weighted MAPE** √® una variante della **Mean Absolute Percentage Error (MAPE)** che assegna **pesi differenti** ai periodi temporali, permettendo di enfatizzare i periodi pi√π importanti in base al contesto. Questa metrica √® particolarmente utile nelle serie temporali con:\n",
    "- **Osservazioni critiche** (es. periodi recenti o stagioni rilevanti).\n",
    "- **Errori con importanze diverse** a seconda del tempo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula del WMAPE**\n",
    "\n",
    "$$\\text{WMAPE} = \\frac{\\sum_{i=1}^{n} w_i |y_i - \\hat{y}_i|}{\\sum_{i=1}^{n} w_i |y_i|}$$\n",
    "\n",
    "#### Significato dei termini:\n",
    "- $w_i$: peso assegnato all'osservazione al tempo $i$.\n",
    "- $y_i$: valore osservato reale.\n",
    "- $\\hat{y}_i$: valore previsto.\n",
    "- $n$: numero totale di osservazioni.\n",
    "\n",
    "**Cosa fa il WMAPE:**\n",
    "- Il numeratore √® la somma degli errori assoluti, ponderati dai pesi ($w_i$).\n",
    "- Il denominatore scala questi errori rispetto alla magnitudine dei valori osservati reali, anche questa ponderata.\n",
    "\n",
    "---\n",
    "\n",
    "## **Implementazione: Funzione WMAPE**\n",
    "\n",
    "### **Codice**\n",
    "\n",
    "Ecco la funzione Python per calcolare il WMAPE:\n",
    "\n",
    "```python\n",
    "def wmape(y_true, y_pred, weights=None):\n",
    "    \"\"\"Calcola la Weighted Mean Absolute Percentage Error (WMAPE).\n",
    "    \n",
    "    Parametri:\n",
    "    - y_true (array-like): Valori osservati reali.\n",
    "    - y_pred (array-like): Valori previsti.\n",
    "    - weights (array-like, opzionale): Pesi per ogni osservazione.\n",
    "    \n",
    "    Ritorna:\n",
    "    - float: Valore del WMAPE.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.ones_like(y_true)  # Usa pesi uniformi (uguali per tutti i periodi)\n",
    "        \n",
    "    # Conversione in array numpy per calcoli efficienti\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Evita la divisione per zero (considera solo i valori reali non nulli)\n",
    "    mask = y_true != 0  # Filtra i valori reali che non sono zero\n",
    "    \n",
    "    return np.sum(weights[mask] * np.abs(y_true[mask] - y_pred[mask])) / np.sum(weights[mask] * np.abs(y_true[mask]))\n",
    "```\n",
    "\n",
    "### **Riga per riga**\n",
    "1. **Gestione dei pesi:**  \n",
    "   Se non vengono forniti pesi, la funzione assegna un peso uguale a tutte le osservazioni (`np.ones_like(y_true)`).\n",
    "   \n",
    "2. **Maschera per evitare divisioni per zero:**  \n",
    "   Il filtro `mask = y_true != 0` rimuove i valori reali uguali a zero per evitare errori di calcolo.\n",
    "\n",
    "3. **Calcolo del numeratore:**  \n",
    "   $\\sum w_i |y_i - \\hat{y}_i|$ calcola l'errore assoluto ponderato.\n",
    "\n",
    "4. **Calcolo del denominatore:**  \n",
    "   $\\sum w_i |y_i|$ normalizza l'errore rispetto alla magnitudine reale dei dati, rendendo il valore della metrica **indipendente dalla scala**.\n",
    "\n",
    "5. **Valore finale del WMAPE:**  \n",
    "   Restituisce il rapporto tra il numeratore (errori ponderati) e il denominatore (magnitudine ponderata).\n",
    "\n",
    "---\n",
    "\n",
    "## **Applicazione del WMAPE**\n",
    "\n",
    "### **1. Creare Pesi Personalizzati**\n",
    "Per enfatizzare osservazioni pi√π recenti:\n",
    "```python\n",
    "n = data.shape[0]  # Numero totale di osservazioni\n",
    "recency_weights = np.linspace(0.5, 1.5, n)  # Pesi che aumentano con il tempo\n",
    "```\n",
    "\n",
    "Qui, i pesi:\n",
    "- Partono da **0.5** per le prime osservazioni.\n",
    "- Aumentano linearmente fino a **1.5** per le osservazioni pi√π recenti.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Calcolo del WMAPE**\n",
    "\n",
    "#### **a. MAPE standard (pesi uguali)**\n",
    "\n",
    "Calcoliamo il MAPE standard assegnando pesi uniformi (nessun peso speciale):\n",
    "\n",
    "```python\n",
    "standard_mape = wmape(data['actual'], data['forecast_good'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **b. WMAPE con pesi per osservazioni recenti**\n",
    "\n",
    "Calcoliamo il WMAPE enfatizzando i dati recenti con `recency_weights`:\n",
    "\n",
    "```python\n",
    "recency_wmape = wmape(data['actual'], data['forecast_good'], recency_weights)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Risultati**\n",
    "\n",
    "Stampiamo i risultati calcolati per confronto:\n",
    "\n",
    "```python\n",
    "print(f\"Standard MAPE: {standard_mape:.4f}\")\n",
    "print(f\"Recency-Weighted MAPE: {recency_wmape:.4f}\")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "- **Standard MAPE:** 0.0885\n",
    "- **Recency-Weighted MAPE:** 0.0790\n",
    "\n",
    "---\n",
    "\n",
    "## **Interpretazione dei Risultati**\n",
    "\n",
    "1. **Standard MAPE:**\n",
    "   - Assegna lo stesso peso a tutte le osservazioni.\n",
    "   - Valore: **0.0885**.\n",
    "   - Rappresenta l'errore medio assoluto in percentuale senza alcuna considerazione di importanza temporale.\n",
    "\n",
    "2. **Recency-Weighted MAPE:**\n",
    "   - Aumenta l'importanza delle osservazioni pi√π recenti.\n",
    "   - Valore pi√π basso: **0.0790**.\n",
    "   - Indica che le previsioni si allineano meglio ai dati recenti, grazie all'enfasi sui periodi temporali recenti.\n",
    "\n",
    "---\n",
    "\n",
    "## **Vantaggi e Limitazioni del WMAPE**\n",
    "\n",
    "### **Vantaggi**\n",
    "- **Flessibilit√†:** Permette di adattare i pesi in base al contesto (es. dati recenti, picchi stagionali).\n",
    "- **Indipendenza dalla scala:** √à una metrica normalizzata e confrontabile tra serie diverse.\n",
    "- **Personalizzazione:** Pu√≤ integrare priorit√† aziendali o operative.\n",
    "\n",
    "### **Limitazioni**\n",
    "- Richiede la definizione appropriata dei **pesi ($w_i$)**, che pu√≤ essere soggettiva.\n",
    "- Pu√≤ essere sensibile ai valori estremi nei pesi o nei dati osservati.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953e05a-d68d-454c-939d-644f29f9cb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af945c5c-4172-49fe-ae72-859580846cd2",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "---\n",
    "\n",
    "### **9.1 Caricamento del Dataset e Generazione Dati di Vendita**  \n",
    "\n",
    "```python\n",
    "np.random.seed(42)\n",
    "date_rng = pd.date_range(start='2019-01-01', end='2022-12-31', freq='D')\n",
    "df = pd.DataFrame(date_rng, columns=['date'])\n",
    "```\n",
    "- Imposta un seme (`42`) per la generazione casuale dei dati, garantendo risultati riproducibili.  \n",
    "- Crea una sequenza di date (`date_rng`) tra il 1¬∞ gennaio 2019 e il 31 dicembre 2022 con frequenza giornaliera.  \n",
    "- Inizializza un DataFrame `df` e assegna le date alla colonna `'date'`.  \n",
    "\n",
    "```python\n",
    "level = 1000  # Livello base delle vendite\n",
    "trend = np.linspace(0, 500, len(date_rng))  # Tendenza crescente (linearmente) nel tempo\n",
    "```\n",
    "- Definisce un valore base (`1000`) per le vendite iniziali.  \n",
    "- Genera un trend lineare che aumenta di 500 unit√† nell'intero periodo.  \n",
    "\n",
    "```python\n",
    "weekly = 50 * np.sin(np.arange(len(date_rng)) * (2 * np.pi / 7))\n",
    "yearly = 300 * np.sin(np.arange(len(date_rng)) * (2 * np.pi / 365))\n",
    "```\n",
    "- Introduce una stagionalit√† settimanale con un'oscillazione sinusoidale.  \n",
    "- Aggiunge una stagionalit√† annuale con un‚Äôoscillazione pi√π ampia (¬±300).  \n",
    "\n",
    "```python\n",
    "noise = np.random.normal(0, 50, len(date_rng))  # Rumore casuale con media 0 e deviazione standard 50\n",
    "df['sales'] = level + trend + weekly + yearly + noise  # Combina tutti i componenti\n",
    "df['sales'] = df['sales'].clip(lower=0)  # Evita valori negativi\n",
    "```\n",
    "- Crea una componente casuale (`noise`).  \n",
    "- Somma le varie componenti per ottenere i valori di vendita finali.  \n",
    "- Imposta un limite minimo di `0` per evitare vendite negative.  \n",
    "\n",
    "```python\n",
    "df.set_index('date', inplace=True)  # Imposta 'date' come indice del DataFrame\n",
    "df.head()  # Mostra le prime righe\n",
    "```\n",
    "- Definisce la colonna `date` come indice della serie temporale.  \n",
    "- Visualizza i primi 5 valori del DataFrame.  \n",
    "\n",
    "---\n",
    "\n",
    "### **9.2 Visualizzazione del Dataset**  \n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df.index, df['sales'])\n",
    "plt.title('Daily Retail Sales Data (2019-2022)', fontsize=15)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Sales', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "- Crea un grafico di dimensioni 14x7.  \n",
    "- Plotta le vendite (`sales`) nel tempo.  \n",
    "- Aggiunge titolo, etichette e griglia per migliorare la leggibilit√†.  \n",
    "\n",
    "---\n",
    "\n",
    "### **9.3 Divisione del Dataset in Training e Test**  \n",
    "\n",
    "```python\n",
    "train = df[:'2022-09-30']\n",
    "test = df['2022-10-01':]\n",
    "```\n",
    "- Divide i dati in:\n",
    "  - **Training set:** fino al 30 settembre 2022.  \n",
    "  - **Test set:** dal 1¬∞ ottobre 2022.  \n",
    "\n",
    "```python\n",
    "print(f\"Training data: {train.shape[0]} days\")\n",
    "print(f\"Testing data: {test.shape[0]} days\")\n",
    "```\n",
    "- Stampa il numero di giorni nel training e test set.  \n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(train.index, train['sales'], label='Training Data')\n",
    "plt.plot(test.index, test['sales'], label='Testing Data', color='red')\n",
    "plt.title('Train-Test Split for Retail Sales Data', fontsize=15)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Sales', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "- Visualizza la suddivisione del dataset:  \n",
    "  - I dati di **training** in blu.  \n",
    "  - I dati di **test** in rosso.  \n",
    "\n",
    "---\n",
    "\n",
    "### **9.4 Implementazione dei Modelli di Previsione**  \n",
    "\n",
    "```python\n",
    "forecasts = pd.DataFrame(index=test.index)\n",
    "forecasts['actual'] = test['sales']\n",
    "```\n",
    "- Crea un nuovo DataFrame per memorizzare le previsioni.  \n",
    "- Aggiunge la colonna `'actual'` con i valori reali di vendita.  \n",
    "\n",
    "#### **1. Naive Forecast (Ultimo Valore Osservato)**  \n",
    "```python\n",
    "forecasts['naive'] = train['sales'].iloc[-1]\n",
    "```\n",
    "- Assegna l'ultimo valore osservato del training set come previsione costante.  \n",
    "\n",
    "#### **2. Seasonal Naive (Stessa Vendita della Settimana Precedente)**  \n",
    "```python\n",
    "forecasts['seasonal_naive'] = [train['sales'].iloc[-(i % 7 + 7)] for i in range(len(test))]\n",
    "```\n",
    "- Prende il valore corrispondente della settimana precedente per ogni giorno del test set.  \n",
    "\n",
    "#### **3. Exponential Smoothing (Holt-Winters)**  \n",
    "```python\n",
    "hw_model = ExponentialSmoothing(\n",
    "    train['sales'], seasonal_periods=7, trend='add', seasonal='add', use_boxcox=True\n",
    ")\n",
    "hw_fit = hw_model.fit(optimized=True)\n",
    "forecasts['exponential_smoothing'] = hw_fit.forecast(len(test))\n",
    "```\n",
    "- Applica il modello **Holt-Winters** con:\n",
    "  - **Trend** e **stagionalit√† additive**.  \n",
    "  - **Box-Cox transformation** per stabilizzare la varianza.  \n",
    "- Effettua la previsione sulla finestra di test.  \n",
    "\n",
    "#### **4. ARIMA (AutoRegressive Integrated Moving Average)**  \n",
    "```python\n",
    "arima_model = ARIMA(train['sales'], order=(1, 1, 1), seasonal_order=(1, 1, 0, 7))\n",
    "arima_fit = arima_model.fit()\n",
    "forecasts['arima'] = arima_fit.forecast(len(test))\n",
    "```\n",
    "- Utilizza il modello **ARIMA(1,1,1)(1,1,0)7**, che incorpora:\n",
    "  - **1 autoregressione (AR)**.  \n",
    "  - **1 differenziazione (I)** per rendere i dati stazionari.  \n",
    "  - **1 media mobile (MA)**.  \n",
    "  - **1 componente stagionale settimanale (SAR)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **9.5 Valutazione delle Previsioni**  \n",
    "\n",
    "```python\n",
    "def calculate_all_metrics(actual, predicted, name):\n",
    "    naive_errors = np.abs(train['sales'].diff().dropna()).mean()\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'MAE': mean_absolute_error(actual, predicted),\n",
    "        'MSE': mean_squared_error(actual, predicted),\n",
    "        'RMSE': np.sqrt(mean_squared_error(actual, predicted)),\n",
    "        'MAPE': np.mean(np.abs((actual - predicted) / actual)) * 100,\n",
    "        'SMAPE': 200 * np.mean(np.abs(actual - predicted) / (np.abs(actual) + np.abs(predicted))),\n",
    "        'MASE': mean_absolute_error(actual, predicted) / naive_errors,\n",
    "    }\n",
    "```\n",
    "- **Calcola vari metriche di errore:**  \n",
    "  - **MAE:** errore assoluto medio.  \n",
    "  - **MSE:** errore quadratico medio.  \n",
    "  - **RMSE:** radice quadrata dell‚ÄôMSE.  \n",
    "  - **MAPE:** errore percentuale medio assoluto.  \n",
    "  - **SMAPE:** errore percentuale simmetrico.  \n",
    "  - **MASE:** errore rispetto al Naive Forecast.  \n",
    "\n",
    "```python\n",
    "metrics = [calculate_all_metrics(forecasts['actual'], forecasts[col], col) for col in forecasts.columns if col != 'actual']\n",
    "metrics_df = pd.DataFrame(metrics).set_index('Model')\n",
    "```\n",
    "- Applica la funzione a tutti i modelli e memorizza i risultati in un DataFrame.  \n",
    "\n",
    "```python\n",
    "metrics_df\n",
    "```\n",
    "- Mostra la tabella dei risultati comparativi tra i modelli.  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b79feb-24a8-4e6f-8a0b-9411537d2907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38c28862-abc7-4ccb-9c8b-fc14af93b844",
   "metadata": {},
   "source": [
    "# 9.4 Interpretazione dei risultati\n",
    "Analizziamo cosa ci dicono queste metriche sulle prestazioni di ciascun modello:\n",
    "\n",
    "Metriche dipendenti dalla scala (MAE, MSE, RMSE):\n",
    "\n",
    "Queste metriche ci forniscono l'entit√† assoluta degli errori nelle unit√† originali (importo delle vendite).\n",
    "Valori inferiori indicano prestazioni migliori.\n",
    "Dai nostri risultati, possiamo vedere che [interpreta quale modello funziona meglio su queste metriche].\n",
    "Errori percentuali (MAPE, SMAPE):\n",
    "\n",
    "Queste metriche esprimono gli errori come percentuali, rendendoli indipendenti dalla scala.\n",
    "Ci aiutano a comprendere la dimensione relativa degli errori rispetto ai valori effettivi.\n",
    "Osserviamo che [interpreta i risultati degli errori percentuali].\n",
    "Errori scalati (MASE):\n",
    "\n",
    "MASE confronta gli errori del nostro modello con una previsione ingenua.\n",
    "Valori inferiori a 1 indicano un miglioramento rispetto al modello ingenuo.\n",
    "La nostra analisi mostra [interpreta i risultati MASE].\n",
    "10.2 Casi di utilizzo comuni e metriche consigliate\n",
    "Esaminiamo diversi scenari comuni di previsione delle serie temporali e le metriche che sono in genere pi√π appropriate per ciascuno:\n",
    "\n",
    "#### Previsione delle vendite al dettaglio\n",
    "\n",
    "Metriche primarie: MAPE, SMAPE, MASE\n",
    "\n",
    "Motivazione:\n",
    "\n",
    "Gli errori percentuali sono intuitivi per gli stakeholder aziendali (\"le previsioni erano sbagliate del 5%\")\n",
    "MASE aiuta a confrontare con semplici metodi di base\n",
    "Se si confrontano le prestazioni tra diverse categorie di prodotti con volumi di vendita variabili, sono essenziali errori percentuali o scalati\n",
    "Previsione della domanda per la catena di fornitura\n",
    "\n",
    "Metriche primarie: MAPE ponderato (con pesi maggiori per articoli ad alto volume), MASE, Pinball Loss (se si utilizzano previsioni quantili)\n",
    "\n",
    "Motivazione:\n",
    "\n",
    "Nella catena di fornitura, il costo dell'errore dipende spesso dal volume dell'articolo\n",
    "Costi asimmetrici di sovraprevisione rispetto a sottoprevisione (esaurimento scorte rispetto a inventario in eccesso)\n",
    "Le previsioni probabilistiche possono aiutare a gestire il rischio di inventario\n",
    "Carico energetico Previsione\n",
    "\n",
    "Metriche primarie: RMSE, MAPE, precisione direzionale\n",
    "\n",
    "Motivazione:\n",
    "\n",
    "RMSE penalizza gli errori di grandi dimensioni, che possono essere particolarmente costosi nei mercati energetici\n",
    "La direzione dei cambiamenti √® spesso importante per pianificare la capacit√† di generazione\n",
    "Sia la precisione assoluta che la cattura dei pattern sono importanti\n",
    "Serie temporali finanziarie/Previsione del prezzo delle azioni\n",
    "\n",
    "Metriche primarie: precisione direzionale, MASE, U di Theil\n",
    "\n",
    "Motivazione:\n",
    "\n",
    "Spesso la direzione del movimento √® pi√π importante del valore esatto\n",
    "Il confronto con previsioni ingenue √® essenziale (i mercati sono spesso efficienti)\n",
    "Nelle strategie di trading, essere direzionalmente corretti √® spesso pi√π redditizio che minimizzare l'errore\n",
    "Previsione del traffico del sito Web\n",
    "\n",
    "Metriche primarie: RMSE, MAPE, precisione direzionale\n",
    "\n",
    "Motivazione:\n",
    "\n",
    "Sia la precisione che la cattura dei pattern sono importanti\n",
    "Gli errori percentuali sono intuitivi per la reportistica\n",
    "La previsione dei picchi di traffico (grandi deviazioni) pu√≤ essere particolarmente importante per la pianificazione delle risorse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6452c5b-d5ff-447b-8f75-a5907f105d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
